<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>The best one in ComplexWebQuestions LeaderBoard is 70.4</title>
      <link href="2021/04/27/The-best-one-in-ComplexWebQuestions-LeaderBoard-is-70-4/"/>
      <url>2021/04/27/The-best-one-in-ComplexWebQuestions-LeaderBoard-is-70-4/</url>
      
        <content type="html"><![CDATA[<p>From <a href="https://www.tau-nlp.org/compwebq-leaderboard">The leaderboard in ComplexWebQuestions</a>, the accuracy of best one is 70.4, this result from paper <a href="https://arxiv.org/abs/2104.08762">Case-based Reasoning for Natural Language Queries over Knowledge Bases-2021</a>.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Finish my first work in Complex KGQA</title>
      <link href="2021/04/24/Finish-my-first-work-in-Complex-KGQA/"/>
      <url>2021/04/24/Finish-my-first-work-in-Complex-KGQA/</url>
      
        <content type="html"><![CDATA[<p><font color=green>ST-BERT: A BERT-based framework for Complex Question Answering over Knowledge Graph</font></p><p>There are two types of complexity need to be addressed in answering complex question over knowledge graph (Complex KGQA), question with multi-hop relations and question with constraints. In this work, we handle both types of complexity. </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Sentence-BERT</title>
      <link href="2021/03/11/Sentence-BERT/"/>
      <url>2021/03/11/Sentence-BERT/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/UKPLab/sentence-transformers">UKPLab-sentence-transformers</a></p><p><a href="https://blog.ceshine.net/post/zero-shot-bert-sent-emb/">Other reference</a></p><p>The demo of semantic search <a href="https://www.sbert.net/examples/applications/semantic-search/README.html">Semantic Search</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">This is a simple application for sentence embeddings: semantic search</span><br><span class="line"></span><br><span class="line">We have a corpus with various sentences. Then, for a given query sentence,</span><br><span class="line">we want to find the most similar sentence in this corpus.</span><br><span class="line"></span><br><span class="line">This script outputs for various queries the top 5 most similar sentences in the corpus.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from sentence_transformers import SentenceTransformer, util</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">embedder &#x3D; SentenceTransformer(&#39;paraphrase-distilroberta-base-v1&#39;)</span><br><span class="line"></span><br><span class="line"># Corpus with example sentences</span><br><span class="line">corpus &#x3D; [&#39;A man is eating food.&#39;,</span><br><span class="line">          &#39;A man is eating a piece of bread.&#39;,</span><br><span class="line">          &#39;The girl is carrying a baby.&#39;,</span><br><span class="line">          &#39;A man is riding a horse.&#39;,</span><br><span class="line">          &#39;A woman is playing violin.&#39;,</span><br><span class="line">          &#39;Two men pushed carts through the woods.&#39;,</span><br><span class="line">          &#39;A man is riding a white horse on an enclosed ground.&#39;,</span><br><span class="line">          &#39;A monkey is playing drums.&#39;,</span><br><span class="line">          &#39;A cheetah is running behind its prey.&#39;</span><br><span class="line">          ]</span><br><span class="line">corpus_embeddings &#x3D; embedder.encode(corpus, convert_to_tensor&#x3D;True)</span><br><span class="line"></span><br><span class="line"># Query sentences:</span><br><span class="line">queries &#x3D; [&#39;A man is eating pasta.&#39;, &#39;Someone in a gorilla costume is playing a set of drums.&#39;, &#39;A cheetah chases prey on across a field.&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity</span><br><span class="line">top_k &#x3D; min(5, len(corpus))</span><br><span class="line">for query in queries:</span><br><span class="line">    query_embedding &#x3D; embedder.encode(query, convert_to_tensor&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # We use cosine-similarity and torch.topk to find the highest 5 scores</span><br><span class="line">    cos_scores &#x3D; util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]</span><br><span class="line">    top_results &#x3D; torch.topk(cos_scores, k&#x3D;top_k)</span><br><span class="line"></span><br><span class="line">    print(&quot;\n\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n\n&quot;)</span><br><span class="line">    print(&quot;Query:&quot;, query)</span><br><span class="line">    print(&quot;\nTop 5 most similar sentences in corpus:&quot;)</span><br><span class="line"></span><br><span class="line">    for score, idx in zip(top_results[0], top_results[1]):</span><br><span class="line">        print(corpus[idx], &quot;(Score: &#123;:.4f&#125;)&quot;.format(score))</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk</span><br><span class="line">    hits &#x3D; util.semantic_search(query_embedding, corpus_embeddings, top_k&#x3D;5)</span><br><span class="line">    hits &#x3D; hits[0]      #Get the hits for the first query</span><br><span class="line">    for hit in hits:</span><br><span class="line">        print(corpus[hit[&#39;corpus_id&#39;]], &quot;(Score: &#123;:.4f&#125;)&quot;.format(hit[&#39;score&#39;]))</span><br><span class="line">    &quot;&quot;&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Transformers_version_str_object has no attribute_dim</title>
      <link href="2021/03/09/Transformers-version-str-object-has-no-attribute-dim/"/>
      <url>2021/03/09/Transformers-version-str-object-has-no-attribute-dim/</url>
      
        <content type="html"><![CDATA[<p>Yes, its a bug, when you import transformers to excute your BERT or other language model.</p><p>Full error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AttributeError: &#39;str&#39; object has no attribute &#39;dim&#39;</span><br></pre></td></tr></table></figure><p>This error occues in transformers 3.4.0, you just fix it by:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers&#x3D;&#x3D;3.0.0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Matchzoo-ImportError-losses_utils</title>
      <link href="2021/03/09/Matchzoo-ImportError-losses-utils/"/>
      <url>2021/03/09/Matchzoo-ImportError-losses-utils/</url>
      
        <content type="html"><![CDATA[<p>When install MatchZoo, I meet </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: cannot import name &#39;losses_utils</span><br></pre></td></tr></table></figure><p>package version, you just need:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras &#x3D;&#x3D; 2.3.0 </span><br><span class="line">tabulate &gt;&#x3D; 0.8.2 </span><br><span class="line">tensorflow &gt;&#x3D; 2.0.0</span><br></pre></td></tr></table></figure><p>There still an error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AttributeError: module &#39;tensorflow.python.framework.ops&#39; has no attribute &#39;_TensorLike</span><br></pre></td></tr></table></figure><p>Yes, fuck version error:</p><p>Just:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras &#x3D;&#x3D; 2.4.0 </span><br><span class="line">tabulate &gt;&#x3D; 0.8.2 </span><br><span class="line">tensorflow &#x3D;&#x3D; 2.3.0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Using BERT for any task you want</title>
      <link href="2021/03/08/Using-BERT-for-any-task-you-want/"/>
      <url>2021/03/08/Using-BERT-for-any-task-you-want/</url>
      
        <content type="html"><![CDATA[<p>This blog just for my learn, <a href="https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209">the reference sit</a></p><p>Using BERT for any task you want</p><p>Although Text Summarization, Question answering, and a basic Language Model are especially important, often, people want to use BERT for other unspecified tasks, especially in research. The way that they do this is by taking the raw outputs of the stacked encoders of BERT, and attaching their own specific model to it, most commonly a linear layer, and then fine-tuning this model on their specific dataset. When doing this in Pytorch using the Hugging Face transformer library, it is best to set this up as a Pytorch deep learning model like such:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from transformers import BertModel</span><br><span class="line">class Bert_Model(nn.Module):</span><br><span class="line">   def __init__(self, class):</span><br><span class="line">       super(Bert_Model, self).__init__()</span><br><span class="line">       self.bert &#x3D; BertModel.from_pretrained(&#39;bert-base-uncased&#39;)</span><br><span class="line">       self.out &#x3D; nn.Linear(self.bert.config.hidden_size, classes)</span><br><span class="line">   def forward(self, input):</span><br><span class="line">       _, output &#x3D; self.bert(**input)</span><br><span class="line">       out &#x3D; self.out(output)</span><br><span class="line">       return out</span><br></pre></td></tr></table></figure><p>As you can see, instead of downloading a specific BERT Model already designed for a specific task like Question Answering, I downloaded the raw pre-trained BertModel, which does not come with any heads attached to it.<br>To get the size of the raw BERT outputs, simply use self.bert.config.hidden_size, and attach this to the number of classes you want your linear layer to output.<br>To use the code above for sentiment analysis, which is surprisingly a task that does not come downloaded/already done in the hugging face transformer library, you can simply add a sigmoid activation function onto the end of the linear layer and specify the classes to equal 1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from transformers import BertModel</span><br><span class="line">class Bert_Model(nn.Module):</span><br><span class="line">   def __init__(self, class):</span><br><span class="line">       super(Bert_Model, self).__init__()</span><br><span class="line">       self.bert &#x3D; BertModel.from_pretrained(&#39;bert-base-uncased&#39;)</span><br><span class="line">       self.out &#x3D; nn.Linear(self.bert.config.hidden_size, classes)</span><br><span class="line">       self.sigmoid &#x3D; nn.Sigmoid()</span><br><span class="line">   def forward(self, input, attention_mask):</span><br><span class="line">       _, output &#x3D; self.bert(input, attention_mask &#x3D; attention_mask)</span><br><span class="line">       out &#x3D; self.sigmoid(self.out(output))</span><br><span class="line">       return out</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Begain, my blog!</title>
      <link href="2021/03/08/hello-world/"/>
      <url>2021/03/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>I should celebrate this day, this is the first time I have used Hexo and MarkdownPad to write my blog, It’s cool!</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Language model for text classification in NLP</title>
      <link href="2021/03/07/Language-model-for-text-classification-in-NLP/"/>
      <url>2021/03/07/Language-model-for-text-classification-in-NLP/</url>
      
        <content type="html"><![CDATA[<p>Recently, there are many language model, such as BERT, GPT-2, XLNet, DistilBERT, RoBERTa… Base these model, we can fine-tune in the last layer to finish our task, such as text calsssification.</p><p><a href="https://huggingface.co/transformers/pretrained_models.html">Pretrained models in huggingface</a></p><p><a href="https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1">Part 2: BERT Fine-Tuning Tutorial with PyTorch for Text Classification on The Corpus of Linguistic Acceptability (COLA) Dataset.</a></p><p><a href="https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209">How to use BERT from the Hugging Face transformer library</a></p><p><a href="https://mccormickml.com/2019/09/19/XLNet-fine-tuning/">XLNet Fine-Tuning Tutorial with PyTorch</a></p><p><a href="https://gmihaila.medium.com/gpt2-for-text-classification-using-hugging-face-transformers-574555451832">GPT for Fine-Tuning Tutorial with PyTorch</a></p><p>I refer these blog, and add model to provide test, save checkpoints.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from keras.preprocessing.sequence import pad_sequences</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler</span><br><span class="line">from transformers import BertForSequenceClassification, AdamW, BertConfig</span><br><span class="line">from transformers import BertTokenizer</span><br><span class="line">import  numpy as np</span><br><span class="line">from transformers import get_linear_schedule_with_warmup</span><br><span class="line">tokenizer &#x3D; BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;, do_lower_case&#x3D;True)</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line">import datetime</span><br><span class="line">import random</span><br><span class="line">MAX_LEN&#x3D;15</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">reference: https:&#x2F;&#x2F;medium.com&#x2F;@aniruddha.choudhury94&#x2F;part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1</span><br><span class="line">pretraind_model: https:&#x2F;&#x2F;huggingface.co&#x2F;transformers&#x2F;pretrained_models.html</span><br><span class="line">there are many retrained models such as: bert-large-uncased,bert-base-multilingual-uncased,bert-base-cased-finetuned-mrpc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_data(file):</span><br><span class="line">    sentences&#x3D;[]</span><br><span class="line">    labels&#x3D;[]</span><br><span class="line">    with open(file,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line&#x3D;line.strip().split(&quot;\t&quot;)</span><br><span class="line">            sentences.append(line[0])</span><br><span class="line">            labels.append(np.int64(line[-1]))</span><br><span class="line">    return  sentences,labels</span><br><span class="line"></span><br><span class="line">def get_input_and_mask(sentences):</span><br><span class="line">    input_ids &#x3D; []</span><br><span class="line">    # For every sentence...</span><br><span class="line">    for sent in sentences:</span><br><span class="line">        # &#96;encode&#96; will:</span><br><span class="line">        #   (1) Tokenize the sentence.</span><br><span class="line">        #   (2) Prepend the &#96;[CLS]&#96; token to the start.</span><br><span class="line">        #   (3) Append the &#96;[SEP]&#96; token to the end.</span><br><span class="line">        #   (4) Map tokens to their IDs.</span><br><span class="line">        encoded_sent &#x3D; tokenizer.encode(</span><br><span class="line">            sent,  # Sentence to encode.</span><br><span class="line">            add_special_tokens&#x3D;True,  # Add &#39;[CLS]&#39; and &#39;[SEP]&#39;</span><br><span class="line">            # This function also supports truncation and conversion</span><br><span class="line">            # to pytorch tensors, but we need to do padding, so we</span><br><span class="line">            # can&#39;t use these features :( .</span><br><span class="line">            # max_length &#x3D; 128,          # Truncate all sentences.</span><br><span class="line">            # return_tensors &#x3D; &#39;pt&#39;,     # Return pytorch tensors.</span><br><span class="line">        )</span><br><span class="line">        # Add the encoded sentence to the list.</span><br><span class="line">        input_ids.append(encoded_sent)</span><br><span class="line">    # Print sentence 0, now as a list of IDs.</span><br><span class="line"></span><br><span class="line">    input_ids &#x3D; pad_sequences(input_ids, maxlen&#x3D;MAX_LEN, dtype&#x3D;&quot;long&quot;,</span><br><span class="line">                              value&#x3D;0, truncating&#x3D;&quot;post&quot;, padding&#x3D;&quot;post&quot;)</span><br><span class="line"></span><br><span class="line">    # Create attention masks</span><br><span class="line">    attention_masks &#x3D; []</span><br><span class="line">    # For each sentence...</span><br><span class="line">    for sent in input_ids:</span><br><span class="line">        # Create the attention mask.</span><br><span class="line">        #   - If a token ID is 0, then it&#39;s padding, set the mask to 0.</span><br><span class="line">        #   - If a token ID is &gt; 0, then it&#39;s a real token, set the mask to 1.</span><br><span class="line">        att_mask &#x3D; [int(token_id &gt; 0) for token_id in sent]</span><br><span class="line"></span><br><span class="line">        # Store the attention mask for this sentence.</span><br><span class="line">        attention_masks.append(att_mask)</span><br><span class="line"></span><br><span class="line">    return  input_ids, attention_masks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_input_id():</span><br><span class="line">    train_sentences, train_labels &#x3D; get_data(&quot;data&#x2F;webseb_SS_step1_train.txt&quot;)</span><br><span class="line">    train_input_ids, train_attention_masks&#x3D;get_input_and_mask(train_sentences)</span><br><span class="line"></span><br><span class="line">    dev_sentences, dev_labels &#x3D; get_data(&quot;data&#x2F;webseb_SS_step1_test.txt&quot;)</span><br><span class="line">    dev_input_ids, dev_attention_masks &#x3D; get_input_and_mask(dev_sentences)</span><br><span class="line">    # print(train_sentences)</span><br><span class="line">    train_inputs &#x3D; torch.tensor(train_input_ids)</span><br><span class="line">    validation_inputs &#x3D; torch.tensor(dev_input_ids)</span><br><span class="line"></span><br><span class="line">    train_labels &#x3D; torch.tensor(train_labels)</span><br><span class="line">    validation_labels &#x3D; torch.tensor(dev_labels)</span><br><span class="line"></span><br><span class="line">    train_masks &#x3D; torch.tensor(train_attention_masks)</span><br><span class="line">    validation_masks &#x3D; torch.tensor(dev_attention_masks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    batch_size &#x3D; 32</span><br><span class="line">    # Create the DataLoader for our training set.</span><br><span class="line">    train_data &#x3D; TensorDataset(train_inputs, train_masks, train_labels)</span><br><span class="line">    train_sampler &#x3D; RandomSampler(train_data)</span><br><span class="line"></span><br><span class="line">    train_dataloader &#x3D; DataLoader(train_data, sampler&#x3D;train_sampler, batch_size&#x3D;batch_size)</span><br><span class="line">    # Create the DataLoader for our validation set.</span><br><span class="line">    validation_data &#x3D; TensorDataset(validation_inputs, validation_masks, validation_labels)</span><br><span class="line">    validation_sampler &#x3D; SequentialSampler(validation_data) # 不打乱</span><br><span class="line">    validation_dataloader &#x3D; DataLoader(validation_data, sampler&#x3D;validation_sampler, batch_size&#x3D;batch_size)</span><br><span class="line"></span><br><span class="line">    return  train_dataloader, validation_dataloader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def flat_accuracy(preds, labels):</span><br><span class="line">    pred_flat &#x3D; np.argmax(preds, axis&#x3D;1).flatten()</span><br><span class="line">    labels_flat &#x3D; labels.flatten()</span><br><span class="line">    return np.sum(pred_flat &#x3D;&#x3D; labels_flat) &#x2F; len(labels_flat)</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">def format_time(elapsed):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Takes a time in seconds and returns a string hh:mm:ss</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # Round to the nearest second.</span><br><span class="line">    elapsed_rounded &#x3D; int(round((elapsed)))</span><br><span class="line"></span><br><span class="line">    # Format as hh:mm:ss</span><br><span class="line">    return str(datetime.timedelta(seconds&#x3D;elapsed_rounded))</span><br><span class="line"></span><br><span class="line">def TE(model,validation_dataloader):</span><br><span class="line">    model.eval()</span><br><span class="line">    # Tracking variables</span><br><span class="line">    eval_loss, eval_accuracy &#x3D; 0, 0</span><br><span class="line">    nb_eval_steps, nb_eval_examples &#x3D; 0, 0</span><br><span class="line">    # Evaluate data for one epoch</span><br><span class="line">    for batch in validation_dataloader:</span><br><span class="line">        # Add batch to GPU</span><br><span class="line">        batch &#x3D; tuple(t.cuda() for t in batch)</span><br><span class="line"></span><br><span class="line">        # Unpack the inputs from our dataloader</span><br><span class="line">        b_input_ids, b_input_mask, b_labels &#x3D; batch</span><br><span class="line"></span><br><span class="line">        # speeding up validation</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            outputs &#x3D; model(b_input_ids,</span><br><span class="line">                            token_type_ids&#x3D;None,</span><br><span class="line">                            attention_mask&#x3D;b_input_mask)</span><br><span class="line"></span><br><span class="line">        # Get the &quot;logits&quot; output by the model. The &quot;logits&quot; are the output</span><br><span class="line">        # values prior to applying an activation function like the softmax.</span><br><span class="line">        logits &#x3D; outputs[0]</span><br><span class="line">        # Move logits and labels to CPU</span><br><span class="line">        logits &#x3D; logits.detach().cpu().numpy()</span><br><span class="line">        label_ids &#x3D; b_labels.to(&#39;cpu&#39;).numpy()</span><br><span class="line"></span><br><span class="line">        # Calculate the accuracy for this batch of test sentences.</span><br><span class="line">        tmp_eval_accuracy &#x3D; flat_accuracy(logits, label_ids)</span><br><span class="line"></span><br><span class="line">        # Accumulate the total accuracy.</span><br><span class="line">        eval_accuracy +&#x3D; tmp_eval_accuracy</span><br><span class="line">        # Track the number of batches</span><br><span class="line">        nb_eval_steps +&#x3D; 1</span><br><span class="line">    # Report the final accuracy for this validation run.</span><br><span class="line">    print(&quot;  Accuracy: &#123;0:.2f&#125;&quot;.format(eval_accuracy &#x2F; nb_eval_steps))</span><br><span class="line">    print(&quot;  Validation took: &#123;:&#125;&quot;.format(format_time(time.time() - t0)))</span><br><span class="line">    global_results&#x3D;eval_accuracy &#x2F; nb_eval_steps</span><br><span class="line">    return  global_results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    patience&#x3D;5</span><br><span class="line">    model &#x3D; BertForSequenceClassification.from_pretrained(</span><br><span class="line">        &quot;bert-base-uncased&quot;,  # Use the 12-layer BERT model, with an uncased vocab.</span><br><span class="line">        num_labels&#x3D;5,  # The number of output labels--2 for binary classification.</span><br><span class="line">        # You can increase this for multi-class tasks.</span><br><span class="line">        output_attentions&#x3D;False,  # Whether the model returns attentions weights.</span><br><span class="line">        output_hidden_states&#x3D;False,  # Whether the model returns all hidden-states.</span><br><span class="line">    )</span><br><span class="line">    # Tell pytorch to run this model on the GPU.</span><br><span class="line">    model.cuda()</span><br><span class="line">    best_model &#x3D; model.state_dict()</span><br><span class="line">    optimizer &#x3D; AdamW(model.parameters(), lr&#x3D;5e-5, eps&#x3D;1e-8)  # args.adam_epsilon  - default is 1e-8. )</span><br><span class="line"></span><br><span class="line">    train_dataloader, validation_dataloader &#x3D; get_input_id()</span><br><span class="line"></span><br><span class="line">    # Create the learning rate scheduler.</span><br><span class="line">    epochs &#x3D; 30</span><br><span class="line">    total_steps &#x3D; len(train_dataloader) * epochs</span><br><span class="line"></span><br><span class="line">    scheduler &#x3D; get_linear_schedule_with_warmup(optimizer,num_warmup_steps&#x3D;0,  # Default value in run_glue.py</span><br><span class="line">                                                num_training_steps&#x3D;total_steps)</span><br><span class="line"></span><br><span class="line">    seed_val &#x3D; 42</span><br><span class="line">    random.seed(seed_val)</span><br><span class="line">    np.random.seed(seed_val)</span><br><span class="line">    torch.manual_seed(seed_val)</span><br><span class="line">    torch.cuda.manual_seed_all(seed_val)</span><br><span class="line">    # Store the average loss after each epoch so we can plot them.</span><br><span class="line">    loss_values &#x3D; []</span><br><span class="line">    # For each epoch...</span><br><span class="line">    best_score &#x3D; -float(&quot;inf&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for epoch_i in range(0, epochs):</span><br><span class="line">        # Perform one full pass over the training set.</span><br><span class="line">        print(&quot;&quot;)</span><br><span class="line">        print(&#39;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Epoch &#123;:&#125; &#x2F; &#123;:&#125; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#39;.format(epoch_i + 1, epochs))</span><br><span class="line">        print(&#39;Training...&#39;)</span><br><span class="line">        # Measure how long the training epoch takes.</span><br><span class="line">        t0 &#x3D; time.time()</span><br><span class="line">        # Reset the total loss for this epoch.</span><br><span class="line">        total_loss &#x3D; 0</span><br><span class="line">        model.train()</span><br><span class="line">        # For each batch of training data...</span><br><span class="line">        for step, batch in enumerate(train_dataloader):</span><br><span class="line">            # Progress update every 40 batches.</span><br><span class="line">            if step % 40 &#x3D;&#x3D; 0 and not step &#x3D;&#x3D; 0:</span><br><span class="line">                # Calculate elapsed time in minutes.</span><br><span class="line">                elapsed &#x3D; format_time(time.time() - t0)</span><br><span class="line">                # Report progress.</span><br><span class="line">                print(&#39;  Batch &#123;:&gt;5,&#125;  of  &#123;:&gt;5,&#125;.    Elapsed: &#123;:&#125;.&#39;.format(step, len(train_dataloader), elapsed))</span><br><span class="line">            b_input_ids &#x3D; batch[0].cuda()#to(device)</span><br><span class="line">            b_input_mask &#x3D; batch[1].cuda()#to(device)</span><br><span class="line">            b_labels &#x3D; batch[2].cuda()#to(device)</span><br><span class="line">            model.zero_grad()</span><br><span class="line"></span><br><span class="line">            outputs &#x3D; model(b_input_ids,</span><br><span class="line">                            token_type_ids&#x3D;None,</span><br><span class="line">                            attention_mask&#x3D;b_input_mask,</span><br><span class="line">                            labels&#x3D;b_labels)</span><br><span class="line"></span><br><span class="line">            loss &#x3D; outputs[0]</span><br><span class="line"></span><br><span class="line">            total_loss +&#x3D; loss.item()</span><br><span class="line">            # Perform a backward pass to calculate the gradients.</span><br><span class="line">            loss.backward()</span><br><span class="line">            # Clip the norm of the gradients to 1.0.</span><br><span class="line">            # This is to help prevent the &quot;exploding gradients&quot; problem.</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)</span><br><span class="line">            # Update parameters and take a step using the computed gradient.</span><br><span class="line">            # The optimizer dictates the &quot;update rule&quot;--how the parameters are</span><br><span class="line">            # modified based on their gradients, the learning rate, etc.</span><br><span class="line">            optimizer.step()</span><br><span class="line">            # Update the learning rate.</span><br><span class="line">            scheduler.step()</span><br><span class="line">        # Calculate the average loss over the training data.</span><br><span class="line">        avg_train_loss &#x3D; total_loss &#x2F; len(train_dataloader)</span><br><span class="line"></span><br><span class="line">        # Store the loss value for plotting the learning curve.</span><br><span class="line">        loss_values.append(avg_train_loss)</span><br><span class="line">        print(&quot;&quot;)</span><br><span class="line">        print(&quot;  Average training loss: &#123;0:.2f&#125;&quot;.format(avg_train_loss))</span><br><span class="line">        print(&quot;  Training epcoh took: &#123;:&#125;&quot;.format(format_time(time.time() - t0)))</span><br><span class="line"></span><br><span class="line">        # &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">        #               Validation</span><br><span class="line">        # &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">        # After the completion of each training epoch, measure our performance on</span><br><span class="line">        # our validation set.</span><br><span class="line">        print(&quot;&quot;)</span><br><span class="line">        print(&quot;Running Validation...&quot;)</span><br><span class="line">        t0 &#x3D; time.time()</span><br><span class="line">        # Put the model in evaluation mode--the dropout layers behave differently</span><br><span class="line">        # during evaluation.</span><br><span class="line">        global_results&#x3D;TE(model, validation_dataloader)</span><br><span class="line">        eps &#x3D; 0.0001</span><br><span class="line">        if global_results &gt; best_score + eps:</span><br><span class="line">            best_score &#x3D; global_results</span><br><span class="line">            no_update &#x3D; 0</span><br><span class="line">            best_model &#x3D; model.state_dict()</span><br><span class="line">            print(&quot; accuracy %s increased from previous epoch&quot; % (str(global_results)))</span><br><span class="line">            checkpoint_path &#x3D; &#39;checkpoints&#x2F;&#39;</span><br><span class="line">            checkpoint_file_name &#x3D; checkpoint_path + &quot;.pt&quot;</span><br><span class="line">            torch.save(model.state_dict(), checkpoint_file_name)</span><br><span class="line"></span><br><span class="line">        elif (global_results &lt; best_score + eps) and (no_update &lt; patience):</span><br><span class="line">            no_update +&#x3D; 1</span><br><span class="line">            print(&quot;Validation accuracy decreases to %s from %s, %d more epoch to check&quot; % (</span><br><span class="line">            global_results, best_score, patience - no_update))</span><br><span class="line">        elif no_update &#x3D;&#x3D; patience:</span><br><span class="line">            print(&quot;Model has exceed patience. Saving best model and exiting&quot;)</span><br><span class="line">            torch.save(best_model, checkpoint_path + &quot;best_score_model.pt&quot;)</span><br><span class="line">            exit()</span><br><span class="line">        if epoch_i &#x3D;&#x3D; epochs - 1:</span><br><span class="line">            print(&quot;Final Epoch has reached. Stopping and saving model.&quot;)</span><br><span class="line">            torch.save(best_model, checkpoint_path + &quot;best_score_model.pt&quot;)</span><br><span class="line">            exit()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>If you want to use different model, you just adjust:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification,AdamW</span><br><span class="line">from transformers import  RobertaModel, RobertaTokenizer, RobertaForSequenceClassification</span><br><span class="line">from transformers import  DistilBertModel, DistilBertTokenizer,DistilBertForSequenceClassification</span><br><span class="line">from transformers import  CamembertTokenizer, CamembertForSequenceClassification</span><br><span class="line"></span><br><span class="line">from transformers import AlbertTokenizer,AlbertForSequenceClassification</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>tokenizer </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1): tokenizer &#x3D; AlbertTokenizer.from_pretrained(&#39;albert-base-v2&#39;, do_lower_case&#x3D;True)</span><br><span class="line">(2): tokenizer &#x3D; BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;, do_lower_case&#x3D;True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Note: “bert-base-uncased” from <a href="https://huggingface.co/transformers/pretrained_models.html">huggingface-pretrained model</a></p><p>model</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)   model &#x3D; AlbertForSequenceClassification.from_pretrained(&quot;albert-base-v2&quot;, num_labels&#x3D;5)</span><br><span class="line"></span><br><span class="line">(2) model &#x3D; XLNetForSequenceClassification.from_pretrained(&quot;xlnet-base-cased&quot;, num_labels&#x3D;5)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Fine tuning for classification by BERT</title>
      <link href="2021/03/05/Fine-tuning-for-classification-by-BERT/"/>
      <url>2021/03/05/Fine-tuning-for-classification-by-BERT/</url>
      
        <content type="html"><![CDATA[<p>BERT Fine-Tuning Tutorial with PyTorch for Text Classification on The Corpus of Linguistic Acceptability (COLA) Dataset. I refer <a href="https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1">There</a></p><p>Whole code</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from keras.preprocessing.sequence import pad_sequences</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler</span><br><span class="line">from transformers import BertForSequenceClassification, AdamW, BertConfig</span><br><span class="line"></span><br><span class="line">from transformers import BertForQuestionAnswering</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">https:&#x2F;&#x2F;medium.com&#x2F;@aniruddha.choudhury94&#x2F;part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load the dataset into a pandas dataframe.</span><br><span class="line">df &#x3D; pd.read_csv(&quot;cola_public&#x2F;raw&#x2F;in_domain_train.tsv&quot;, delimiter&#x3D;&#39;\t&#39;, header&#x3D;None, names&#x3D;[&#39;sentence_source&#39;, &#39;label&#39;, &#39;label_notes&#39;, &#39;sentence&#39;])</span><br><span class="line"># Report the number of sentences.</span><br><span class="line">from transformers import BertTokenizer</span><br><span class="line">print(&#39;Number of training sentences: &#123;:,&#125;\n&#39;.format(df.shape[0]))</span><br><span class="line"></span><br><span class="line"># Display 10 random rows from the data.</span><br><span class="line">s&#x3D;df.sample(10)</span><br><span class="line"># Get the lists of sentences and their labels.</span><br><span class="line">sentences &#x3D; df.sentence.values</span><br><span class="line">labels &#x3D; df.label.values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load the BERT tokenizer.</span><br><span class="line">print(&#39;Loading BERT tokenizer...&#39;)</span><br><span class="line">tokenizer &#x3D; BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;, do_lower_case&#x3D;True)</span><br><span class="line"></span><br><span class="line"># Tokenize all of the sentences and map the tokens to thier word IDs.</span><br><span class="line">input_ids &#x3D; []</span><br><span class="line"># For every sentence...</span><br><span class="line">for sent in sentences:</span><br><span class="line">    # &#96;encode&#96; will:</span><br><span class="line">    #   (1) Tokenize the sentence.</span><br><span class="line">    #   (2) Prepend the &#96;[CLS]&#96; token to the start.</span><br><span class="line">    #   (3) Append the &#96;[SEP]&#96; token to the end.</span><br><span class="line">    #   (4) Map tokens to their IDs.</span><br><span class="line">    encoded_sent &#x3D; tokenizer.encode(</span><br><span class="line">        sent,  # Sentence to encode.</span><br><span class="line">        add_special_tokens&#x3D;True,  # Add &#39;[CLS]&#39; and &#39;[SEP]&#39;</span><br><span class="line">        # This function also supports truncation and conversion</span><br><span class="line">        # to pytorch tensors, but we need to do padding, so we</span><br><span class="line">        # can&#39;t use these features :( .</span><br><span class="line">        # max_length &#x3D; 128,          # Truncate all sentences.</span><br><span class="line">        # return_tensors &#x3D; &#39;pt&#39;,     # Return pytorch tensors.</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # Add the encoded sentence to the list.</span><br><span class="line">    input_ids.append(encoded_sent)</span><br><span class="line"># Print sentence 0, now as a list of IDs.</span><br><span class="line">print(&#39;Original: &#39;, sentences[0])</span><br><span class="line">print(&#39;Token IDs:&#39;, input_ids[0])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># We&#39;ll borrow the &#96;pad_sequences&#96; utility function to do this.</span><br><span class="line"></span><br><span class="line"># Set the maximum sequence length.</span><br><span class="line"># I&#39;ve chosen 64 somewhat arbitrarily. It&#39;s slightly larger than the</span><br><span class="line"># maximum training sentence length of 47...</span><br><span class="line">MAX_LEN &#x3D; 64</span><br><span class="line">print(&#39;\nPadding&#x2F;truncating all sentences to %d values...&#39; % MAX_LEN)</span><br><span class="line">print(&#39;\nPadding token: &quot;&#123;:&#125;&quot;, ID: &#123;:&#125;&#39;.format(tokenizer.pad_token, tokenizer.pad_token_id))</span><br><span class="line"># Pad our input tokens with value 0.</span><br><span class="line"># &quot;post&quot; indicates that we want to pad and truncate at the end of the sequence,</span><br><span class="line"># as opposed to the beginning.</span><br><span class="line">input_ids &#x3D; pad_sequences(input_ids, maxlen&#x3D;MAX_LEN, dtype&#x3D;&quot;long&quot;,</span><br><span class="line">                          value&#x3D;0, truncating&#x3D;&quot;post&quot;, padding&#x3D;&quot;post&quot;)</span><br><span class="line">print(&#39;\Done.&#39;)</span><br><span class="line"></span><br><span class="line"># Create attention masks</span><br><span class="line">attention_masks &#x3D; []</span><br><span class="line"># For each sentence...</span><br><span class="line">for sent in input_ids:</span><br><span class="line">    # Create the attention mask.</span><br><span class="line">    #   - If a token ID is 0, then it&#39;s padding, set the mask to 0.</span><br><span class="line">    #   - If a token ID is &gt; 0, then it&#39;s a real token, set the mask to 1.</span><br><span class="line">    att_mask &#x3D; [int(token_id &gt; 0) for token_id in sent]</span><br><span class="line"></span><br><span class="line">    # Store the attention mask for this sentence.</span><br><span class="line">    attention_masks.append(att_mask)</span><br><span class="line"></span><br><span class="line"># Use 90% for training and 10% for validation.</span><br><span class="line">train_inputs, validation_inputs, train_labels, validation_labels &#x3D; train_test_split(input_ids, labels,</span><br><span class="line">                                                            random_state&#x3D;2018, test_size&#x3D;0.1)</span><br><span class="line"># Do the same for the masks.</span><br><span class="line">train_masks, validation_masks, _, _ &#x3D; train_test_split(attention_masks, labels,</span><br><span class="line">                                             random_state&#x3D;2018, test_size&#x3D;0.1)</span><br><span class="line"></span><br><span class="line"># Convert all inputs and labels into torch tensors, the required datatype</span><br><span class="line"># for our model.</span><br><span class="line"></span><br><span class="line">train_inputs &#x3D; torch.tensor(train_inputs)</span><br><span class="line">validation_inputs &#x3D; torch.tensor(validation_inputs)</span><br><span class="line">train_labels &#x3D; torch.tensor(train_labels)</span><br><span class="line">validation_labels &#x3D; torch.tensor(validation_labels)</span><br><span class="line">train_masks &#x3D; torch.tensor(train_masks)</span><br><span class="line">validation_masks &#x3D; torch.tensor(validation_masks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># The DataLoader needs to know our batch size for training, so we specify it</span><br><span class="line"># here.</span><br><span class="line"># For fine-tuning BERT on a specific task, the authors recommend a batch size of</span><br><span class="line"># 16 or 32.</span><br><span class="line">batch_size &#x3D; 32</span><br><span class="line"># Create the DataLoader for our training set.</span><br><span class="line">train_data &#x3D; TensorDataset(train_inputs, train_masks, train_labels)</span><br><span class="line">train_sampler &#x3D; RandomSampler(train_data)</span><br><span class="line">train_dataloader &#x3D; DataLoader(train_data, sampler&#x3D;train_sampler, batch_size&#x3D;batch_size)</span><br><span class="line"># Create the DataLoader for our validation set.</span><br><span class="line">validation_data &#x3D; TensorDataset(validation_inputs, validation_masks, validation_labels)</span><br><span class="line">validation_sampler &#x3D; SequentialSampler(validation_data)</span><br><span class="line">validation_dataloader &#x3D; DataLoader(validation_data, sampler&#x3D;validation_sampler, batch_size&#x3D;batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load BertForSequenceClassification, the pretrained BERT model with a single</span><br><span class="line"># linear classification layer on top.</span><br><span class="line">model &#x3D; BertForSequenceClassification.from_pretrained(</span><br><span class="line">    &quot;bert-base-uncased&quot;, # Use the 12-layer BERT model, with an uncased vocab.</span><br><span class="line">    num_labels &#x3D; 2, # The number of output labels--2 for binary classification.</span><br><span class="line">                    # You can increase this for multi-class tasks.</span><br><span class="line">    output_attentions &#x3D; False, # Whether the model returns attentions weights.</span><br><span class="line">    output_hidden_states &#x3D; False, # Whether the model returns all hidden-states.</span><br><span class="line">)</span><br><span class="line"># Tell pytorch to run this model on the GPU.</span><br><span class="line">model.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Get all of the model&#39;s parameters as a list of tuples.</span><br><span class="line"># params &#x3D; list(model.named_parameters())</span><br><span class="line"># print(&#39;The BERT model has &#123;:&#125; different named parameters.\n&#39;.format(len(params)))</span><br><span class="line"># print(&#39;&#x3D;&#x3D;&#x3D;&#x3D; Embedding Layer &#x3D;&#x3D;&#x3D;&#x3D;\n&#39;)</span><br><span class="line"># for p in params[0:5]:</span><br><span class="line">#     print(&quot;&#123;:&lt;55&#125; &#123;:&gt;12&#125;&quot;.format(p[0], str(tuple(p[1].size()))))</span><br><span class="line"># print(&#39;\n&#x3D;&#x3D;&#x3D;&#x3D; First Transformer &#x3D;&#x3D;&#x3D;&#x3D;\n&#39;)</span><br><span class="line"># for p in params[5:21]:</span><br><span class="line">#     print(&quot;&#123;:&lt;55&#125; &#123;:&gt;12&#125;&quot;.format(p[0], str(tuple(p[1].size()))))</span><br><span class="line"># print(&#39;\n&#x3D;&#x3D;&#x3D;&#x3D; Output Layer &#x3D;&#x3D;&#x3D;&#x3D;\n&#39;)</span><br><span class="line"># for p in params[-4:]:</span><br><span class="line">#     print(&quot;&#123;:&lt;55&#125; &#123;:&gt;12&#125;&quot;.format(p[0], str(tuple(p[1].size()))))</span><br><span class="line"># Note: AdamW is a class from the huggingface library (as opposed to pytorch)</span><br><span class="line"># I believe the &#39;W&#39; stands for &#39;Weight Decay fix&quot;</span><br><span class="line">optimizer &#x3D; AdamW(model.parameters(),</span><br><span class="line">                  lr &#x3D; 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5</span><br><span class="line">                  eps &#x3D; 1e-8 # args.adam_epsilon  - default is 1e-8.</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">from transformers import get_linear_schedule_with_warmup</span><br><span class="line"># Number of training epochs (authors recommend between 2 and 4)</span><br><span class="line">epochs &#x3D; 4</span><br><span class="line"># Total number of training steps is number of batches * number of epochs.</span><br><span class="line">total_steps &#x3D; len(train_dataloader) * epochs</span><br><span class="line"># Create the learning rate scheduler.</span><br><span class="line">scheduler &#x3D; get_linear_schedule_with_warmup(optimizer,</span><br><span class="line">                                            num_warmup_steps &#x3D; 0, # Default value in run_glue.py</span><br><span class="line">                                            num_training_steps &#x3D; total_steps)</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"># Function to calculate the accuracy of our predictions vs labels</span><br><span class="line">def flat_accuracy(preds, labels):</span><br><span class="line">    pred_flat &#x3D; np.argmax(preds, axis&#x3D;1).flatten()</span><br><span class="line">    labels_flat &#x3D; labels.flatten()</span><br><span class="line">    return np.sum(pred_flat &#x3D;&#x3D; labels_flat) &#x2F; len(labels_flat)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def format_time(elapsed):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Takes a time in seconds and returns a string hh:mm:ss</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # Round to the nearest second.</span><br><span class="line">    elapsed_rounded &#x3D; int(round((elapsed)))</span><br><span class="line"></span><br><span class="line">    # Format as hh:mm:ss</span><br><span class="line">    return str(datetime.timedelta(seconds&#x3D;elapsed_rounded))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line"># This training code is based on the &#96;run_glue.py&#96; script here:</span><br><span class="line"># https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;blob&#x2F;5bfcd0485ece086ebcbed2d008813037968a9e58&#x2F;examples&#x2F;run_glue.py#L128</span><br><span class="line"># Set the seed value all over the place to make this reproducible.</span><br><span class="line">seed_val &#x3D; 42</span><br><span class="line">random.seed(seed_val)</span><br><span class="line">np.random.seed(seed_val)</span><br><span class="line">torch.manual_seed(seed_val)</span><br><span class="line">torch.cuda.manual_seed_all(seed_val)</span><br><span class="line"># Store the average loss after each epoch so we can plot them.</span><br><span class="line">loss_values &#x3D; []</span><br><span class="line"># For each epoch...</span><br><span class="line">for epoch_i in range(0, epochs):</span><br><span class="line"></span><br><span class="line">    # &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    #               Training</span><br><span class="line">    # &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">    # Perform one full pass over the training set.</span><br><span class="line">    print(&quot;&quot;)</span><br><span class="line">    print(&#39;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Epoch &#123;:&#125; &#x2F; &#123;:&#125; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#39;.format(epoch_i + 1, epochs))</span><br><span class="line">    print(&#39;Training...&#39;)</span><br><span class="line">    # Measure how long the training epoch takes.</span><br><span class="line">    t0 &#x3D; time.time()</span><br><span class="line">    # Reset the total loss for this epoch.</span><br><span class="line">    total_loss &#x3D; 0</span><br><span class="line">    # Put the model into training mode. Don&#39;t be mislead--the call to</span><br><span class="line">    # &#96;train&#96; just changes the *mode*, it doesn&#39;t *perform* the training.</span><br><span class="line">    # &#96;dropout&#96; and &#96;batchnorm&#96; layers behave differently during training</span><br><span class="line">    # vs. test (source: https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;51433378&#x2F;what-does-model-train-do-in-pytorch)</span><br><span class="line">    model.train()</span><br><span class="line">    # For each batch of training data...</span><br><span class="line">    for step, batch in enumerate(train_dataloader):</span><br><span class="line">        # Progress update every 40 batches.</span><br><span class="line">        if step % 40 &#x3D;&#x3D; 0 and not step &#x3D;&#x3D; 0:</span><br><span class="line">            # Calculate elapsed time in minutes.</span><br><span class="line">            elapsed &#x3D; format_time(time.time() - t0)</span><br><span class="line"></span><br><span class="line">            # Report progress.</span><br><span class="line">            print(&#39;  Batch &#123;:&gt;5,&#125;  of  &#123;:&gt;5,&#125;.    Elapsed: &#123;:&#125;.&#39;.format(step, len(train_dataloader), elapsed))</span><br><span class="line">        # Unpack this training batch from our dataloader.</span><br><span class="line">        #</span><br><span class="line">        # As we unpack the batch, we&#39;ll also copy each tensor to the GPU using the</span><br><span class="line">        # &#96;to&#96; method.</span><br><span class="line">        #</span><br><span class="line">        # &#96;batch&#96; contains three pytorch tensors:</span><br><span class="line">        #   [0]: input ids</span><br><span class="line">        #   [1]: attention masks</span><br><span class="line">        #   [2]: labels</span><br><span class="line">        b_input_ids &#x3D; batch[0].cuda()#to(device)</span><br><span class="line">        b_input_mask &#x3D; batch[1].cuda()#to(device)</span><br><span class="line">        b_labels &#x3D; batch[2].cuda()#to(device)</span><br><span class="line">        # Always clear any previously calculated gradients before performing a</span><br><span class="line">        # backward pass. PyTorch doesn&#39;t do this automatically because</span><br><span class="line">        # accumulating the gradients is &quot;convenient while training RNNs&quot;.</span><br><span class="line">        # (source: https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;48001598&#x2F;why-do-we-need-to-call-zero-grad-in-pytorch)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        # Perform a forward pass (evaluate the model on this training batch).</span><br><span class="line">        # This will return the loss (rather than the model output) because we</span><br><span class="line">        # have provided the &#96;labels&#96;.</span><br><span class="line">        # The documentation for this &#96;model&#96; function is here:</span><br><span class="line">        # https:&#x2F;&#x2F;huggingface.co&#x2F;transformers&#x2F;v2.2.0&#x2F;model_doc&#x2F;bert.html#transformers.BertForSequenceClassification</span><br><span class="line">        outputs &#x3D; model(b_input_ids,</span><br><span class="line">                        token_type_ids&#x3D;None,</span><br><span class="line">                        attention_mask&#x3D;b_input_mask,</span><br><span class="line">                        labels&#x3D;b_labels)</span><br><span class="line"></span><br><span class="line">        # The call to &#96;model&#96; always returns a tuple, so we need to pull the</span><br><span class="line">        # loss value out of the tuple.</span><br><span class="line">        loss &#x3D; outputs[0]</span><br><span class="line">        # Accumulate the training loss over all of the batches so that we can</span><br><span class="line">        # calculate the average loss at the end. &#96;loss&#96; is a Tensor containing a</span><br><span class="line">        # single value; the &#96;.item()&#96; function just returns the Python value</span><br><span class="line">        # from the tensor.</span><br><span class="line">        total_loss +&#x3D; loss.item()</span><br><span class="line">        # Perform a backward pass to calculate the gradients.</span><br><span class="line">        loss.backward()</span><br><span class="line">        # Clip the norm of the gradients to 1.0.</span><br><span class="line">        # This is to help prevent the &quot;exploding gradients&quot; problem.</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)</span><br><span class="line">        # Update parameters and take a step using the computed gradient.</span><br><span class="line">        # The optimizer dictates the &quot;update rule&quot;--how the parameters are</span><br><span class="line">        # modified based on their gradients, the learning rate, etc.</span><br><span class="line">        optimizer.step()</span><br><span class="line">        # Update the learning rate.</span><br><span class="line">        scheduler.step()</span><br><span class="line">    # Calculate the average loss over the training data.</span><br><span class="line">    avg_train_loss &#x3D; total_loss &#x2F; len(train_dataloader)</span><br><span class="line"></span><br><span class="line">    # Store the loss value for plotting the learning curve.</span><br><span class="line">    loss_values.append(avg_train_loss)</span><br><span class="line">    print(&quot;&quot;)</span><br><span class="line">    print(&quot;  Average training loss: &#123;0:.2f&#125;&quot;.format(avg_train_loss))</span><br><span class="line">    print(&quot;  Training epcoh took: &#123;:&#125;&quot;.format(format_time(time.time() - t0)))</span><br><span class="line"></span><br><span class="line">    # &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    #               Validation</span><br><span class="line">    # &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">    # After the completion of each training epoch, measure our performance on</span><br><span class="line">    # our validation set.</span><br><span class="line">    print(&quot;&quot;)</span><br><span class="line">    print(&quot;Running Validation...&quot;)</span><br><span class="line">    t0 &#x3D; time.time()</span><br><span class="line">    # Put the model in evaluation mode--the dropout layers behave differently</span><br><span class="line">    # during evaluation.</span><br><span class="line">    model.eval()</span><br><span class="line">    # Tracking variables</span><br><span class="line">    eval_loss, eval_accuracy &#x3D; 0, 0</span><br><span class="line">    nb_eval_steps, nb_eval_examples &#x3D; 0, 0</span><br><span class="line">    # Evaluate data for one epoch</span><br><span class="line">    for batch in validation_dataloader:</span><br><span class="line">        # Add batch to GPU</span><br><span class="line">        batch &#x3D; tuple(t.cuda() for t in batch)</span><br><span class="line"></span><br><span class="line">        # Unpack the inputs from our dataloader</span><br><span class="line">        b_input_ids, b_input_mask, b_labels &#x3D; batch</span><br><span class="line"></span><br><span class="line">        # Telling the model not to compute or store gradients, saving memory and</span><br><span class="line">        # speeding up validation</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            # Forward pass, calculate logit predictions.</span><br><span class="line">            # This will return the logits rather than the loss because we have</span><br><span class="line">            # not provided labels.</span><br><span class="line">            # token_type_ids is the same as the &quot;segment ids&quot;, which</span><br><span class="line">            # differentiates sentence 1 and 2 in 2-sentence tasks.</span><br><span class="line">            # The documentation for this &#96;model&#96; function is here:</span><br><span class="line">            # https:&#x2F;&#x2F;huggingface.co&#x2F;transformers&#x2F;v2.2.0&#x2F;model_doc&#x2F;bert.html#transformers.BertForSequenceClassification</span><br><span class="line">            outputs &#x3D; model(b_input_ids,</span><br><span class="line">                            token_type_ids&#x3D;None,</span><br><span class="line">                            attention_mask&#x3D;b_input_mask)</span><br><span class="line"></span><br><span class="line">        # Get the &quot;logits&quot; output by the model. The &quot;logits&quot; are the output</span><br><span class="line">        # values prior to applying an activation function like the softmax.</span><br><span class="line">        logits &#x3D; outputs[0]</span><br><span class="line">        # Move logits and labels to CPU</span><br><span class="line">        logits &#x3D; logits.detach().cpu().numpy()</span><br><span class="line">        label_ids &#x3D; b_labels.to(&#39;cpu&#39;).numpy()</span><br><span class="line"></span><br><span class="line">        # Calculate the accuracy for this batch of test sentences.</span><br><span class="line">        tmp_eval_accuracy &#x3D; flat_accuracy(logits, label_ids)</span><br><span class="line"></span><br><span class="line">        # Accumulate the total accuracy.</span><br><span class="line">        eval_accuracy +&#x3D; tmp_eval_accuracy</span><br><span class="line">        # Track the number of batches</span><br><span class="line">        nb_eval_steps +&#x3D; 1</span><br><span class="line">    # Report the final accuracy for this validation run.</span><br><span class="line">    print(&quot;  Accuracy: &#123;0:.2f&#125;&quot;.format(eval_accuracy &#x2F; nb_eval_steps))</span><br><span class="line">    print(&quot;  Validation took: &#123;:&#125;&quot;.format(format_time(time.time() - t0)))</span><br><span class="line">print(&quot;&quot;)</span><br><span class="line">print(&quot;Training complete!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Load the dataset into a pandas dataframe.</span><br><span class="line">df &#x3D; pd.read_csv(&quot;cola_public&#x2F;raw&#x2F;out_of_domain_dev.tsv&quot;, delimiter&#x3D;&#39;\t&#39;, header&#x3D;None,</span><br><span class="line">                 names&#x3D;[&#39;sentence_source&#39;, &#39;label&#39;, &#39;label_notes&#39;, &#39;sentence&#39;])</span><br><span class="line"># Report the number of sentences.</span><br><span class="line">print(&#39;Number of test sentences: &#123;:,&#125;\n&#39;.format(df.shape[0]))</span><br><span class="line"># Create sentence and label lists</span><br><span class="line">sentences &#x3D; df.sentence.values</span><br><span class="line">labels &#x3D; df.label.values</span><br><span class="line"># Tokenize all of the sentences and map the tokens to thier word IDs.</span><br><span class="line">input_ids &#x3D; []</span><br><span class="line"># For every sentence...</span><br><span class="line">for sent in sentences:</span><br><span class="line">    # &#96;encode&#96; will:</span><br><span class="line">    #   (1) Tokenize the sentence.</span><br><span class="line">    #   (2) Prepend the &#96;[CLS]&#96; token to the start.</span><br><span class="line">    #   (3) Append the &#96;[SEP]&#96; token to the end.</span><br><span class="line">    #   (4) Map tokens to their IDs.</span><br><span class="line">    encoded_sent &#x3D; tokenizer.encode(</span><br><span class="line">        sent,  # Sentence to encode.</span><br><span class="line">        add_special_tokens&#x3D;True,  # Add &#39;[CLS]&#39; and &#39;[SEP]&#39;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    input_ids.append(encoded_sent)</span><br><span class="line"># Pad our input tokens</span><br><span class="line">input_ids &#x3D; pad_sequences(input_ids, maxlen&#x3D;MAX_LEN,</span><br><span class="line">                          dtype&#x3D;&quot;long&quot;, truncating&#x3D;&quot;post&quot;, padding&#x3D;&quot;post&quot;)</span><br><span class="line"># Create attention masks</span><br><span class="line">attention_masks &#x3D; []</span><br><span class="line"># Create a mask of 1s for each token followed by 0s for padding</span><br><span class="line">for seq in input_ids:</span><br><span class="line">    seq_mask &#x3D; [float(i &gt; 0) for i in seq]</span><br><span class="line">    attention_masks.append(seq_mask)</span><br><span class="line"># Convert to tensors.</span><br><span class="line">prediction_inputs &#x3D; torch.tensor(input_ids)</span><br><span class="line">prediction_masks &#x3D; torch.tensor(attention_masks)</span><br><span class="line">prediction_labels &#x3D; torch.tensor(labels)</span><br><span class="line"># Set the batch size.</span><br><span class="line">batch_size &#x3D; 32</span><br><span class="line"># Create the DataLoader.</span><br><span class="line">prediction_data &#x3D; TensorDataset(prediction_inputs, prediction_masks, prediction_labels)</span><br><span class="line">prediction_sampler &#x3D; SequentialSampler(prediction_data)</span><br><span class="line">prediction_dataloader &#x3D; DataLoader(prediction_data, sampler&#x3D;prediction_sampler, batch_size&#x3D;batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Prediction on test set</span><br><span class="line">print(&#39;Predicting labels for &#123;:,&#125; test sentences...&#39;.format(len(prediction_inputs)))</span><br><span class="line"># Put model in evaluation mode</span><br><span class="line">model.eval()</span><br><span class="line"># Tracking variables</span><br><span class="line">predictions, true_labels &#x3D; [], []</span><br><span class="line"># Predict</span><br><span class="line">for batch in prediction_dataloader:</span><br><span class="line">    # Add batch to GPU</span><br><span class="line">    batch &#x3D; tuple(t.cuda() for t in batch)</span><br><span class="line"></span><br><span class="line">    # Unpack the inputs from our dataloader</span><br><span class="line">    b_input_ids, b_input_mask, b_labels &#x3D; batch</span><br><span class="line"></span><br><span class="line">    # Telling the model not to compute or store gradients, saving memory and</span><br><span class="line">    # speeding up prediction</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        # Forward pass, calculate logit predictions</span><br><span class="line">        outputs &#x3D; model(b_input_ids, token_type_ids&#x3D;None,</span><br><span class="line">                        attention_mask&#x3D;b_input_mask)</span><br><span class="line">    logits &#x3D; outputs[0]</span><br><span class="line">    # Move logits and labels to CPU</span><br><span class="line">    logits &#x3D; logits.detach().cpu().numpy()</span><br><span class="line">    label_ids &#x3D; b_labels.to(&#39;cpu&#39;).numpy()</span><br><span class="line"></span><br><span class="line">    # Store predictions and true labels</span><br><span class="line">    predictions.append(logits)</span><br><span class="line">    true_labels.append(label_ids)</span><br><span class="line">print(&#39;DONE.&#39;)</span><br><span class="line"></span><br><span class="line"># Prediction on test set</span><br><span class="line">print(&#39;Predicting labels for &#123;:,&#125; test sentences...&#39;.format(len(prediction_inputs)))</span><br><span class="line"># Put model in evaluation mode</span><br><span class="line">model.eval()</span><br><span class="line"># Tracking variables</span><br><span class="line">predictions, true_labels &#x3D; [], []</span><br><span class="line"># Predict</span><br><span class="line">for batch in prediction_dataloader:</span><br><span class="line">    # Add batch to GPU</span><br><span class="line">    batch &#x3D; tuple(t.cuda() for t in batch)</span><br><span class="line"></span><br><span class="line">    # Unpack the inputs from our dataloader</span><br><span class="line">    b_input_ids, b_input_mask, b_labels &#x3D; batch</span><br><span class="line"></span><br><span class="line">    # Telling the model not to compute or store gradients, saving memory and</span><br><span class="line">    # speeding up prediction</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        # Forward pass, calculate logit predictions</span><br><span class="line">        outputs &#x3D; model(b_input_ids, token_type_ids&#x3D;None,</span><br><span class="line">                        attention_mask&#x3D;b_input_mask)</span><br><span class="line">    logits &#x3D; outputs[0]</span><br><span class="line">    # Move logits and labels to CPU</span><br><span class="line">    logits &#x3D; logits.detach().cpu().numpy()</span><br><span class="line">    label_ids &#x3D; b_labels.to(&#39;cpu&#39;).numpy()</span><br><span class="line"></span><br><span class="line">    # Store predictions and true labels</span><br><span class="line">    predictions.append(logits)</span><br><span class="line">    true_labels.append(label_ids)</span><br><span class="line">print(&#39;DONE.&#39;)</span><br><span class="line">print(&#39;Positive samples: %d of %d (%.2f%%)&#39; % (df.label.sum(), len(df.label), (df.label.sum() &#x2F; len(df.label) * 100.0)))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Error [torch.FloatTensor [1, 800]], which is output 0 of SliceBackward</title>
      <link href="2021/03/02/Error-torch-FloatTensor-1-800-which-is-output-0-of-SliceBackward/"/>
      <url>2021/03/02/Error-torch-FloatTensor-1-800-which-is-output-0-of-SliceBackward/</url>
      
        <content type="html"><![CDATA[<p>The error: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\torch\autograd\__init__.py&quot;, line 99, in backward</span><br><span class="line">    allow_unreachable&#x3D;True)  # allow_unreachable flag</span><br><span class="line">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 800]], which is output 0 of SliceBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</span><br><span class="line"></span><br><span class="line">Process finished with exit code 1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In the code:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">relation_&#x3D;nn.Parameter(torch.rand([2,2,3]))</span><br><span class="line"></span><br><span class="line">last_head_embedding&#x3D;nn.Parameter(torch.ones([2,3]))</span><br><span class="line">relation_[:, 1, :]&#x3D;last_head_embedding.clone()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(relation_graph)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>I want to update a row in the relation_, in this demo, I found when  last_head_embedding and relation_ are all “grad”, the result is right, but this situation is not run in my another code, we should let (get out grad by follow):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">last_head_embedding&#x3D;Variable(last_head_embedding,requires_grad&#x3D;False)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How to restrict the search space in KG</title>
      <link href="2021/02/14/How-to-restrict-the-search-space-in-KG/"/>
      <url>2021/02/14/How-to-restrict-the-search-space-in-KG/</url>
      
        <content type="html"><![CDATA[<p>The main challenge in searching is how to restrict the search space, i.e., to reduce the number of multi-hop relation paths to be considered, because the search space grows exponentially with the length of relation paths. One idea is to use beam search. For example, <a href="https://arxiv.org/abs/1904.01246">Chen et al. (2019)-Uhop</a> and <a href="https://ieeexplore.ieee.org/abstract/document/8970943">Lan et al. (2019b)</a> proposed to consider only the best matching relation instead of all relations when extending a relation path.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Errorbar in error chart</title>
      <link href="2021/01/31/Errorbar-in-error-chart/"/>
      <url>2021/01/31/Errorbar-in-error-chart/</url>
      
        <content type="html"><![CDATA[<p>The simple excample for error char  in Errorbar python.</p><p><a href="https://www.cnblogs.com/czz0508/p/10454492.html">python Errorbar in Chinese web</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">k_choices &#x3D; [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]</span><br><span class="line"></span><br><span class="line">k_to_accuracies&#x3D;&#123;1: [0.263, 0.257, 0.264, 0.278, 0.266], 3: [0.257, 0.263, 0.273, 0.282, 0.27], 5: [0.265, 0.275, 0.295, 0.298, 0.284], 8: [0.272, 0.295, 0.284, 0.298, 0.29], 10: [0.272, 0.303, 0.289, 0.292, 0.285], 12: [0.271, 0.305, 0.285, 0.289, 0.281], 15: [0.26, 0.302, 0.292, 0.292, 0.285], 20: [0.268, 0.293, 0.291, 0.287, 0.286], 50: [0.273, 0.291, 0.274, 0.267, 0.273], 100: [0.261, 0.272, 0.267, 0.26, 0.267]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for k in k_choices:</span><br><span class="line">    accuracies &#x3D; k_to_accuracies[k]</span><br><span class="line"></span><br><span class="line">    plt.scatter([k] * len(accuracies), accuracies)</span><br><span class="line">#</span><br><span class="line">accuracies_mean &#x3D; np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])</span><br><span class="line">#accuracies_mean--[0.2656 0.269  0.2834 0.2878 0.2882 0.2862 0.2862 0.285  0.2756 0.2654]</span><br><span class="line"></span><br><span class="line">accuracies_std &#x3D; np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])</span><br><span class="line">print(accuracies_std)#点代表平均值，上下长条线的范围代表标准差</span><br><span class="line">plt.errorbar(k_choices, accuracies_mean, yerr&#x3D;accuracies_std)</span><br><span class="line">plt.title(&#39;Cross-validation on k&#39;)</span><br><span class="line">plt.xlabel(&#39;k&#39;)</span><br><span class="line">plt.ylabel(&#39;Cross-validation accuracy&#39;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;-----another excample--------&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">plt.errorbar(x&#x3D;[1, 2, 3, 4], y&#x3D;[1, 3, 3, 4], yerr&#x3D;[1, 2, 3, 4])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Three method for Euclidean Distance</title>
      <link href="2021/01/31/Three-method-for-Euclidean-Distance/"/>
      <url>2021/01/31/Three-method-for-Euclidean-Distance/</url>
      
        <content type="html"><![CDATA[<p>For two matrix eg: x:[400,3570], y:[600,3570], how to claculate the Euclidean Distance for them, </p><p><a href="https://iamlimingchen.com/2021/01/25/KNN-in-DL-class/">Euclidean Distance function</a></p><h2 id="method1-two–hop"><a href="#method1-two–hop" class="headerlink" title="method1 two–hop"></a>method1 two–hop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def compute_distances_two_loops(X,Y):</span><br><span class="line"></span><br><span class="line">   Inputs:</span><br><span class="line">   - X: Y</span><br><span class="line"></span><br><span class="line">   Returns:</span><br><span class="line">   - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span><br><span class="line">     is the Euclidean distance between the ith test point and the jth training</span><br><span class="line">     point.</span><br><span class="line">   &quot;&quot;&quot;</span><br><span class="line">   #.shape: (500, 3072)</span><br><span class="line">   num_test &#x3D; X.shape[0] #500</span><br><span class="line"></span><br><span class="line">   num_train &#x3D; Y.shape[0] #5000</span><br><span class="line"></span><br><span class="line">   dists &#x3D; np.zeros((num_test, num_train))</span><br><span class="line">   # code by Mingchen Li</span><br><span class="line">   for i in xrange(num_test):</span><br><span class="line">     for j in xrange(num_train):</span><br><span class="line">       test_vector&#x3D;X[i,:]</span><br><span class="line">       train_vector&#x3D;Y[j,:]</span><br><span class="line">       diff&#x3D;train_vector-test_vector</span><br><span class="line">       sqrDiff &#x3D; diff ** 2</span><br><span class="line">       sqrDiffSum &#x3D; sqrDiff.sum(axis&#x3D;0)</span><br><span class="line">       distances &#x3D; sqrDiffSum ** 0.5</span><br><span class="line">       dists[i,j]&#x3D;distances</span><br><span class="line"></span><br><span class="line">   return dists</span><br></pre></td></tr></table></figure><h2 id="method2-one–hop"><a href="#method2-one–hop" class="headerlink" title="method2 one–hop"></a>method2 one–hop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def compute_distances_one_loop(X,Y):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  Compute the distance between each test point in X and each training point</span><br><span class="line">  in Y using a single loop over the test data.</span><br><span class="line"></span><br><span class="line">  Input &#x2F; Output: Same as compute_distances_two_loops</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  num_test &#x3D; X.shape[0]</span><br><span class="line">  num_train &#x3D; Y.shape[0]</span><br><span class="line">  dists &#x3D; np.zeros((num_test, num_train))</span><br><span class="line">  for i in xrange(num_test):</span><br><span class="line"></span><br><span class="line">    diff &#x3D; np.tile(X[i,: ], (num_train, 1)) - Y</span><br><span class="line">    sqrDiff &#x3D; diff ** 2</span><br><span class="line">    #</span><br><span class="line">    sqrDiffSum &#x3D; sqrDiff.sum(axis&#x3D;1)</span><br><span class="line">    distances &#x3D; sqrDiffSum ** 0.5</span><br><span class="line">    dists[i, :]&#x3D;distances</span><br><span class="line"></span><br><span class="line">  return dists</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="method3-none–hop"><a href="#method3-none–hop" class="headerlink" title="method3 none–hop"></a>method3 none–hop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.spatial import distance</span><br><span class="line"></span><br><span class="line"># get the Euclidean Distance between to matrix</span><br><span class="line">#   (x-y)^2 &#x3D; |x|^2 + |y|^2 - 2xy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def EuclideanDistances2(A, B):</span><br><span class="line">    BT &#x3D; B.transpose()</span><br><span class="line">    # vecProd &#x3D; A * BT</span><br><span class="line">    vecProd &#x3D; np.dot(A, BT)</span><br><span class="line"></span><br><span class="line">    SqA &#x3D; A ** 2</span><br><span class="line">    print(SqA)</span><br><span class="line">    sumSqA &#x3D; np.matrix((np.sum(SqA, axis&#x3D;1)))</span><br><span class="line">    print(sumSqA)</span><br><span class="line">    sumSqAEx &#x3D; np.tile(sumSqA.transpose(), (1, vecProd.shape[1]))</span><br><span class="line">    print(sumSqAEx)</span><br><span class="line"></span><br><span class="line">    SqB &#x3D; B ** 2</span><br><span class="line">    print(&quot;sqB&quot;,SqB)</span><br><span class="line">    sumSqB &#x3D; np.sum(SqB, axis&#x3D;1)</span><br><span class="line">    print(&quot;sumSqB&quot;,sumSqB)</span><br><span class="line">    sumSqBEx &#x3D; np.tile(sumSqB, (vecProd.shape[0], 1))</span><br><span class="line">    print(&quot;sumSqBEx&quot;,sumSqBEx)</span><br><span class="line">    SqED &#x3D; sumSqBEx + sumSqAEx - 2 * vecProd</span><br><span class="line">    # SqED[SqED &lt; 0] &#x3D; 0.0</span><br><span class="line">    ED &#x3D; np.sqrt(SqED)</span><br><span class="line">    return ED</span><br><span class="line"></span><br><span class="line">x&#x3D;np.array([[2,3],[4,4]]) #(2,3)</span><br><span class="line">y&#x3D;np.array([[2,3],[4,4],[4,4]]) #(3,2)</span><br><span class="line">dis&#x3D;EuclideanDistances2(x,y)</span><br><span class="line">print(dis)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s&#x3D;distance.cdist(x, y, &#39;euclidean&#39;)</span><br><span class="line">print(s)</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">dists&#x3D;np.zeros((2,3))</span><br><span class="line"></span><br><span class="line">for i in range(2):</span><br><span class="line">    for j in range(3):</span><br><span class="line">        test_vector &#x3D; x[i, :]</span><br><span class="line">        train_vector &#x3D; y[j, :]</span><br><span class="line">        diff &#x3D; train_vector - test_vector</span><br><span class="line">        sqrDiff &#x3D; diff ** 2</span><br><span class="line">        sqrDiffSum &#x3D; sqrDiff.sum(axis&#x3D;0)</span><br><span class="line">        distances &#x3D; sqrDiffSum ** 0.5</span><br><span class="line">        dists[i, j] &#x3D; distances</span><br><span class="line"></span><br><span class="line">print(dists)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dits:</span><br><span class="line">[[0.         2.23606798 2.23606798]</span><br><span class="line"> [2.23606798 0.         0.        ]]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Error: TypeError: new() received an invalid combina...</title>
      <link href="2021/01/30/Error-TypeError-new-received-an-invalid-combina/"/>
      <url>2021/01/30/Error-TypeError-new-received-an-invalid-combina/</url>
      
        <content type="html"><![CDATA[<p>When I run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embedding_bilstm &#x3D; nn.LSTM(input_size&#x3D;300, hidden_size&#x3D;hidden_layer_dim &#x2F; 2,</span><br><span class="line">                                bidirectional&#x3D;True, bias&#x3D;True, batch_first&#x3D;True, dropout&#x3D;0.1,</span><br><span class="line">                                num_layers&#x3D;1)</span><br></pre></td></tr></table></figure><p>Error: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  w_ih &#x3D; Parameter(torch.Tensor(gate_size, layer_input_size))</span><br><span class="line">TypeError: new() received an invalid combination of arguments - got (float, int), but expected one of:</span><br><span class="line"> * (torch.device device)</span><br><span class="line"> * (torch.Storage storage)</span><br><span class="line"> * (Tensor other)</span><br><span class="line"> * (tuple of ints size, torch.device device)</span><br><span class="line">      didn&#39;t match because some of the arguments have invalid types: (!float!, !int!)</span><br><span class="line"> * (object data, torch.device device)</span><br><span class="line">      didn&#39;t match because some of the arguments have invalid types: (!float!, !int!)</span><br></pre></td></tr></table></figure><p>To be honest, you just: hidden_size=hidden_layer_dim / 2——–&gt;&gt;hidden_size=int(hidden_layer_dim / 2). Over!!</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Can I return something in python Class</title>
      <link href="2021/01/30/Can-I-return-something-in-python-Class/"/>
      <url>2021/01/30/Can-I-return-something-in-python-Class/</url>
      
        <content type="html"><![CDATA[<p> In python class, we all know, there are many fucntions, so can I return something when I run F=Class(), F is a value, so the <strong>str</strong>(self) is a good function.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Cat:</span><br><span class="line">    def __init__(self,color):</span><br><span class="line">        self.color&#x3D;color</span><br><span class="line">    def eat(self):</span><br><span class="line">        print(&quot;can you eat something&quot;)</span><br><span class="line">    def printinfo(self):</span><br><span class="line">        print(self.color)</span><br><span class="line">    def __str__(self):</span><br><span class="line">        return  &quot;the color of this object is:&quot;+self.color</span><br><span class="line"></span><br><span class="line">xiaoqiang&#x3D;Cat(&quot;green&quot;)</span><br><span class="line">print(xiaoqiang)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>So, in class Cat, the return value is the value of the content of <strong>str</strong>(self).</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>KNN in DL class</title>
      <link href="2021/01/25/KNN-in-DL-class/"/>
      <url>2021/01/25/KNN-in-DL-class/</url>
      
        <content type="html"><![CDATA[<p>This is the Nearest neighbor classifier. </p><p>(1)With N examples, how fast are training and prediction. The answer is Train O(1), Predict O(N). Bad things: we want classifiers that are fast at prediction, slow for training is ok. </p><p>(2) Two distance Metrics: L1 (Manhattan) distance, L2(Eulidean) distance.</p><p>(3) Two references <a href="https://www.cnblogs.com/lyuzt/p/10471617.html">Python with KNN</a> and <a href="https://towardsdatascience.com/knn-in-python-835643e2fb53">KNN in Python</a>.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TF-IDF in my Chinese health QA</title>
      <link href="2021/01/24/TF-IDF-in-my-Chinese-health-QA/"/>
      <url>2021/01/24/TF-IDF-in-my-Chinese-health-QA/</url>
      
        <content type="html"><![CDATA[<p>IF–IDF is the basic information retrival model, so in my work about Chinese health QA task, I deside use it as my basic model, ok, there are many model to finish your TF-IDF model, in the first, I decide to buid this model by skelearn, but I found it’s too slow, so I decide to use another model -gensim to do it.<br><a href="https://provenclei.github.io/2020/02/01/TF-IDF.html">Refer 1</a>, ok in the next I will show my code and the relevant results:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import jieba</span><br><span class="line">from gensim import corpora, models, similarities</span><br><span class="line">import numpy as np</span><br><span class="line">root_path&#x3D;os.getcwd()  # this dic</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loadtrainset(path):</span><br><span class="line"></span><br><span class="line">    processed_textset &#x3D; []</span><br><span class="line">    allclasstags &#x3D; []</span><br><span class="line">    with open(path,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line&#x3D;line.strip().split(&quot;\t&quot;)</span><br><span class="line">            processed_textset.append((line[0]+&quot; &quot;+line[1]).split(&quot; &quot;))</span><br><span class="line">            allclasstags.append(int(line[2]))</span><br><span class="line">    return processed_textset,allclasstags</span><br><span class="line">def change_(labels, scores):</span><br><span class="line">    couple &#x3D; list(zip(labels, scores))</span><br><span class="line">    return np.array(sorted(couple, key&#x3D;lambda x: x[1], reverse&#x3D;True))</span><br><span class="line"></span><br><span class="line">def DCG(y_true, y_pred,k,_threshold&#x3D;0):</span><br><span class="line"></span><br><span class="line">    coupled_pair &#x3D; change_(y_true, y_pred)</span><br><span class="line">    print(coupled_pair)</span><br><span class="line">    result &#x3D; 0.</span><br><span class="line">    for i, (label, score) in enumerate(coupled_pair):</span><br><span class="line">        print(label)</span><br><span class="line">        if i &gt;&#x3D; k:</span><br><span class="line">            break</span><br><span class="line">        if label &gt; _threshold:</span><br><span class="line">            result +&#x3D; (math.pow(2., label) - 1.) &#x2F; math.log(2. + i)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">def MRR(y_true, y_pred,_threshold&#x3D;0):</span><br><span class="line">    coupled_pair &#x3D; change_(y_true, y_pred)</span><br><span class="line">    for idx, (label, pred) in enumerate(coupled_pair):</span><br><span class="line">        if label &gt; _threshold:</span><br><span class="line">            return 1. &#x2F; (idx + 1)</span><br><span class="line">    return 0.</span><br><span class="line"></span><br><span class="line">def cos(vector1,vector2):</span><br><span class="line">    dot_product&#x3D;0</span><br><span class="line">    normA&#x3D;0.0</span><br><span class="line">    normB&#x3D;0.0</span><br><span class="line">    for a,b in zip(vector1,vector2):</span><br><span class="line">        dot_product+&#x3D;a*b</span><br><span class="line"></span><br><span class="line">        # print(&quot;dot&quot;,dot_product)</span><br><span class="line">        normA+&#x3D;a**2</span><br><span class="line">        normB+&#x3D;b**2</span><br><span class="line">    if normA&#x3D;&#x3D;0.0 or normB&#x3D;&#x3D;0.0:</span><br><span class="line">        return None</span><br><span class="line">    else:</span><br><span class="line">        return dot_product&#x2F;(math.sqrt(normA)*math.sqrt(normB))</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    train_data, classtags_list&#x3D;loadtrainset(&quot;LiCHD_jieba_train.txt&quot;)</span><br><span class="line">    dictionary &#x3D; corpora.Dictionary(train_data, prune_at&#x3D;2000000)</span><br><span class="line"></span><br><span class="line">    corpus_model &#x3D; [dictionary.doc2bow(train) for train in train_data]</span><br><span class="line">    tfidf_model &#x3D; models.TfidfModel(corpus_model)</span><br><span class="line">    # 对语料生成tfidf</span><br><span class="line">    corpus_tfidf &#x3D; tfidf_model[corpus_model]</span><br><span class="line">    print(&quot;---------data reload over-------&quot;)</span><br><span class="line">    path&#x3D;&quot;LiCHD_jieba_test.txt&quot;</span><br><span class="line">    true&#x3D;[]</span><br><span class="line">    predict&#x3D;[]</span><br><span class="line">    i&#x3D;0</span><br><span class="line">    with open(path, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            i&#x3D;i+1</span><br><span class="line">            if i%1000&#x3D;&#x3D;0:</span><br><span class="line">                print(i)</span><br><span class="line">            line &#x3D; line.strip().split(&quot;\t&quot;)</span><br><span class="line">            question&#x3D;line[0].split(&quot; &quot;)</span><br><span class="line">            answer&#x3D;line[1].split(&quot; &quot;)</span><br><span class="line">            label&#x3D;int(line[2])</span><br><span class="line">            true.append(label)</span><br><span class="line"></span><br><span class="line">            question_emb &#x3D; dictionary.doc2bow(question)</span><br><span class="line">            question_emb_tfidf &#x3D; tfidf_model[question_emb]</span><br><span class="line">            question_vector &#x3D; [x[1] for x in question_emb_tfidf]</span><br><span class="line"></span><br><span class="line">            answer_emb &#x3D; dictionary.doc2bow(answer)</span><br><span class="line">            answer_emb_tfidf &#x3D; tfidf_model[answer_emb]</span><br><span class="line">            answer_vector &#x3D; [x[1] for x in answer_emb_tfidf]</span><br><span class="line"></span><br><span class="line">            sim&#x3D;cos(question_vector,answer_vector)</span><br><span class="line">            if sim&#x3D;&#x3D;None:</span><br><span class="line">                sim&#x3D;0.0</span><br><span class="line">            sim&#x3D;float(sim)</span><br><span class="line">            if sim&gt;0.9:</span><br><span class="line">                predict.append(2)</span><br><span class="line">            elif 0.6&lt;sim&lt;0.9:</span><br><span class="line">                predict.append(1)</span><br><span class="line">            else:</span><br><span class="line">                predict.append(0)</span><br><span class="line"></span><br><span class="line">    s&#x3D;list(range(len(true)))</span><br><span class="line">    flag&#x3D;s[::30]</span><br><span class="line"></span><br><span class="line">    ndcg1_all&#x3D;0</span><br><span class="line">    ndcg5_all &#x3D; 0</span><br><span class="line">    ndcg10_all&#x3D; 0</span><br><span class="line">    mrr_all&#x3D;0</span><br><span class="line">    for i in flag:</span><br><span class="line">        rankedlist&#x3D;(true[i:i+30])</span><br><span class="line">        test_matrix&#x3D;(predict[i:i+30])</span><br><span class="line"></span><br><span class="line">        dcg1 &#x3D; (DCG(rankedlist, test_matrix, 1))</span><br><span class="line">        idcg1 &#x3D; (DCG(rankedlist, rankedlist, 1))</span><br><span class="line">        if idcg1!&#x3D;0:</span><br><span class="line">            ndcg1&#x3D;dcg1&#x2F;idcg1</span><br><span class="line">        else:</span><br><span class="line">            ndcg1&#x3D;0</span><br><span class="line">        ndcg1_all&#x3D;ndcg1_all+ndcg1</span><br><span class="line"></span><br><span class="line">        dcg5 &#x3D; (DCG(rankedlist, test_matrix, 5))</span><br><span class="line">        idcg5 &#x3D; (DCG(rankedlist, rankedlist, 5))</span><br><span class="line"></span><br><span class="line">        if idcg5 !&#x3D; 0:</span><br><span class="line">            ndcg5 &#x3D; dcg5 &#x2F; idcg5</span><br><span class="line">        else:</span><br><span class="line">            ndcg5 &#x3D; 0</span><br><span class="line">        ndcg5_all &#x3D; ndcg5_all + ndcg5</span><br><span class="line"></span><br><span class="line">        dcg10 &#x3D; (DCG(rankedlist, test_matrix, 10))</span><br><span class="line">        idcg10 &#x3D; (DCG(rankedlist, rankedlist, 10))</span><br><span class="line"></span><br><span class="line">        if idcg10 !&#x3D; 0:</span><br><span class="line">            ndcg10 &#x3D; dcg10 &#x2F; idcg10</span><br><span class="line">        else:</span><br><span class="line">            ndcg10 &#x3D; 0</span><br><span class="line"></span><br><span class="line">        ndcg10_all &#x3D; ndcg10_all + ndcg10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        mrr &#x3D; MRR(rankedlist, test_matrix)</span><br><span class="line">        mrr_all&#x3D;mrr_all+mrr</span><br><span class="line"></span><br><span class="line">    print(&quot;ndcg1_all is:&quot;,ndcg1_all&#x2F;len(flag))</span><br><span class="line">    print(&quot;ndcg5_all is:&quot;, ndcg5_all &#x2F; len(flag))</span><br><span class="line">    print(&quot;ndcg10_all is:&quot;, ndcg10_all &#x2F; len(flag))</span><br><span class="line">    print(&quot;mrr_all is:&quot;, mrr_all &#x2F; len(flag))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The results:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ndcg1_all is: 0.12772925764192092</span><br><span class="line">ndcg5_all is: 0.2728327402057438</span><br><span class="line">ndcg10_all is: 0.42472485595031345</span><br><span class="line">mrr_all is: 0.430098479172095</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>No module named &#39;torch._C&#39;</title>
      <link href="2021/01/13/No-module-named-torch-C/"/>
      <url>2021/01/13/No-module-named-torch-C/</url>
      
        <content type="html"><![CDATA[<p>When pip  torch, error: ModuleNotFoundError: No module named ‘torch._C’</p><p>Solution, you should use 64-bit Python instead 32-bit</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Just use: Anaconda3-5.2.0-Windows-x86_64.exe in your x86_64 </span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Five Chinese segmentation tools</title>
      <link href="2021/01/09/Five-Chinese-segmentation-tool/"/>
      <url>2021/01/09/Five-Chinese-segmentation-tool/</url>
      
        <content type="html"><![CDATA[<p>This blog will describe five Chinese segmenation tool, about how to install it, how to use it, the source web,<br>There are: <a href="https://github.com/fxsjy/jieba">jieba</a>, <a href="https://github.com/isnowfy/snownlp">SnowNLP</a>, <a href="https://github.com/lancopku/pkuseg-python">PkuSeg</a>, <a href="https://github.com/thunlp/THULAC-Python">THULAC</a>, <a href="https://github.com/hankcs/pyhanlp">pyhanlp</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#  jieba: pip install jieba</span><br><span class="line">import jieba</span><br><span class="line">seg_list &#x3D; jieba.cut(&quot;我在天安门广场吃炸鸡&quot;)</span><br><span class="line">print(&quot;Default Mode: &quot; + &quot; &quot;.join(seg_list))</span><br><span class="line"></span><br><span class="line"># SnowNLP: https:&#x2F;&#x2F;github.com&#x2F;isnowfy&#x2F;snownlp  #   pip install snownlp</span><br><span class="line">from snownlp import SnowNLP</span><br><span class="line">s &#x3D; SnowNLP(&quot;我在天安门广场吃炸鸡&quot;)</span><br><span class="line">print(&#39; &#39;.join(s.words))</span><br><span class="line"></span><br><span class="line">#  PkuSeg: https:&#x2F;&#x2F;github.com&#x2F;lancopku&#x2F;pkuseg-python #   pip install pkuseg</span><br><span class="line">import pkuseg</span><br><span class="line">pku_seg &#x3D; pkuseg.pkuseg()</span><br><span class="line">print(&#39; &#39;.join(pku_seg.cut(&#39;我在天安门广场吃炸鸡&#39;)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#   THULAC: https:&#x2F;&#x2F;github.com&#x2F;thunlp&#x2F;THULAC-Python   pip install thulac</span><br><span class="line">import thulac</span><br><span class="line">thu_lac &#x3D; thulac.thulac(seg_only&#x3D;True)</span><br><span class="line">thu_result &#x3D; thu_lac.cut(&quot;我在天安门广场吃炸鸡&quot;, text&#x3D;True)</span><br><span class="line">print(&quot;thu_lac&quot;,thu_result)</span><br><span class="line"></span><br><span class="line"># pyhanlp: https:&#x2F;&#x2F;github.com&#x2F;hankcs&#x2F;pyhanlp</span><br><span class="line">from pyhanlp import HanLP</span><br><span class="line"></span><br><span class="line">han_word_seg &#x3D; HanLP.segment(&#39;我在天安门广场吃炸鸡&#39;)</span><br><span class="line">print(&#39; &#39;.join([term.word for term in han_word_seg]))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>edge_softmax in DGL</title>
      <link href="2021/01/08/edge-softmax-in-DGL/"/>
      <url>2021/01/08/edge-softmax-in-DGL/</url>
      
        <content type="html"><![CDATA[<p>When use GATConv in DGL, like:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, graph,feather,config):</span><br><span class="line">      graph&#x3D;graph.to(config.device)</span><br><span class="line">      feather&#x3D;feather.to(config.device)</span><br><span class="line">      # conv &#x3D; GATConv(3, 3, num_heads&#x3D;2,allow_zero_in_degree&#x3D;True).to(config.device)</span><br><span class="line">      conv &#x3D; GATConv(3, 3, num_heads&#x3D;5).to(config.device)</span><br><span class="line">      # conv &#x3D; GraphConv(3, 3,allow_zero_in_degree&#x3D;True).to(config.device)</span><br><span class="line"></span><br><span class="line">      res &#x3D; conv(graph, feather)</span><br><span class="line">      graph.ndata[&quot;x&quot;]&#x3D;F.relu(res)</span><br><span class="line">      cucda_graph&#x3D;graph.to(config.device)</span><br><span class="line"></span><br><span class="line">      hg &#x3D; dgl.mean_nodes(cucda_graph,&quot;x&quot;)</span><br></pre></td></tr></table></figure><p>I want to print the attention weights for the around edges,  by function (3) in <a href="https://docs.dgl.ai/tutorials/models/1_gnn/9_gat.html">Graph attention network-DGL</a>, I note that: softmax: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">  g &#x3D; dgl.DGLGraph()</span><br><span class="line">  g.add_nodes(3)</span><br><span class="line">  g.add_edges([0, 0, 0, 1, 1, 2], [0, 1, 2, 1, 2, 2])</span><br><span class="line">  edata &#x3D; th.ones(6, 1).float()</span><br><span class="line">  print(edata)</span><br><span class="line">    tensor([[1.],</span><br><span class="line">            [1.],</span><br><span class="line">            [1.],</span><br><span class="line">            [1.],</span><br><span class="line">            [1.],</span><br><span class="line">            [1.]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(edge_softmax(g, edata))</span><br><span class="line">    tensor([[1.0000],</span><br><span class="line">        [0.5000],</span><br><span class="line">        [0.3333],</span><br><span class="line">        [0.5000],</span><br><span class="line">        [0.3333],</span><br><span class="line">        [0.3333]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Ok, this graph is directed circle graph, I found that if the graph is<br>emissive graph, like below</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0--------&gt;1</span><br><span class="line">0---------&gt;3</span><br><span class="line">0----------4</span><br></pre></td></tr></table></figure><p>The edge softmax is 1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#emcoding&#x3D;utf-8</span><br><span class="line">import dgl</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;dmlc&#x2F;dgl&#x2F;blob&#x2F;master&#x2F;python&#x2F;dgl&#x2F;nn&#x2F;pytorch&#x2F;conv&#x2F;gatconv.py</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import torch as th</span><br><span class="line">from dgl.nn import GATConv</span><br><span class="line">import networkx as nx</span><br><span class="line">G &#x3D; dgl.DGLGraph()</span><br><span class="line">G.add_nodes(3)</span><br><span class="line">G.add_edges(0, [1, 2])</span><br><span class="line">feat&#x3D;th.ones(3,1)</span><br><span class="line">gatconv&#x3D;GATConv(1,2,num_heads&#x3D;2)</span><br><span class="line">res&#x3D;gatconv(G,feat)</span><br><span class="line">nx.draw(G.to_networkx())</span><br><span class="line">plt.show()</span><br><span class="line"># print(res.size())</span><br></pre></td></tr></table></figure><p>result:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">feat_src torch.Size([3, 2, 2])</span><br><span class="line">e torch.Size([2, 2, 1])</span><br><span class="line">tensor([[0.0303],</span><br><span class="line">        [0.0303]], grad_fn&#x3D;&lt;SumBackward1&gt;)</span><br><span class="line">--------</span><br><span class="line">weight: torch.Size([2, 2, 1])</span><br><span class="line">edge_softmax:</span><br><span class="line">tensor([[[1.],</span><br><span class="line">         [1.]],</span><br><span class="line"></span><br><span class="line">        [[1.],</span><br><span class="line">         [1.]]], grad_fn&#x3D;&lt;EdgeSoftmaxBackward&gt;)</span><br></pre></td></tr></table></figure><p>We can not let the attention weight all is 1.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Text unicode normalization</title>
      <link href="2021/01/05/Text-unicode-normalization/"/>
      <url>2021/01/05/Text-unicode-normalization/</url>
      
        <content type="html"><![CDATA[<p><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c02/p09_normalize_unicode_text_to_regexp.html">Unicode normalization in text</a></p><p>Input: the SetÃºbal District belong to?<br>Output: the seta bal district belong to ?</p><p>Code:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import re</span><br><span class="line">import unicodedata</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def normalizeString(s):</span><br><span class="line">  s &#x3D; unicodeToAscii(s.lower().strip())</span><br><span class="line">  s &#x3D; re.sub(r&quot;([.!?])&quot;, r&quot; \1&quot;, s)</span><br><span class="line">  s &#x3D; re.sub(r&quot;[^a-zA-Z.!?]+&quot;, r&quot; &quot;, s)</span><br><span class="line">  return s</span><br><span class="line"></span><br><span class="line">def unicodeToAscii(s):</span><br><span class="line">  return &#39;&#39;.join(</span><br><span class="line">    c for c in unicodedata.normalize(&#39;NFD&#39;, s)</span><br><span class="line">    if unicodedata.category(c) !&#x3D; &#39;Mn&#39;</span><br><span class="line">  )</span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">  fw1&#x3D;open(&quot;wee.txt&quot;,&quot;w&quot;,encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">  # sd&#x3D;GetEntitiesByQuery(&#39;What country sharing borders with Spain does the SetÃºbal District belong to?&#39;)</span><br><span class="line">  sd&#x3D;&quot;What country sharing borders with Spain does the SetÃºbal District belong to?&quot;</span><br><span class="line">  a&#x3D;normalizeString(sd)</span><br><span class="line">  print(a)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Oh! Dexter2 for Entity Linking</title>
      <link href="2021/01/04/Oh-Dexter2-for-Entity-Linking/"/>
      <url>2021/01/04/Oh-Dexter2-for-Entity-Linking/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/dexter/dexter">Dexter</a> is a tool about entity linking, in this blog, I will discribe how to install it and get the entity mention in a sentence, Ok, let’s roll!!!</p><h1 id="Install-Dexter"><a href="#Install-Dexter" class="headerlink" title="Install Dexter"></a>Install Dexter</h1><p>You just install in in your linux server by these commands:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;hpc.isti.cnr.it&#x2F;~ceccarelli&#x2F;dexter2.tar.gz</span><br><span class="line">tar -xvzf dexter2.tar.gz</span><br><span class="line">cd dexter2</span><br><span class="line">java -Xmx4000m -jar dexter-2.1.0.jar</span><br></pre></td></tr></table></figure><p>The dir of dexter2:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">libs</span><br><span class="line">en-model-20140707</span><br><span class="line">dexter-conf.xml</span><br><span class="line">dexter-2.1.0.jar</span><br></pre></td></tr></table></figure><p>And then visit on your browser the address <a href="http://localhost:8080/dexter-webapp/dev">http://localhost:8080/dexter-webapp/dev</a>. It will show the available REST-API. Enjoy!</p><h1 id="Let-us-write-a-code-to-get-the-entity-mentions-in-a-sentence"><a href="#Let-us-write-a-code-to-get-the-entity-mentions-in-a-sentence" class="headerlink" title="Let us write a code to get the entity mentions in a sentence"></a>Let us write a code to get the entity mentions in a sentence</h1><p>In the sentence: “What country in the United Kingdom contains the Down District Council?”, the entity mentions are “down district council” and “united kingdom</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import urllib</span><br><span class="line">from urllib import request</span><br><span class="line">from urllib import parse</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def GetAnnotateUrl(query, n &#x3D; 5, conf &#x3D; 0.5):</span><br><span class="line">  url &#x3D; &#39;http:&#x2F;&#x2F;localhost:8080&#x2F;dexter-webapp&#x2F;api&#x2F;rest&#x2F;spot?text&#x3D;&#39;</span><br><span class="line">  query &#x3D; query.replace(&#39; &#39;, &#39;%20&#39;)</span><br><span class="line">  url +&#x3D; query</span><br><span class="line">  url +&#x3D; (&#39;&amp;n&#x3D;&#39; + str(n))</span><br><span class="line">  url +&#x3D; (&#39;&amp;min-conf&#x3D;&#39; + str(conf))</span><br><span class="line">  url +&#x3D; &#39;&amp;wn&#x3D;false&amp;debug&#x3D;false&amp;format&#x3D;text&#39;</span><br><span class="line">  return url</span><br><span class="line"></span><br><span class="line">def GetId2EntityUrl(id):</span><br><span class="line">  url &#x3D; &#39;http:&#x2F;&#x2F;localhost:8080&#x2F;dexter-webapp&#x2F;api&#x2F;rest&#x2F;get-desc?title-only&#x3D;true&amp;id&#x3D;&#39;</span><br><span class="line">  url +&#x3D; str(id)</span><br><span class="line">  return url</span><br><span class="line"></span><br><span class="line">def GetRequest(url):</span><br><span class="line">  req &#x3D; request.Request(url)</span><br><span class="line">  data &#x3D; request.urlopen(req).read().decode(&#39;utf-8&#39;)</span><br><span class="line">  Json &#x3D; json.loads(data)</span><br><span class="line">  return Json</span><br><span class="line"></span><br><span class="line">def GetEntitiesByQuery(query, n &#x3D; 5, conf &#x3D; 0.5 ):</span><br><span class="line">  url &#x3D; GetAnnotateUrl(query, n, conf)</span><br><span class="line"></span><br><span class="line">  AnnoData &#x3D; GetRequest(url)</span><br><span class="line">  Spots &#x3D; AnnoData[&#39;spots&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  # Entities &#x3D; &#123;&#125;</span><br><span class="line">  for session in Spots:</span><br><span class="line">    print(session[&quot;mention&quot;])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">  GetEntitiesByQuery(&#39;What country in the United Kingdom contains the Down District Council?&#39;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Summary for My 2020</title>
      <link href="2021/01/01/Summary%20for%20My%202020/"/>
      <url>2021/01/01/Summary%20for%20My%202020/</url>
      
        <content type="html"><![CDATA[<p>2020 is an important year for me, thanks for my advisor allow me to get into his group in GSU, US dream I always pursue. My research career officially started! Fuck Covid 19, I have witnessed innocent people losing their lives. I just want to this word become better and better, no virus, no power struggle, no war, no arms. </p><p>To my futher!! Cheers~</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AttributeError: &#39;Tensor&#39; object has no attribute &#39;bool&#39;</title>
      <link href="2020/12/31/AttributeError-Tensor-object-has-no-attribute-bool/"/>
      <url>2020/12/31/AttributeError-Tensor-object-has-no-attribute-bool/</url>
      
        <content type="html"><![CDATA[<p>“AttributeError: ‘’Tensor’’ object has no attribute ‘’bool’’’, this is a problem about your torch version, when I reproduce the transformer model about encoder to decoder, I met this issue in my win, but it is good in linux. My torch version in win is torch-1.2.0.</p><p>Solution: </p><p>Please update your torch version, after updating, my torch version is torch-1.3.0, download it from <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/win-64/">Index of /anaconda/cloud/pytorch/win-64/</a></p><p>And then,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch-1.3.0-py3.6_cpu_0.tar.bz2</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Why I cannot load m30k_deen_shr.pkl</title>
      <link href="2020/12/28/Why-I-cannot-load-m30k-deen-shr-pkl/"/>
      <url>2020/12/28/Why-I-cannot-load-m30k-deen-shr-pkl/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">jadore801120-attention-is-all-you-need-pytorch</a><br>When I reproduce the MT model by transformer encoder and decoder. I download the “m30k_deen_shr.pkl” from server to win, I found that I can not read it by   pickle or dill, </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import dill</span><br><span class="line"># import pickle</span><br><span class="line">path&#x3D;r&quot;G:\1_Project_Li\4_2020_query_logical_answers\torch_transformers_seq2seq\Attention_seq2seq\m30k_deen_shr.pkl&quot;</span><br><span class="line">data &#x3D; dill.load(open(&quot;m30k_deen_shr.pkl&quot;, &#39;rb&#39;),encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><p>Soluction: you just generate m30k_deen_shr.pkl by pregress.py again in your Win.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Loss is Nan when training the DL model</title>
      <link href="2020/12/25/Loss-is-Nan-when-training-the-DL-model/"/>
      <url>2020/12/25/Loss-is-Nan-when-training-the-DL-model/</url>
      
        <content type="html"><![CDATA[<p>When I training a text matching model, I meet an error, the loss is nan, so, the solution is that:<br>Some blogs alter the learning rate, down the learning rate. But I know in my project, what happened.<br>When I train and eval the model, I define the different weights for attention mechanism, this will lead model can not learn the effective parameters. So it’s wrong.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Welcome (#SMM4H) Workshop and Shared Task 2021</title>
      <link href="2020/12/22/Welcome-SMM4H-Workshop-and-Shared-Task-2021/"/>
      <url>2020/12/22/Welcome-SMM4H-Workshop-and-Shared-Task-2021/</url>
      
        <content type="html"><![CDATA[<p><a href="https://healthlanguageprocessing.org/smm4h-2021/">Welcome (#SMM4H) Workshop and Shared Task 2021</a></p><p>The Social Media Mining for Health Applications (#SMM4H) workshop serves as a venue for bringing together researchers interested in automatic methods for the collection, extraction, representation, analysis, and validation of social media data (e.g., Twitter, Facebook) for health informatics. The 6th #SMM4H Workshop, co-located at NAACL 2021, invites the submission of papers on original.</p><p>Organizers</p><p>Graciela Gonzalez-Hernandez, University of Pennsylvania, USA</p><p>Davy Weissenbacher, University of Pennsylvania, USA</p><p>Ari Z. Klein, University of Pennsylvania, USA</p><p>Karen O’Connor, University of Pennsylvania, USA</p><p>Abeed Sarker, Emory University, USA</p><p>Elena Tutubalina, Kazan Federal University, Russia</p><p>Zulfat Miftahutdinov, Kazan Federal University, Russia</p><p>Ilsear Alimova, Kazan Federal University, Russia</p><p>Martin Krallinger, Barcelona Supercomputing Center, Spain</p><p>Juan Banda, Georgia State University, USA</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>NDCG and MRR in matchzoo</title>
      <link href="2020/12/21/NDCG-and-MRR-in-matchzoo/"/>
      <url>2020/12/21/NDCG-and-MRR-in-matchzoo/</url>
      
        <content type="html"><![CDATA[<p>This blog will analyies the source code about NDCG and MRR in matchzoo</p><h4 id="NDCG"><a href="#NDCG" class="headerlink" title="NDCG"></a>NDCG</h4><p>you can refer <a href="https://www.cnblogs.com/by-dream/p/9403984.html">搜索评价指标——NDCG</a>,<br>for the source code, you can do it by:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">y_true &#x3D; [2,1,1,0,0,0]</span><br><span class="line">y_pred &#x3D; [2,2,0,2,2,0]</span><br><span class="line">def change_(labels, scores):</span><br><span class="line">    couple &#x3D; list(zip(labels, scores))</span><br><span class="line">    return np.array(sorted(couple, key&#x3D;lambda x: x[1], reverse&#x3D;True))</span><br><span class="line"></span><br><span class="line"># print(change_(y_true,y_pred))</span><br><span class="line"></span><br><span class="line">def DCG(y_true, y_pred,k,_threshold&#x3D;0):</span><br><span class="line"></span><br><span class="line">    coupled_pair &#x3D; change_(y_true, y_pred)</span><br><span class="line">    print(coupled_pair)</span><br><span class="line">    result &#x3D; 0.</span><br><span class="line">    for i, (label, score) in enumerate(coupled_pair):</span><br><span class="line">        print(label)</span><br><span class="line">        if i &gt;&#x3D; k:</span><br><span class="line">            break</span><br><span class="line">        if label &gt; _threshold:</span><br><span class="line">            result +&#x3D; (math.pow(2., label) - 1.) &#x2F; math.log(2. + i)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">dcg&#x3D;(DCG(y_true,y_pred,3))</span><br><span class="line">idcg&#x3D;(DCG(y_true,y_true,3))</span><br><span class="line">print(dcg&#x2F;idcg)</span><br></pre></td></tr></table></figure><p>result:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">DCG:</span><br><span class="line">[[2 2]</span><br><span class="line"> [1 2]</span><br><span class="line"> [0 2]</span><br><span class="line"> [0 2]</span><br><span class="line"> [1 0]</span><br><span class="line"> [0 0]]</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">0</span><br><span class="line">0</span><br><span class="line">-------------------------------</span><br><span class="line">IDCG: </span><br><span class="line">[[2 2]</span><br><span class="line"> [1 1]</span><br><span class="line"> [1 1]</span><br><span class="line"> [0 0]</span><br><span class="line"> [0 0]</span><br><span class="line"> [0 0]]</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">0</span><br><span class="line">results: 0.878961873034099</span><br></pre></td></tr></table></figure><p>Of cource, you can also get the same result by matchzoo toolkit, but you should pip it in the first,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matchzoo as mz</span><br><span class="line"></span><br><span class="line">y_true &#x3D; [2,1,1,0,0,0]</span><br><span class="line">y_pred &#x3D; [2,2,0,2,2,0]</span><br><span class="line">ndcg &#x3D; mz.metrics.NormalizedDiscountedCumulativeGain</span><br><span class="line">dcg&#x3D; mz.metrics.DiscountedCumulativeGain</span><br><span class="line">s3&#x3D;round(ndcg(k&#x3D;3)(y_true, y_pred), 3)</span><br><span class="line">print(s3)#  0.879</span><br><span class="line"></span><br><span class="line">s4&#x3D;round(dcg(k&#x3D;3)(y_true, y_pred), 3)</span><br><span class="line">print(s4) # 5.238</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="MRR"><a href="#MRR" class="headerlink" title="MRR"></a>MRR</h4><p>source code:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">y_true &#x3D; [2,1,1,0,0,0]</span><br><span class="line">y_pred &#x3D; [2,2,0,2,2,0]</span><br><span class="line">def change_(labels, scores):</span><br><span class="line">    couple &#x3D; list(zip(labels, scores))</span><br><span class="line">    return np.array(sorted(couple, key&#x3D;lambda x: x[1], reverse&#x3D;True))</span><br><span class="line"></span><br><span class="line">def MRR(y_true, y_pred,_threshold&#x3D;0):</span><br><span class="line">    coupled_pair &#x3D; change_(y_true, y_pred)</span><br><span class="line">    for idx, (label, pred) in enumerate(coupled_pair):</span><br><span class="line">        if label &gt; _threshold:</span><br><span class="line">            return 1. &#x2F; (idx + 1)</span><br><span class="line">    return 0.</span><br><span class="line"></span><br><span class="line">mrr&#x3D;MRR(y_true,y_pred)</span><br><span class="line">print(mrr): 1.0</span><br></pre></td></tr></table></figure><p>matchzoo:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import matchzoo as mz</span><br><span class="line"></span><br><span class="line">y_true &#x3D; [2,1,1,0,0,0]</span><br><span class="line">y_pred &#x3D; [2,2,0,2,2,0]</span><br><span class="line">mrr&#x3D;mz.metrics.MeanReciprocalRank</span><br><span class="line">s5&#x3D;mrr()(y_true, y_pred)</span><br><span class="line">print(s5)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Review the MatchZoo in Chinese Text matching</title>
      <link href="2020/12/20/Review-the-MatchZoo-in-Chinese-Text-matching/"/>
      <url>2020/12/20/Review-the-MatchZoo-in-Chinese-Text-matching/</url>
      
        <content type="html"><![CDATA[<p>Refer my node: <a href="https://iamlimingchen.com/2019/11/06/CMQA-1/">Realize a semantic matching model (Chinese) with MatchZoo.</a></p><p>(1)-Pip install matchzoo in <a href="https://github.com/NTMC-Community/MatchZoo">MatchZoo</a>.</p><p>(2)-Reload jieba in your file:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Linux: &#x2F;home&#x2F;**&#x2F;anaconda3&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;matchzoo&#x2F;</span><br><span class="line">E:\bao\anaconda_bao\Aanconda3\Lib\site-packages\matchzoo\preprocessors\units\tokenize.py</span><br></pre></td></tr></table></figure><p>By:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def transform(self, input_: str) -&gt; list:</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line">       Process input data from raw terms to list of tokens.</span><br><span class="line"></span><br><span class="line">       :param input_: raw textual input.</span><br><span class="line"></span><br><span class="line">       :return tokens: tokenized tokens as a list.</span><br><span class="line">       &quot;&quot;&quot;</span><br><span class="line">       return jieba.lcut(input_)</span><br><span class="line">       # return nltk.word_tokenize(input_)</span><br></pre></td></tr></table></figure><p>(3) Reload you Chinese pre-trained embedding by adding code in your</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\Anaconda3\Lib\site-packages\MatchZoo-2.2.0-py3.6.egg\matchzoo\datasets\embeddings\__init__.py</span><br></pre></td></tr></table></figure><p>By:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from pathlib import Path</span><br><span class="line">from .load_glove_embedding import load_glove_embedding</span><br><span class="line"></span><br><span class="line">DATA_ROOT &#x3D; Path(__file__).parent</span><br><span class="line">EMBED_RANK &#x3D; DATA_ROOT.joinpath(&#39;embed_rank.txt&#39;)</span><br><span class="line">EMBED_10 &#x3D; DATA_ROOT.joinpath(&#39;embed_10_word2vec.txt&#39;)</span><br><span class="line">EMBED_10_GLOVE &#x3D; DATA_ROOT.joinpath(&#39;embed_10_glove.txt&#39;)</span><br><span class="line">EMBED_CPWS &#x3D; DATA_ROOT.joinpath(&#39;G:\Data\Chinese_vector\sgns.baidubaike.txt&#39;)</span><br></pre></td></tr></table></figure><p>I used <a href="https://github.com/Embedding/Chinese-Word-Vectors">sgns.baidubaike.txt</a>(1.69G) which is trained by word2vec.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pip install matchzoo: error OSError: [WinError 17]..</title>
      <link href="2020/12/20/pip-install-matchzoo-error-OSError-WinError-17/"/>
      <url>2020/12/20/pip-install-matchzoo-error-OSError-WinError-17/</url>
      
        <content type="html"><![CDATA[<p>When I used “pip install matchzoo” in windows, I meet an error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Exception:</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;E:\bao\anaconda_bao\Aanconda3\lib\shutil.py&quot;, line 544, in move</span><br><span class="line">    os.rename(src, real_dst)</span><br><span class="line">OSError: [WinError 17] 系统无法将文件移到不同的磁盘驱动器。: &#39;e:\\bao\\anaconda_bao\\aanconda3\\lib\\site-packages\\pandas&#39; -&gt; &#39;C:\\Users\\mcli2\\AppData\\Local\\Temp\\pip-eus3vsmp-uninstall\\bao\\anaconda_bao\\aanconda3\\lib\\site-packages\\pandas</span><br></pre></td></tr></table></figure><p>Solution:<br>Just go to C:\Windows\System32, run cmd.exe in Administrator, run: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U matchzoo --user</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Recall and Precision at k for Recommender Systems</title>
      <link href="2020/12/20/Recall-and-Precision-at-k-for-Recommender-Systems/"/>
      <url>2020/12/20/Recall-and-Precision-at-k-for-Recommender-Systems/</url>
      
        <content type="html"><![CDATA[<p>Reload the tutorial <a href="https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54">Recall and Precision at k for Recommender Systems</a></p><p>Precision: Precision at k is the proportion of recommended items in the top-k set that are relevant.</p><p>Recall: Recall at k is the proportion of relevant items found in the top-k recommendations</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Error: OSError: libcublas.so.10 when install dgl-cu101</title>
      <link href="2020/12/17/Error-OSError-libcublas-so-10-when-install-dgl-cu101/"/>
      <url>2020/12/17/Error-OSError-libcublas-so-10-when-install-dgl-cu101/</url>
      
        <content type="html"><![CDATA[<p>When install DGL-GPU, error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: libcublas.so.10: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure><p>1: check your cuda version</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc --version</span><br></pre></td></tr></table></figure><p>result:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2019 NVIDIA Corporation</span><br><span class="line">Built on Fri_Feb__8_19:08:17_PST_2019</span><br><span class="line">Cuda compilation tools, release 10.1, V10.1.105</span><br></pre></td></tr></table></figure><p>2: <a href="https://docs.dgl.ai/en/0.4.x/install/">How to install DGL</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install dgl           # For CPU Build</span><br><span class="line">pip install dgl-cu90      # For CUDA 9.0 Build</span><br><span class="line">pip install dgl-cu92      # For CUDA 9.2 Build</span><br><span class="line">pip install dgl-cu100     # For CUDA 10.0 Build</span><br><span class="line">pip install dgl-cu101     # For CUDA 10.1 Build</span><br></pre></td></tr></table></figure><p>If you want to use conda to install it, your python may be undated to the latest version (I do not recommand it)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda install -c dglteam dgl              # For CPU Build</span><br><span class="line">conda install -c dglteam dgl-cuda9.0      # For CUDA 9.0 Build</span><br><span class="line">conda install -c dglteam dgl-cuda10.0     # For CUDA 10.0 Build</span><br><span class="line">conda install -c dglteam dgl-cuda10.1     # For CUDA 10.1 Build</span><br><span class="line">conda install -c dglteam dgl-cuda10.2     # For CUDA 10.2 Build</span><br></pre></td></tr></table></figure><p>3: <a href="https://github.com/dmlc/dgl/issues/1425">Solve this issues</a>:</p><p>Just</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c anaconda cudatoolkit&#x3D;10.1</span><br></pre></td></tr></table></figure><p>4: <a href="https://docs.dgl.ai/en/latest/guide_cn/graph-gpu.html">Use DGLGraph in GPU</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Nodes order in DGL.batch</title>
      <link href="2020/12/16/Nodes-order-in-DGL-batch/"/>
      <url>2020/12/16/Nodes-order-in-DGL-batch/</url>
      
        <content type="html"><![CDATA[<p>Ok, there is a good question, listen:</p><p>There are two graph, each graph has two nodes: graph1 (node1.1, node1.2), graph2: (node2.1, node2.2), by dgl batch, we can find, these two graphs will become a big gragh, it has four nodes (node3.1, node3.2,node3.3, node3.4), we should know are they in order? node2.1?= node3.3.</p><p>Evaluation:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import dgl</span><br><span class="line">import torch as th</span><br><span class="line"></span><br><span class="line">G1 &#x3D; dgl.DGLGraph()</span><br><span class="line">G1.add_nodes(2)</span><br><span class="line">G1.ndata[&quot;x&quot;]&#x3D;th.tensor([[1.],[2.]])</span><br><span class="line">print(G1.ndata[&quot;x&quot;][0])</span><br><span class="line">print(G1.ndata[&quot;x&quot;][1])</span><br><span class="line"></span><br><span class="line">G2 &#x3D; dgl.DGLGraph()</span><br><span class="line">G2.add_nodes(2)</span><br><span class="line">G2.ndata[&quot;x&quot;]&#x3D;th.tensor([[4.],[5.]])</span><br><span class="line">print(G2.ndata[&quot;x&quot;][0])</span><br><span class="line">print(G2.ndata[&quot;x&quot;][1])</span><br><span class="line"></span><br><span class="line">print(&quot;----------&quot;)</span><br><span class="line">bg &#x3D; dgl.batch([G1, G2])</span><br><span class="line">for i in range(4):</span><br><span class="line">    print(bg.ndata[&quot;x&quot;][i])</span><br></pre></td></tr></table></figure><p>By this, we can see the results:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([1.])</span><br><span class="line">tensor([2.])</span><br><span class="line">tensor([4.])</span><br><span class="line">tensor([5.])</span><br><span class="line">----------</span><br><span class="line">tensor([1.])</span><br><span class="line">tensor([2.])</span><br><span class="line">tensor([4.])</span><br><span class="line">tensor([5.])</span><br></pre></td></tr></table></figure><p>So node2.1== node3.3</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Value assignment in graph by DGL</title>
      <link href="2020/12/16/value-assignment-in-graph-by-DGL/"/>
      <url>2020/12/16/value-assignment-in-graph-by-DGL/</url>
      
        <content type="html"><![CDATA[<p>We have calculated the value of node, we should use it to initialize the node, DGL has given us the answers:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import dgl</span><br><span class="line">import torch as th</span><br><span class="line">import dgl</span><br><span class="line">import torch as th</span><br><span class="line">g &#x3D; dgl.DGLGraph()</span><br><span class="line">g.add_nodes(10)</span><br><span class="line"># A couple edges one-by-one</span><br><span class="line">for i in range(1, 4):</span><br><span class="line">    g.add_edge(i, 0)</span><br><span class="line"></span><br><span class="line">x &#x3D; th.randn(10, 3)</span><br><span class="line">g.ndata[&#39;x&#39;] &#x3D; x</span><br><span class="line">g.ndata[&#39;x&#39;][0] &#x3D; th.zeros(1, 3)</span><br><span class="line">print(g.ndata[&#39;x&#39;])</span><br></pre></td></tr></table></figure><p>We can see that the results, the first line is 0, 0, 0:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0000,  0.0000,  0.0000],</span><br><span class="line">        [-0.4050, -0.1420,  0.4302],</span><br><span class="line">        [-0.6895, -1.7041, -0.6472],</span><br><span class="line">        [ 2.4805,  0.9508, -1.0570],</span><br><span class="line">        [ 0.2429, -0.3533,  0.4157],</span><br><span class="line">        [-0.8748,  1.1515, -0.0763],</span><br><span class="line">        [ 0.2340, -1.6076, -1.9350],</span><br><span class="line">        [-0.7349,  1.9511, -0.3561],</span><br><span class="line">        [-2.0050,  0.1554,  0.6282],</span><br><span class="line">        [ 2.5418,  0.8958, -0.6219]])</span><br><span class="line"></span><br><span class="line">&#96;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DGL batch &amp; how to define your data</title>
      <link href="2020/12/15/DGL-batch-how-to-define-your-data/"/>
      <url>2020/12/15/DGL-batch-how-to-define-your-data/</url>
      
        <content type="html"><![CDATA[<p>Ok, in <a href="https://docs.dgl.ai/en/0.4.x/tutorials/basics/4_batch.html">Graph Classification Tutorial</a> we can learn how to use DGL.batch to form a graph mini-batch in the public datasett MiniGCDataset, this data includes eight types of topologies, in this blog I will show how to define your own data.</p><h3 id="Define-your-own-data"><a href="#Define-your-own-data" class="headerlink" title="Define your own data"></a>Define your own data</h3><p>In the first we should add node and edges to your graph, as shown:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import dgl</span><br><span class="line">G &#x3D; dgl.DGLGraph()</span><br><span class="line">G.add_nodes(10)</span><br><span class="line">G.add_edges(0, [1,2,3,4,5,6,7, 8, 9])</span><br></pre></td></tr></table></figure><p>Yes this is graph, the center node is number 0, the nodes [2,3,4,5,6,7,8,9] around node 0.</p><p>And then, we will consider: when we train the data, we always progress the data in a batch, torch or tensorflow, so we should let machine to progress the graph data in a batch:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">all_G&#x3D;[]</span><br><span class="line">for i in range(3):</span><br><span class="line"></span><br><span class="line">    G &#x3D; dgl.DGLGraph()</span><br><span class="line">    G.add_nodes(10)</span><br><span class="line">    G.add_edges(0, [1, 2, 3, 4, 5, 6, 7, 8, 9])</span><br><span class="line">    all_G.append(G)</span><br><span class="line"></span><br><span class="line">batched_graph &#x3D; dgl.batch(all_G)</span><br></pre></td></tr></table></figure><p>So, the results, we can see that:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">this is the all_G results:</span><br><span class="line"></span><br><span class="line">[DGLGraph(num_nodes&#x3D;10, num_edges&#x3D;9,</span><br><span class="line">         ndata_schemes&#x3D;&#123;&#125;</span><br><span class="line">         edata_schemes&#x3D;&#123;&#125;), DGLGraph(num_nodes&#x3D;10, num_edges&#x3D;9,</span><br><span class="line">         ndata_schemes&#x3D;&#123;&#125;</span><br><span class="line">         edata_schemes&#x3D;&#123;&#125;), DGLGraph(num_nodes&#x3D;10, num_edges&#x3D;9,</span><br><span class="line">         ndata_schemes&#x3D;&#123;&#125;</span><br><span class="line">         edata_schemes&#x3D;&#123;&#125;)]</span><br><span class="line"></span><br><span class="line">this is the batched_graph results:</span><br><span class="line">DGLGraph(num_nodes&#x3D;30, num_edges&#x3D;27,</span><br><span class="line">         ndata_schemes&#x3D;&#123;&#125;</span><br><span class="line">         edata_schemes&#x3D;&#123;&#125;)</span><br></pre></td></tr></table></figure><p>Ok let us define the GCN model, and let the data input this model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">feat &#x3D; th.ones(30, 10)</span><br><span class="line">conv &#x3D; GraphConv(10, 2, norm&#x3D;&#39;both&#39;, weight&#x3D;True, bias&#x3D;True)</span><br><span class="line">res&#x3D;conv(batched_graph,feat)</span><br><span class="line">#res.size: [30,2]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Let’s average the node represenation,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batched_graph.ndata[&#39;h&#39;] &#x3D; res</span><br><span class="line"></span><br><span class="line">hg &#x3D; dgl.mean_nodes(batched_graph, &#39;h&#39;)</span><br><span class="line">the size of hg is [3,2]</span><br></pre></td></tr></table></figure><p>Let me think, the size of the conv output is [30,2], why the output of hg is [3,2], becauce there are 3 graphs in this big graph, each graph inculdes 10 nodes, so we should calculate the average value about them, so that results.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">--------------FULL -script-----------</span><br><span class="line">from dgl.data import MiniGCDataset</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import networkx as nx</span><br><span class="line">from dgl.nn.pytorch import GraphConv</span><br><span class="line">import dgl</span><br><span class="line">import torch as th</span><br><span class="line"># g &#x3D; dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))</span><br><span class="line">G &#x3D; dgl.DGLGraph()</span><br><span class="line">G.add_nodes(10)</span><br><span class="line">G.add_edges(0, [1,2,3,4,5,6,7, 8, 9])</span><br><span class="line"># dataset &#x3D; MiniGCDataset(80, 10, 20)</span><br><span class="line"># graph, label &#x3D; dataset[0]</span><br><span class="line"># print(G)</span><br><span class="line"># fig, ax &#x3D; plt.subplots()</span><br><span class="line"># nx.draw(G.to_networkx(), ax&#x3D;ax)</span><br><span class="line"># # ax.set_title(&#39;Class: &#123;:d&#125;&#39;.format(label))</span><br><span class="line"># plt.show()</span><br><span class="line">all_G&#x3D;[]</span><br><span class="line">for i in range(3):</span><br><span class="line"></span><br><span class="line">    G &#x3D; dgl.DGLGraph()</span><br><span class="line">    G.add_nodes(10)</span><br><span class="line">    G.add_edges(0, [1, 2, 3, 4, 5, 6, 7, 8, 9])</span><br><span class="line">    all_G.append(G)</span><br><span class="line"></span><br><span class="line">batched_graph &#x3D; dgl.batch(all_G)</span><br><span class="line"></span><br><span class="line">fig, ax &#x3D; plt.subplots()</span><br><span class="line">nx.draw(batched_graph.to_networkx(), ax&#x3D;ax)</span><br><span class="line"># ax.set_title(&#39;Class: &#123;:d&#125;&#39;.format(label))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">s&#x3D;batched_graph.in_degrees()</span><br><span class="line">feat &#x3D; th.ones(30, 10)</span><br><span class="line">conv &#x3D; GraphConv(10, 2, norm&#x3D;&#39;both&#39;, weight&#x3D;True, bias&#x3D;True)</span><br><span class="line">res&#x3D;conv(batched_graph,feat)</span><br><span class="line">batched_graph.ndata[&#39;h&#39;] &#x3D; res</span><br><span class="line"></span><br><span class="line">hg &#x3D; dgl.mean_nodes(batched_graph, &#39;h&#39;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Of cource, you can also use “from dgl.nn.pytorch import GATConv”, just by:<br>For full, you can refer <a href="https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html">Graph attention network in DGL</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv &#x3D; GATConv(10, 2, num_heads&#x3D;2)</span><br></pre></td></tr></table></figure><h3 id="DGL-batch-and-mean-nodes"><a href="#DGL-batch-and-mean-nodes" class="headerlink" title="DGL batch and   mean_nodes"></a>DGL batch and   mean_nodes</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import dgl</span><br><span class="line">import torch as th</span><br><span class="line">g1&#x3D;dgl.DGLGraph()</span><br><span class="line">g1.add_nodes(2)</span><br><span class="line">g1.ndata[&quot;h&quot;]&#x3D;th.tensor([[1.],[2.]])</span><br><span class="line">g1.ndata[&#39;w&#39;]&#x3D;th.tensor([[3.],[6.]])</span><br><span class="line"># print(g1)</span><br><span class="line">g2 &#x3D; dgl.DGLGraph()</span><br><span class="line">g2.add_nodes(3)</span><br><span class="line">g2.ndata[&#39;h&#39;] &#x3D; th.tensor([[1.], [2.], [3.]])</span><br><span class="line">bg &#x3D; dgl.batch([g1, g2], node_attrs&#x3D;&#39;h&#39;)</span><br><span class="line"></span><br><span class="line">s&#x3D;dgl.mean_nodes(bg, &#39;h&#39;)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">    print(s)</span><br><span class="line">    tensor([[1.5000],    # (1 + 2) &#x2F; 2</span><br><span class="line">            [2.0000]])   # (1 + 2 + 3) &#x2F; 3</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">dgl.mean_nodes(g1, &#39;h&#39;, &#39;w&#39;)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"> dgl.mean_nodes(g1, &#39;h&#39;, &#39;w&#39;) # h1 * (w1 &#x2F; (w1 + w2)) + h2 * (w2 &#x2F; (w1 + w2))</span><br><span class="line">    tensor([[1.6667]])               # 1 * (3 &#x2F; (3 + 6)) + 2 * (6 &#x2F; (3 + 6))</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Doc for GCN</title>
      <link href="2020/12/14/Doc-for-GCN/"/>
      <url>2020/12/14/Doc-for-GCN/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/89503068">1-different function for GCN</a></p><p><a href="https://zhuanlan.zhihu.com/p/132497231">2-Graph Attention Network (GAT)</a></p><p><a href="https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html">DGL-GAT</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>OSError: [E050] Can&#39;t find model &#39;de&#39;</title>
      <link href="2020/12/07/OSError-E050-Can-t-find-model-de/"/>
      <url>2020/12/07/OSError-E050-Can-t-find-model-de/</url>
      
        <content type="html"><![CDATA[<h3 id="OSError-E050"><a href="#OSError-E050" class="headerlink" title="OSError: [E050]"></a>OSError: [E050]</h3><p>Maybe, you can use your spacy language model in your Linux, by download this model, by these commands:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install spacy </span><br><span class="line">python -m spacy download en</span><br><span class="line">python -m spacy download de</span><br></pre></td></tr></table></figure><p>But if you can not install it in your window, how should we do? the error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import spacy</span><br><span class="line">src_lang_model &#x3D; spacy.load(&quot;de&quot;)</span><br><span class="line"></span><br><span class="line">OSError: [E050] Can&#39;t find model &#39;de&#39;. It doesn&#39;t seem to be a shortcut link, a Python package or a valid path to a data directory.</span><br></pre></td></tr></table></figure><p>Solution:</p><p>In the first, you should dowload the “de_core_news_sm-2.1.0.tar” and “en_core_web_sm-2.1.0.tar”, from <a href="https://drive.google.com/drive/folders/1AsgxLc0D3lDEpbuBJBpz-t8J6Q731K8F?usp=sharing">Here</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import spacy</span><br><span class="line">src_lang_model &#x3D; spacy.load(r&quot;G:\bao\en_core_web_sm-2.1.0.tar\dist\en_core_web_sm-2.1.0\en_core_web_sm\en_core_web_sm-2.1.0&quot;)</span><br><span class="line"></span><br><span class="line">this file must contrain the tokenizer file.</span><br></pre></td></tr></table></figure><h3 id="ModuleNotFoundError-No-module-named-‘builtin‘"><a href="#ModuleNotFoundError-No-module-named-‘builtin‘" class="headerlink" title="ModuleNotFoundError: No module named ‘builtin‘"></a>ModuleNotFoundError: No module named ‘<strong>builtin</strong>‘</h3><p>When</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pickle</span><br><span class="line">data &#x3D; pickle.load(open(&quot;m30k_deen_shr.pkl&quot;, &#39;rb&#39;),encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">print(data)</span><br><span class="line"></span><br><span class="line">error: ModuleNotFoundError: No module named &#39;__builtin__&#39;</span><br></pre></td></tr></table></figure><p>you just:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import dill</span><br><span class="line">data &#x3D; dill.load(open(&quot;m30k_deen_shr.pkl&quot;, &#39;rb&#39;),encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">print(data)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>OSError _torchtext.so in torchtext</title>
      <link href="2020/12/06/OSError-torchtext-so-in-torchtext/"/>
      <url>2020/12/06/OSError-torchtext-so-in-torchtext/</url>
      
        <content type="html"><![CDATA[<p>My torch version is 1.5.1+cu101, when I used torchtext to<br>progress data, I meet an error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: &#x2F;home&#x2F;mli&#x2F;anaconda3&#x2F;lib&#x2F;python3.7&#x2F;site-packages&#x2F;torchtext&#x2F;_torchtext.so: undefined symbol: _ZN3c1023_fastEqualsForContainerERKNS_6IValueES2</span><br></pre></td></tr></table></figure><p>So what’s wrong with you?? The reason is my torchtext version is too high, the version of torchtext is 0.8.0,<br>So, </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall torchtext</span><br><span class="line">pip install torchtext&#x3D;&#x3D;0.6.0</span><br></pre></td></tr></table></figure><p>OK! Over.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Transformer MT in torch</title>
      <link href="2020/12/06/Transformer-MT-in-torch/"/>
      <url>2020/12/06/Transformer-MT-in-torch/</url>
      
        <content type="html"><![CDATA[<p><a href="https://luozhouyang.github.io/transformer/">Refer1-a blog about transformer in MT</a></p><p><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">Refer2-https://github.com/jadore801120/attention-is-all-you-need-pytorch</a></p><p><a href="https://github.com/JayParks/transformer">Refer3-jayparks-transformer</a></p><p><a href="https://github.com/foamliu/Transformer">Refer4-foamliu-Transformer</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Read .bin vetcor by python</title>
      <link href="2020/12/05/Read-bin-vetcor-by-python/"/>
      <url>2020/12/05/Read-bin-vetcor-by-python/</url>
      
        <content type="html"><![CDATA[<p>This blog descibe how to read the .bin file, how to use the pre trained vector in KG embedding for your own dict, your own model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import datetime</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">def make_relation2id():</span><br><span class="line"></span><br><span class="line">    with open(&quot;relation2id.txt&quot;, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        inx &#x3D; []  # 1,2,3,4...</span><br><span class="line">        relation&#x3D;[]</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line &#x3D; line.strip().split(&quot;\t&quot;)</span><br><span class="line">            relation.append(line[0])</span><br><span class="line">            inx.append(line[1])</span><br><span class="line">        dic &#x3D; dict(zip(relation, inx))</span><br><span class="line">    return dic</span><br><span class="line"></span><br><span class="line">def read_bin():</span><br><span class="line">    vec &#x3D; np.memmap(&quot;relation2vec.bin&quot;, dtype&#x3D;&#39;float32&#39;, mode&#x3D;&#39;r&#39;, shape&#x3D;(14824, 50))</span><br><span class="line">    return vec</span><br><span class="line"></span><br><span class="line">def make_id_vector():</span><br><span class="line">    RID_dict&#x3D;make_relation2id()</span><br><span class="line">    with open(&quot;relation_set_TS.txt&quot;,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        vec &#x3D; read_bin()</span><br><span class="line">        all_vector&#x3D;[]</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line&#x3D;line.strip().split(&quot;|&quot;)</span><br><span class="line">            relation_vec&#x3D;[0]*50</span><br><span class="line">            for r in line:</span><br><span class="line">                if r in RID_dict:</span><br><span class="line">                    id&#x3D;RID_dict[r]</span><br><span class="line">                    vector&#x3D;vec[int(id)]</span><br><span class="line">                    relation_vec&#x3D;relation_vec+vector</span><br><span class="line">                else:</span><br><span class="line">                    relation_vec&#x3D;np.random.uniform(-0.25, 0.25, 50)</span><br><span class="line">            all_vector.append(relation_vec)</span><br><span class="line"></span><br><span class="line">        np.save(&quot;relation_path_vector.npy&quot;, all_vector)</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    begin&#x3D;datetime.datetime.now()</span><br><span class="line">    relation_embed &#x3D; np.load(&quot;relation_path_vector.npy&quot;)</span><br><span class="line">    weight &#x3D; torch.FloatTensor(relation_embed)</span><br><span class="line">    embedding &#x3D; nn.Embedding.from_pretrained(weight)</span><br><span class="line">    print(embedding)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    end&#x3D;datetime.datetime.now()</span><br><span class="line">    print(&quot;time:&quot;,(end-begin).seconds)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Source for Freebase, wikidata, Map list</title>
      <link href="2020/12/05/Source-for-Freebase-wikidata-Map-list/"/>
      <url>2020/12/05/Source-for-Freebase-wikidata-Map-list/</url>
      
        <content type="html"><![CDATA[<p>The Freebase server has been stopped, but google provide the map file between Freebase and Wikidata, you can download Freebase from <a href="https://developers.google.com/freebase/">FreeBaseAPI</a>, In this web you can download <a href="https://developers.google.com/freebase/#freebase-rdf-dumps">Freebase Triples</a>, <a href="https://developers.google.com/freebase/#freebase-wikidata-mappings">Freebase/Wikidata Mappings</a>. </p><p>OpenKG provide the embedding file by TransE, you can download it from <a href="http://139.129.163.161//">Here</a></p><p><a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Graph Classification by DGL</title>
      <link href="2020/12/05/Graph-Classification-by-DGL/"/>
      <url>2020/12/05/Graph-Classification-by-DGL/</url>
      
        <content type="html"><![CDATA[<p>This is a blog about a Graph Classification by DGL, you can refer <a href="https://docs.dgl.ai/en/latest/api/python/nn.pytorch.html">1</a> and <a href="https://zhuanlan.zhihu.com/p/89503068">2</a></p><h4 id="Graph-classification"><a href="#Graph-classification" class="headerlink" title="Graph classification"></a>Graph classification</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;docs.dgl.ai&#x2F;en&#x2F;0.4.x&#x2F;tutorials&#x2F;basics&#x2F;4_batch.html</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from dgl.data import MiniGCDataset</span><br><span class="line">from dgl.nn.pytorch import GraphConv</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import dgl</span><br><span class="line">import torch</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">class Classifier(nn.Module):</span><br><span class="line">    def __init__(self, in_dim, hidden_dim, n_classes):</span><br><span class="line">        super(Classifier, self).__init__()</span><br><span class="line">        self.conv1 &#x3D; GraphConv(in_dim, hidden_dim)</span><br><span class="line">        self.conv2 &#x3D; GraphConv(hidden_dim, hidden_dim)</span><br><span class="line">        self.classify &#x3D; nn.Linear(hidden_dim, n_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, g):</span><br><span class="line">        # Use node degree as the initial node feature. For undirected graphs, the in-degree</span><br><span class="line">        # is the same as the out_degree.</span><br><span class="line">        h &#x3D; g.in_degrees().view(-1, 1).float()</span><br><span class="line">        print(&quot;------h&quot;,h)</span><br><span class="line">        # Perform graph convolution and activation function.</span><br><span class="line">        h &#x3D; F.relu(self.conv1(g, h))</span><br><span class="line">        h &#x3D; F.relu(self.conv2(g, h))</span><br><span class="line">        g.ndata[&#39;h&#39;] &#x3D; h</span><br><span class="line"></span><br><span class="line">        # Calculate graph representation by averaging all the node representations.</span><br><span class="line">        hg &#x3D; dgl.mean_nodes(g, &#39;h&#39;)</span><br><span class="line">        return self.classify(hg)</span><br><span class="line"></span><br><span class="line">def collate(samples):</span><br><span class="line">    # The input &#96;samples&#96; is a list of pairs</span><br><span class="line">    #  (graph, label).</span><br><span class="line">    graphs, labels &#x3D; map(list, zip(*samples))</span><br><span class="line">    batched_graph &#x3D; dgl.batch(graphs)</span><br><span class="line">    return batched_graph, torch.tensor(labels)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    # Create training and test sets.</span><br><span class="line">    trainset &#x3D; MiniGCDataset(320, 10, 20)</span><br><span class="line">    testset &#x3D; MiniGCDataset(80, 10, 20)</span><br><span class="line">    # Use PyTorch&#39;s DataLoader and the collate function</span><br><span class="line">    # defined before.</span><br><span class="line">    # graph, label &#x3D; trainset[0]</span><br><span class="line">    # print(graph)</span><br><span class="line">    data_loader &#x3D; DataLoader(trainset, batch_size&#x3D;32, shuffle&#x3D;True,</span><br><span class="line">                             collate_fn&#x3D;collate)</span><br><span class="line">    #</span><br><span class="line">    # Create model</span><br><span class="line">    model &#x3D; Classifier(1, 256, trainset.num_classes)</span><br><span class="line">    loss_func &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">    optimizer &#x3D; optim.Adam(model.parameters(), lr&#x3D;0.001)</span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    epoch_losses &#x3D; []</span><br><span class="line">    for epoch in range(80):</span><br><span class="line">        epoch_loss &#x3D; 0</span><br><span class="line">        for iter, (bg, label) in enumerate(data_loader):</span><br><span class="line">            prediction &#x3D; model(bg)</span><br><span class="line">            # print(prediction)</span><br><span class="line">            loss &#x3D; loss_func(prediction, label)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            epoch_loss +&#x3D; loss.detach().item()</span><br><span class="line">        epoch_loss &#x2F;&#x3D; (iter + 1)</span><br><span class="line">        print(&#39;Epoch &#123;&#125;, loss &#123;:.4f&#125;&#39;.format(epoch, epoch_loss))</span><br><span class="line">        epoch_losses.append(epoch_loss)</span><br><span class="line">    plt.title(&#39;cross entropy averaged over minibatches&#39;)</span><br><span class="line">    plt.plot(epoch_losses)</span><br><span class="line">    plt.show()</span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="GraphConv"><a href="#GraphConv" class="headerlink" title="GraphConv"></a>GraphConv</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#https:&#x2F;&#x2F;docs.dgl.ai&#x2F;en&#x2F;latest&#x2F;api&#x2F;python&#x2F;nn.pytorch.html</span><br><span class="line"># https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;89503068</span><br><span class="line"></span><br><span class="line">import dgl</span><br><span class="line">import numpy as np</span><br><span class="line">import torch as th</span><br><span class="line">from dgl.nn import GraphConv</span><br><span class="line"></span><br><span class="line">g &#x3D; dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))</span><br><span class="line"></span><br><span class="line">g &#x3D; dgl.add_self_loop(g)</span><br><span class="line"></span><br><span class="line">feat &#x3D; th.ones(6, 10) # H matrix (feather matrix in their source function)</span><br><span class="line">conv &#x3D; GraphConv(10, 2, norm&#x3D;&#39;both&#39;, weight&#x3D;True, bias&#x3D;True)</span><br><span class="line">res &#x3D; conv(g,feat)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><h4 id="networkx-demo"><a href="#networkx-demo" class="headerlink" title="networkx demo"></a>networkx demo</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import dgl</span><br><span class="line">import torch as th</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import networkx as nx</span><br><span class="line">g &#x3D; dgl.DGLGraph()</span><br><span class="line">g.add_nodes(5, &#123;&#39;n1&#39;: th.randn(5, 10)&#125;)</span><br><span class="line">g.add_edges([0,1,3,4], [2,4,0,3], &#123;&#39;e1&#39;: th.randn(4, 6)&#125;)</span><br><span class="line"></span><br><span class="line">nxg &#x3D; g.to_networkx(node_attrs&#x3D;[&#39;n1&#39;], edge_attrs&#x3D;[&#39;e1&#39;])</span><br><span class="line"></span><br><span class="line">nx.draw(nxg)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ERROR: Cannot uninstall wrapt  when pip tensorflow</title>
      <link href="2020/12/01/ERROR-Cannot-uninstall-wrapt-when-pip-tensorflow/"/>
      <url>2020/12/01/ERROR-Cannot-uninstall-wrapt-when-pip-tensorflow/</url>
      
        <content type="html"><![CDATA[<p>When I pip install tensorflow in centos, by this command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow-gpu&#x3D;&#x3D;1.15.0rc3</span><br><span class="line">Note: until 2020.12.1, the tensorflow version list:</span><br><span class="line">[1.13.1, 1.13.2, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1, 2.1.2, 2.2.0rc0, 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3]</span><br></pre></td></tr></table></figure><p>I met error–1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: Cannot uninstall &#39;wrapt&#39;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</span><br></pre></td></tr></table></figure><p>solution:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U --ignore-installed wrapt enum34 simplejson netaddr</span><br></pre></td></tr></table></figure><p>error–2 again:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.</span><br><span class="line"></span><br><span class="line">We recommend you use --use-feature&#x3D;2020-resolver to test your packages with the new resolver before it becomes the default.</span><br><span class="line"></span><br><span class="line">tensorboard 1.15.0 requires setuptools&gt;&#x3D;41.0.0, but you&#39;ll have setuptools 40.2.0 which is incompatible.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>solution:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade setuptools</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Demo in Standford CoreNLP</title>
      <link href="2020/11/30/Demo-in-Standford-CoreNLP/"/>
      <url>2020/11/30/Demo-in-Standford-CoreNLP/</url>
      
        <content type="html"><![CDATA[<p>StanfordCoreNLP is a NLP tool to analyse, they provide Part-of-Speech, Named Entity Recognition, Basic Dependencies, Enhanced++ Dependencies, Open IE</p><h3 id="pip-Standford-CoreNLP"><a href="#pip-Standford-CoreNLP" class="headerlink" title="pip Standford CoreNLP"></a>pip Standford CoreNLP</h3><h4 id="python-script"><a href="#python-script" class="headerlink" title="python script"></a>python script</h4><ul><li><p>Download from <a href="https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip">https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip</a>, unzip and cd to it.</p></li><li><p>Start server using </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -mx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9003 -timeout 15000</span><br></pre></td></tr></table></figure></li><li><p>excample for word_tokenize:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from stanfordcorenlp import StanfordCoreNLP</span><br><span class="line">nlp &#x3D; StanfordCoreNLP(r&#39;E:\bao\stanford_NLP\stanford-corenlp-full-2018-10-05&#39;)</span><br><span class="line">s&#x3D;nlp.word_tokenize(&#39;This is an example of tokenziation.&#39;)</span><br><span class="line">print(s)</span><br><span class="line">results: [&#39;This&#39;, &#39;is&#39;, &#39;an&#39;, &#39;example&#39;, &#39;of&#39;, &#39;tokenziation&#39;, &#39;.&#39;]</span><br></pre></td></tr></table></figure><h4 id="web"><a href="#web" class="headerlink" title="web"></a>web</h4><p>when you start the serve, you just log <a href="http://localhost:9003/">http://localhost:9003</a></p></li></ul><h3 id="How-to-extrat-triple-from-a-sentence"><a href="#How-to-extrat-triple-from-a-sentence" class="headerlink" title="How to extrat triple from a sentence"></a>How to extrat triple from a sentence</h3><p>By using <a href="https://pypi.org/project/stanford-openie/">OpenIE</a>.<br>Just “pip install stanford-openie”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from openie import StanfordOpenIE</span><br><span class="line"></span><br><span class="line">with StanfordOpenIE() as client:</span><br><span class="line">    text &#x3D; &#39;What country in the United Kingdom contains the Down District Council?&#39;</span><br><span class="line">    print(&#39;Text: %s.&#39; % text)</span><br><span class="line">    for triple in client.annotate(text):</span><br><span class="line">        print(&#39;|-&#39;, triple)</span><br><span class="line">results: </span><br><span class="line">&#123;&#39;subject&#39;: &#39;country&#39;, &#39;relation&#39;: &#39;is in&#39;, &#39;object&#39;: &#39;United Kingdom&#39;&#125;</span><br></pre></td></tr></table></figure><h3 id="Part-of-Speech"><a href="#Part-of-Speech" class="headerlink" title="Part-of-Speech"></a>Part-of-Speech</h3><h4 id="method-1"><a href="#method-1" class="headerlink" title="method 1"></a>method 1</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from stanfordcorenlp import StanfordCoreNLP</span><br><span class="line">nlp &#x3D; StanfordCoreNLP(r&quot;E:\bao\stanford_NLP\stanford-corenlp-full-2018-10-05&quot;)</span><br><span class="line">question &#x3D; &quot;What year did the team with Baltimore Fight Song win the Superbowl&quot;</span><br><span class="line">print(&#39;Part of Speech:&#39;, nlp.pos_tag(question))</span><br><span class="line"></span><br><span class="line">results:</span><br><span class="line">Part of Speech: [(&#39;What&#39;, &#39;WDT&#39;), (&#39;year&#39;, &#39;NN&#39;), (&#39;did&#39;, &#39;VBD&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;team&#39;, &#39;NN&#39;), (&#39;with&#39;, &#39;IN&#39;), (&#39;Baltimore&#39;, &#39;NNP&#39;), (&#39;Fight&#39;, &#39;NN&#39;), (&#39;Song&#39;, &#39;NN&#39;), (&#39;win&#39;, &#39;VBP&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;Superbowl&#39;, &#39;NNP&#39;)]</span><br><span class="line">&#123;</span><br></pre></td></tr></table></figure><h4 id="method-2"><a href="#method-2" class="headerlink" title="method 2"></a>method 2</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">question &#x3D; &quot;What year did the team with Baltimore Fight Song win the Superbowl&quot;</span><br><span class="line"> output &#x3D; nlp.annotate(question, properties&#x3D;&#123;</span><br><span class="line">        &#39;tokenize.whitespace&#39;: True,</span><br><span class="line">        &quot;ssplit.eolonly&quot;: True,</span><br><span class="line">        &#39;annotators&#39;: &quot;pos&quot;,</span><br><span class="line">        &#39;outputFormat&#39;: &#39;json&#39;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    print(output)</span><br><span class="line">results: </span><br><span class="line">&#123;</span><br><span class="line">  &quot;sentences&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;index&quot;: 0,</span><br><span class="line">      &quot;line&quot;: 1,</span><br><span class="line">      &quot;tokens&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;index&quot;: 1,</span><br><span class="line">          &quot;word&quot;: &quot;What&quot;,</span><br><span class="line">          &quot;originalText&quot;: &quot;What&quot;,</span><br><span class="line">          &quot;characterOffsetBegin&quot;: 0,</span><br><span class="line">          &quot;characterOffsetEnd&quot;: 4,</span><br><span class="line">          &quot;pos&quot;: &quot;WDT&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;index&quot;: 2,</span><br><span class="line">          &quot;word&quot;: &quot;year&quot;,</span><br><span class="line">          &quot;originalText&quot;: &quot;year&quot;,</span><br><span class="line">          &quot;characterOffsetBegin&quot;: 5,</span><br><span class="line">          &quot;characterOffsetEnd&quot;: 9,</span><br><span class="line">          &quot;pos&quot;: &quot;NN&quot;</span><br><span class="line">        ------------------</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from stanfordcorenlp import StanfordCoreNLP</span><br><span class="line">nlp &#x3D; StanfordCoreNLP(r&quot;E:\bao\stanford_NLP\stanford-corenlp-full-2018-10-05&quot;)</span><br><span class="line">question &#x3D; &quot;What year did the team with Baltimore Fight Song win the Superbowl&quot;</span><br><span class="line">print(nlp.dependency_parse(question))</span><br><span class="line"></span><br><span class="line">results:</span><br><span class="line">[(&#39;ROOT&#39;, 0, 10), (&#39;det&#39;, 2, 1), (&#39;nmod:tmod&#39;, 10, 2), (&#39;aux&#39;, 10, 3), (&#39;det&#39;, 5, 4), (&#39;nsubj&#39;, 10, 5), (&#39;case&#39;, 9, 6), (&#39;compound&#39;, 9, 7), (&#39;compound&#39;, 9, 8), (&#39;nmod&#39;, 5, 9), (&#39;det&#39;, 12, 11), (&#39;dobj&#39;, 10, 12)]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="The-label-in-standford-NLP"><a href="#The-label-in-standford-NLP" class="headerlink" title="The label in standford NLP"></a>The label in standford NLP</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line">ROOT：要处理文本的语句</span><br><span class="line">IP：简单从句</span><br><span class="line">NP：名词短语</span><br><span class="line">VP：动词短语</span><br><span class="line">PU：断句符，通常是句号、问号、感叹号等标点符号</span><br><span class="line">LCP：方位词短语</span><br><span class="line">PP：介词短语</span><br><span class="line">CP：由‘的’构成的表示修饰性关系的短语</span><br><span class="line">DNP：由‘的’构成的表示所属关系的短语</span><br><span class="line">ADVP：副词短语</span><br><span class="line">ADJP：形容词短语</span><br><span class="line">DP：限定词短语</span><br><span class="line">QP：量词短语</span><br><span class="line">NN：常用名词</span><br><span class="line">NR：固有名词</span><br><span class="line">NT：时间名词</span><br><span class="line">PN：代词</span><br><span class="line">VV：动词</span><br><span class="line">VC：是</span><br><span class="line">CC：表示连词</span><br><span class="line">VE：有</span><br><span class="line">VA：表语形容词</span><br><span class="line">AS：内容标记（如：了）</span><br><span class="line">VRD：动补复合词</span><br><span class="line">CD: 表示基数词</span><br><span class="line">DT: determiner 表示限定词</span><br><span class="line">EX: existential there 存在句</span><br><span class="line">FW: foreign word 外来词</span><br><span class="line">IN: preposition or conjunction, subordinating 介词或从属连词</span><br><span class="line">JJ: adjective or numeral, ordinal 形容词或序数词</span><br><span class="line">JJR: adjective, comparative 形容词比较级</span><br><span class="line">JJS: adjective, superlative 形容词最高级</span><br><span class="line">LS: list item marker 列表标识</span><br><span class="line">MD: modal auxiliary 情态助动词</span><br><span class="line">PDT: pre-determiner 前位限定词</span><br><span class="line">POS: genitive marker 所有格标记</span><br><span class="line">PRP: pronoun, personal 人称代词</span><br><span class="line">RB: adverb 副词</span><br><span class="line">RBR: adverb, comparative 副词比较级</span><br><span class="line">RBS: adverb, superlative 副词最高级</span><br><span class="line">RP: particle 小品词 </span><br><span class="line">SYM: symbol 符号</span><br><span class="line">TO:”to” as preposition or infinitive marker 作为介词或不定式标记 </span><br><span class="line">WDT: WH-determiner WH限定词</span><br><span class="line">WP: WH-pronoun WH代词</span><br><span class="line">WP$: WH-pronoun, possessive WH所有格代词</span><br><span class="line">WRB:Wh-adverb WH副词</span><br><span class="line"> </span><br><span class="line">关系表示</span><br><span class="line">abbrev: abbreviation modifier，缩写</span><br><span class="line">acomp: adjectival complement，形容词的补充；</span><br><span class="line">advcl : adverbial clause modifier，状语从句修饰词</span><br><span class="line">advmod: adverbial modifier状语</span><br><span class="line">agent: agent，代理，一般有by的时候会出现这个</span><br><span class="line">amod: adjectival modifier形容词</span><br><span class="line">appos: appositional modifier,同位词</span><br><span class="line">attr: attributive，属性</span><br><span class="line">aux: auxiliary，非主要动词和助词，如BE,HAVE SHOULD&#x2F;COULD等到</span><br><span class="line">auxpass: passive auxiliary 被动词</span><br><span class="line">cc: coordination，并列关系，一般取第一个词</span><br><span class="line">ccomp: clausal complement从句补充</span><br><span class="line">complm: complementizer，引导从句的词好重聚中的主要动词</span><br><span class="line">conj : conjunct，连接两个并列的词。</span><br><span class="line">cop: copula。系动词（如be,seem,appear等），（命题主词与谓词间的）连系</span><br><span class="line">csubj : clausal subject，从主关系</span><br><span class="line">csubjpass: clausal passive subject 主从被动关系</span><br><span class="line">dep: dependent依赖关系</span><br><span class="line">det: determiner决定词，如冠词等</span><br><span class="line">dobj : direct object直接宾语</span><br><span class="line">expl: expletive，主要是抓取there</span><br><span class="line">infmod: infinitival modifier，动词不定式</span><br><span class="line">iobj : indirect object，非直接宾语，也就是所以的间接宾语；</span><br><span class="line">mark: marker，主要出现在有“that” or “whether”“because”, “when”,</span><br><span class="line">mwe: multi-word expression，多个词的表示</span><br><span class="line">neg: negation modifier否定词</span><br><span class="line">nn: noun compound modifier名词组合形式</span><br><span class="line">npadvmod: noun phrase as adverbial modifier名词作状语</span><br><span class="line">nsubj : nominal subject，名词主语</span><br><span class="line">nsubjpass: passive nominal subject，被动的名词主语</span><br><span class="line">num: numeric modifier，数值修饰</span><br><span class="line">number: element of compound number，组合数字</span><br><span class="line">parataxis: parataxis: parataxis，并列关系</span><br><span class="line">partmod: participial modifier动词形式的修饰</span><br><span class="line">pcomp: prepositional complement，介词补充</span><br><span class="line">pobj : object of a preposition，介词的宾语</span><br><span class="line">poss: possession modifier，所有形式，所有格，所属</span><br><span class="line">possessive: possessive modifier，这个表示所有者和那个’S的关系</span><br><span class="line">preconj : preconjunct，常常是出现在 “either”, “both”, “neither”的情况下</span><br><span class="line">predet: predeterminer，前缀决定，常常是表示所有</span><br><span class="line">prep: prepositional modifier</span><br><span class="line">prepc: prepositional clausal modifier</span><br><span class="line">prt: phrasal verb particle，动词短语</span><br><span class="line">punct: punctuation，这个很少见，但是保留下来了，结果当中不会出现这个</span><br><span class="line">purpcl : purpose clause modifier，目的从句</span><br><span class="line">quantmod: quantifier phrase modifier，数量短语</span><br><span class="line">rcmod: relative clause modifier相关关系</span><br><span class="line">ref : referent，指示物，指代</span><br><span class="line">rel : relative</span><br><span class="line">root: root，最重要的词，从它开始，根节点</span><br><span class="line">tmod: temporal modifier</span><br><span class="line">xcomp: open clausal complement</span><br><span class="line">xsubj : controlling subject 掌控者</span><br><span class="line">中心语为谓词</span><br><span class="line">  subj — 主语</span><br><span class="line"> nsubj — 名词性主语（nominal subject） （同步，建设）</span><br><span class="line">   top — 主题（topic） （是，建筑）</span><br><span class="line">npsubj — 被动型主语（nominal passive subject），专指由“被”引导的被动句中的主语，一般是谓词语义上的受事 （称作，镍）</span><br><span class="line"> csubj — 从句主语（clausal subject），中文不存在</span><br><span class="line"> xsubj — x主语，一般是一个主语下面含多个从句 （完善，有些）</span><br><span class="line">中心语为谓词或介词   </span><br><span class="line">   obj — 宾语</span><br><span class="line">  dobj — 直接宾语 （颁布，文件）</span><br><span class="line">  iobj — 间接宾语（indirect object），基本不存在</span><br><span class="line"> range — 间接宾语为数量词，又称为与格 （成交，元）</span><br><span class="line">  pobj — 介词宾语 （根据，要求）</span><br><span class="line">  lobj — 时间介词 （来，近年）</span><br><span class="line">中心语为谓词</span><br><span class="line">  comp — 补语</span><br><span class="line"> ccomp — 从句补语，一般由两个动词构成，中心语引导后一个动词所在的从句(IP) （出现，纳入）</span><br><span class="line"> xcomp — x从句补语（xclausal complement），不存在   </span><br><span class="line"> acomp — 形容词补语（adjectival complement）</span><br><span class="line"> tcomp — 时间补语（temporal complement） （遇到，以前）</span><br><span class="line">lccomp — 位置补语（localizer complement） （占，以上）</span><br><span class="line">       — 结果补语（resultative complement）</span><br><span class="line">中心语为名词</span><br><span class="line">   mod — 修饰语（modifier）</span><br><span class="line">  pass — 被动修饰（passive）</span><br><span class="line">  tmod — 时间修饰（temporal modifier）</span><br><span class="line"> rcmod — 关系从句修饰（relative clause modifier） （问题，遇到）</span><br><span class="line"> numod — 数量修饰（numeric modifier） （规定，若干）</span><br><span class="line">ornmod — 序数修饰（numeric modifier）</span><br><span class="line">   clf — 类别修饰（classifier modifier） （文件，件）</span><br><span class="line">  nmod — 复合名词修饰（noun compound modifier） （浦东，上海）</span><br><span class="line">  amod — 形容词修饰（adjetive modifier） （情况，新）</span><br><span class="line">advmod — 副词修饰（adverbial modifier） （做到，基本）</span><br><span class="line">  vmod — 动词修饰（verb modifier，participle modifier）</span><br><span class="line">prnmod — 插入词修饰（parenthetical modifier）</span><br><span class="line">   neg — 不定修饰（negative modifier） (遇到，不)</span><br><span class="line">   det — 限定词修饰（determiner modifier） （活动，这些）</span><br><span class="line"> possm — 所属标记（possessive marker），NP</span><br><span class="line">  poss — 所属修饰（possessive modifier），NP</span><br><span class="line">  dvpm — DVP标记（dvp marker），DVP （简单，的）</span><br><span class="line">dvpmod — DVP修饰（dvp modifier），DVP （采取，简单）</span><br><span class="line">  assm — 关联标记（associative marker），DNP （开发，的）</span><br><span class="line">assmod — 关联修饰（associative modifier），NP|QP （教训，特区）</span><br><span class="line">  prep — 介词修饰（prepositional modifier） NP|VP|IP（采取，对）</span><br><span class="line"> clmod — 从句修饰（clause modifier） （因为，开始）</span><br><span class="line"> plmod — 介词性地点修饰（prepositional localizer modifier） （在，上）</span><br><span class="line">   asp — 时态标词（aspect marker） （做到，了）</span><br><span class="line">partmod– 分词修饰（participial modifier） 不存在</span><br><span class="line">   etc — 等关系（etc） （办法，等）</span><br><span class="line">中心语为实词</span><br><span class="line">  conj — 联合(conjunct)</span><br><span class="line">   cop — 系动(copula) 双指助动词？？？？</span><br><span class="line">    cc — 连接(coordination)，指中心词与连词 （开发，与）</span><br><span class="line">其它</span><br><span class="line">  attr — 属性关系 （是，工程）</span><br><span class="line">cordmod– 并列联合动词（coordinated verb compound） （颁布，实行）</span><br><span class="line">  mmod — 情态动词（modal verb） （得到，能）</span><br><span class="line">    ba — 把字关系</span><br><span class="line">tclaus — 时间从句 （以后，积累）</span><br><span class="line">       — semantic dependent</span><br><span class="line">   cpm — 补语化成分（complementizer），一般指“的”引导的CP （振兴，的）</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Solve version CXXABI_1.3.9 not found</title>
      <link href="2020/11/29/Solve-version-CXXABI-1-3-9-not-found/"/>
      <url>2020/11/29/Solve-version-CXXABI-1-3-9-not-found/</url>
      
        <content type="html"><![CDATA[<p>ERROR: ImportError: /lib64/libstdc++.so.6: version `CXXABI_1.3.9’ not found </p><p>Solution (no root):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi  ~&#x2F;.bash_profile</span><br><span class="line">LD_LIBRARY_PATH&#x3D;&#x2F;opt&#x2F;anaconda3&#x2F;lib:$LD_LIBRARY_PATH     </span><br><span class="line">export LD_LIBRARY_PATH</span><br><span class="line">$ source  ~&#x2F;.bash_profile</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Triple to .nt by rdflib</title>
      <link href="2020/11/19/Triple-to-nt-by-rdflib/"/>
      <url>2020/11/19/Triple-to-nt-by-rdflib/</url>
      
        <content type="html"><![CDATA[<p>Virtuoso just supports these styles:</p><p>````<br>.gradf          Geospatial RDF<br>.np            N-Quads<br>.nt        N-Triples<br>.owl            OWL<br>.rdf        RDF/XML<br>.trig          TriG<br>.ttl            Turtle<br>.xml           RDF/XML</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This blog will describe how to change triple into &quot;.nt&quot; file, [reference](https:&#x2F;&#x2F;rilzob.com&#x2F;2019&#x2F;12&#x2F;08&#x2F;Freebase%E5%8F%8A%E5%85%B6%E5%A4%84%E7%90%86%E5%92%8C%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93&#x2F;)</span><br><span class="line">###  Change subfile in freebase to .nt</span><br><span class="line">#### freebase styles</span><br></pre></td></tr></table></figure><p><a href="http://www.freebase.com/m/0n1vy1h">www.freebase.com/m/0n1vy1h</a>    <a href="http://www.freebase.com/people/person/gender">www.freebase.com/people/person/gender</a>    <a href="http://www.freebase.com/m/05zppz">www.freebase.com/m/05zppz</a><br><a href="http://www.freebase.com/m/038b5bg">www.freebase.com/m/038b5bg</a>    <a href="http://www.freebase.com/music/release/track">www.freebase.com/music/release/track</a>    <a href="http://www.freebase.com/m/010bt_f">www.freebase.com/m/010bt_f</a> <a href="http://www.freebase.com/m/010btzs">www.freebase.com/m/010btzs</a><br><a href="http://www.freebase.com/m/0cz9079">www.freebase.com/m/0cz9079</a>    <a href="http://www.freebase.com/soccer/football_player/position_s">www.freebase.com/soccer/football_player/position_s</a>    <a href="http://www.freebase.com/m/02nzb8">www.freebase.com/m/02nzb8</a><br><a href="http://www.freebase.com/m/03d1n81">www.freebase.com/m/03d1n81</a>    <a href="http://www.freebase.com/organization/organization_founder/organizations_founded">www.freebase.com/organization/organization_founder/organizations_founded</a>    <a href="http://www.freebase.com/m/04hk45">www.freebase.com/m/04hk45</a><br><a href="http://www.freebase.com/m/03byqr1">www.freebase.com/m/03byqr1</a>    <a href="http://www.freebase.com/cvg/computer_videogame/cvg_genre">www.freebase.com/cvg/computer_videogame/cvg_genre</a>    <a href="http://www.freebase.com/m/033th">www.freebase.com/m/033th</a> <a href="http://www.freebase.com/m/0_678">www.freebase.com/m/0_678</a><br><a href="http://www.freebase.com/m/0n5xzdf">www.freebase.com/m/0n5xzdf</a>    <a href="http://www.freebase.com/people/person/place_of_birth">www.freebase.com/people/person/place_of_birth</a>    <a href="http://www.freebase.com/m/0c_m3">www.freebase.com/m/0c_m3</a><br><a href="http://www.freebase.com/m/02vwbkk">www.freebase.com/m/02vwbkk</a>    <a href="http://www.freebase.com/people/person/nationality">www.freebase.com/people/person/nationality</a>    <a href="http://www.freebase.com/m/01mk6">www.freebase.com/m/01mk6</a><br><a href="http://www.freebase.com/m/02hl5l7">www.freebase.com/m/02hl5l7</a>    <a href="http://www.freebase.com/cvg/game_version/platform">www.freebase.com/cvg/game_version/platform</a>    <a href="http://www.freebase.com/m/050xd">www.freebase.com/m/050xd</a><br><a href="http://www.freebase.com/m/0b70yxd">www.freebase.com/m/0b70yxd</a>    <a href="http://www.freebase.com/film/film/genre">www.freebase.com/film/film/genre</a>    <a href="http://www.freebase.com/m/07s9rl0">www.freebase.com/m/07s9rl0</a><br><a href="http://www.freebase.com/m/0nd_0ll">www.freebase.com/m/0nd_0ll</a>    <a href="http://www.freebase.com/people/person/gender">www.freebase.com/people/person/gender</a>    <a href="http://www.freebase.com/m/02zsn">www.freebase.com/m/02zsn</a><br><a href="http://www.freebase.com/m/0g5b3gn">www.freebase.com/m/0g5b3gn</a>    <a href="http://www.freebase.com/people/person/gender">www.freebase.com/people/person/gender</a>    <a href="http://www.freebase.com/m/05zppz">www.freebase.com/m/05zppz</a><br><a href="http://www.freebase.com/m/0h972x8">www.freebase.com/m/0h972x8</a>    <a href="http://www.freebase.com/music/album/album_content_type">www.freebase.com/music/album/album_content_type</a>    <a href="http://www.freebase.com/m/02jbfk">www.freebase.com/m/02jbfk</a><br><a href="http://www.freebase.com/m/0l__j">www.freebase.com/m/0l__j</a>    <a href="http://www.freebase.com/common/topic/notable_types">www.freebase.com/common/topic/notable_types</a>    <a href="http://www.freebase.com/m/02scvxs">www.freebase.com/m/02scvxs</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#### Code</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="encoding-utf-8"><a href="#encoding-utf-8" class="headerlink" title="encoding=utf-8"></a>encoding=utf-8</h1><p>from rdflib import Graph, URIRef</p><p>file = open(‘fb2m_test.txt’, ‘r’, encoding=’utf-8’)<br>content = file.readlines()<br>new_content = list(set(content))<br>new_content.sort(key=content.index)  </p><h1 id="print-content"><a href="#print-content" class="headerlink" title="print(content)"></a>print(content)</h1><p>graph = Graph() </p><p>lasttriple = [0,0,0]</p><p>for i in range(len(new_content)-1):<br>    triple = new_content[i].split(‘\t’)<br>    triple[2] = triple[2].rstrip(‘\n’)<br>    triple[2] = triple[2].split(‘ ‘)<br>    # if triple == lasttriple:<br>    #     continue<br>    if len(triple[2]) &gt; 1:<br>        continue<br>    lasttriple = triple<br>    triple[2] = str(triple[2]).lstrip(‘[&#39;‘).rstrip(‘]&#39;‘)<br>    print(triple)<br>    triple = (URIRef(t) for t in triple)<br>    graph.add(triple)</p><p>graph.serialize(‘fb2m_test.nt’, format=’nt’)<br>```</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Install Virtuoso</title>
      <link href="2020/11/19/Install-Virtuoso/"/>
      <url>2020/11/19/Install-Virtuoso/</url>
      
        <content type="html"><![CDATA[<p>There are many graph databases, such as Virtuoso、AllegroGraph、Stardog, Neo4j、OrientDB、Titan. This is the blog for installing the virtuoso in Win and Linux, virtuoso is a graph database, we can use SPARQL to query it. <a href="https://longaspire.github.io/blog/%E5%9B%BE%E8%B0%B1%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B02/">Learn more</a></p><h3 id="Win"><a href="#Win" class="headerlink" title="Win"></a><font color=orange>Win</font></h3><h4 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h4><p>You can download it from <a href="https://sourceforge.net/projects/virtuoso/files/">source</a></p><h4 id="Add-two-values-in-your-environments"><a href="#Add-two-values-in-your-environments" class="headerlink" title="Add two values in your environments"></a>Add two values in your environments</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PATH</span><br><span class="line">;%VIRTUOSO_HOME%&#x2F;bin;%VIRTUOSO_HOME%&#x2F;lib;</span><br><span class="line"></span><br><span class="line">--------------------</span><br><span class="line">VIRTUOSO_HOME</span><br><span class="line">E:\sofeware\Virtuoso_\setup\Virtuoso OpenSource 7.2</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Test-virtuoso"><a href="#Test-virtuoso" class="headerlink" title="Test virtuoso"></a>Test virtuoso</h4><p>cmd: typ:  “virtuoso-t -?”, it will output the help information about virtuoso</p><h4 id="Create-virtuoso-server"><a href="#Create-virtuoso-server" class="headerlink" title="Create virtuoso server"></a>Create virtuoso server</h4><p>go to “c:\virtuoso\database”<br>By command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtuoso-t +service create +instance “InstanceName” +configfile virtuoso.ini</span><br></pre></td></tr></table></figure><p>If error occues:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unable to open the service control manager (5)</span><br></pre></td></tr></table></figure><p>You should run the cmd in administrator.</p><p>If successful, it will</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The Virtuoso_InstanceName service has been registered and is associated with the executable..</span><br></pre></td></tr></table></figure><h4 id="Check-server-and-start"><a href="#Check-server-and-start" class="headerlink" title="Check server and start"></a>Check server and start</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1: virtuoso-t +service list</span><br><span class="line">2: virtuoso-t +instance “InstanceName” +service start (Set to automatic in service management)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Visit-conductor"><a href="#Visit-conductor" class="headerlink" title="Visit conductor"></a>Visit conductor</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;localhost:8890</span><br><span class="line">acount: dba</span><br><span class="line">password:dba</span><br></pre></td></tr></table></figure><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a><font color=orange>Linux</font></h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BatchNormal in Torch</title>
      <link href="2020/10/14/BatchNormal-in-Torch/"/>
      <url>2020/10/14/BatchNormal-in-Torch/</url>
      
        <content type="html"><![CDATA[<p>Yes, it’s a long time not to update my blog, I am doing my first work in multi-KBQA in these days. Almost done! This time, I will write an error when I use BatchNorm1d in torch, in the first, you can train your first model to save your parameters about normal, and then to use them in another model, there is an excample:</p><h3 id="Save-parameters-in-your-first-model"><a href="#Save-parameters-in-your-first-model" class="headerlink" title="Save parameters in your first model"></a>Save parameters in your first model</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def write_embedding_files(self, model):</span><br><span class="line">      model.eval()</span><br><span class="line">      #os.path.abspath(os.path.join(os.getcwd(), &quot;..&quot;)) + &quot;&#x2F;data_new&quot;</span><br><span class="line">      data_dir &#x3D; os.path.abspath(os.path.join(os.getcwd())) + &quot;&#x2F;data_new&quot;</span><br><span class="line">      embedding_dir &#x3D; os.path.abspath(os.path.join(os.getcwd())) + &quot;&#x2F;kg_embedding&quot;</span><br><span class="line">      model_folder &#x3D; embedding_dir</span><br><span class="line">      data_folder &#x3D; data_dir</span><br><span class="line"></span><br><span class="line">      if os.path.exists(model_folder) &#x3D;&#x3D; False:</span><br><span class="line">          os.mkdir(model_folder)</span><br><span class="line">      R_numpy &#x3D; model.R.weight.data.cpu().numpy()</span><br><span class="line">      E_numpy &#x3D; model.E.weight.data.cpu().numpy()</span><br><span class="line">      bn_list &#x3D; []</span><br><span class="line">      for bn in [model.bn0, model.bn1, model.bn2]:</span><br><span class="line">          bn_weight &#x3D; bn.weight.data.cpu().numpy()</span><br><span class="line">          bn_bias &#x3D; bn.bias.data.cpu().numpy()</span><br><span class="line">          bn_running_mean &#x3D; bn.running_mean.data.cpu().numpy()</span><br><span class="line">          bn_running_var &#x3D; bn.running_var.data.cpu().numpy()</span><br><span class="line">          bn_numpy &#x3D; &#123;&#125;</span><br><span class="line">          bn_numpy[&#39;weight&#39;] &#x3D; bn_weight</span><br><span class="line">          bn_numpy[&#39;bias&#39;] &#x3D; bn_bias</span><br><span class="line">          bn_numpy[&#39;running_mean&#39;] &#x3D; bn_running_mean</span><br><span class="line">          bn_numpy[&#39;running_var&#39;] &#x3D; bn_running_var</span><br><span class="line">          bn_list.append(bn_numpy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      W_numpy &#x3D; model.W.detach().cpu().numpy()</span><br><span class="line"></span><br><span class="line">      np.save(model_folder + &#39;&#x2F;E.npy&#39;, E_numpy)</span><br><span class="line">      np.save(model_folder + &#39;&#x2F;R.npy&#39;, R_numpy)</span><br><span class="line">      for i, bn in enumerate(bn_list):</span><br><span class="line">          np.save(model_folder + &#39;&#x2F;bn&#39; + str(i) + &#39;.npy&#39;, bn)</span><br></pre></td></tr></table></figure><h3 id="Use-parameters-in-your-second-model"><a href="#Use-parameters-in-your-second-model" class="headerlink" title="Use parameters in your second model"></a>Use parameters in your second model</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">for i in range(3):</span><br><span class="line">    bn &#x3D; np.load(embedding_folder + &#39;&#x2F;bn&#39; + str(i) + &#39;.npy&#39;, allow_pickle&#x3D;True)</span><br><span class="line">    bn_list.append(bn.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> self.bn0 &#x3D; torch.nn.BatchNorm1d(d1)</span><br><span class="line"> self.bn2 &#x3D; torch.nn.BatchNorm1d(d1)</span><br><span class="line"></span><br><span class="line">        for i in range(3):</span><br><span class="line">            for key, value in self.bn_list[i].items():</span><br><span class="line">                self.bn_list[i][key] &#x3D; torch.Tensor(value).to(device)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.bn0.weight.data &#x3D; self.bn_list[0][&#39;weight&#39;]</span><br><span class="line">        self.bn0.bias.data &#x3D; self.bn_list[0][&#39;bias&#39;]</span><br><span class="line">        self.bn0.running_mean.data &#x3D; self.bn_list[0][&#39;running_mean&#39;]</span><br><span class="line">        self.bn0.running_var.data &#x3D; self.bn_list[0][&#39;running_var&#39;]</span><br><span class="line"></span><br><span class="line">        self.bn2.weight.data &#x3D; self.bn_list[2][&#39;weight&#39;]</span><br><span class="line">        self.bn2.bias.data &#x3D; self.bn_list[2][&#39;bias&#39;]</span><br><span class="line">        self.bn2.running_mean.data &#x3D; self.bn_list[2][&#39;running_mean&#39;]</span><br><span class="line">        self.bn2.running_var.data &#x3D; self.bn_list[2][&#39;running_var&#39;]</span><br></pre></td></tr></table></figure><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1&gt; when you use self.bn0 &#x3D; torch.nn.BatchNorm1d(d1) in your second model, d1 must same the value in your first model.</span><br><span class="line"></span><br><span class="line">2&gt; If you want to normal your tensor, such as :</span><br><span class="line"></span><br><span class="line">bn0 &#x3D; torch.nn.BatchNorm1d(200)</span><br><span class="line">head&#x3D;head  [128,200] or [128,200,1]</span><br><span class="line">bn0(head) is right, if head&#39;s size is [128,2,100], the result is wrong!!</span><br><span class="line">So------!!!!!! the second dim in head must same 200!!</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MarginRankingLoss in python</title>
      <link href="2020/09/15/MarginRankingLoss-in-python/"/>
      <url>2020/09/15/MarginRankingLoss-in-python/</url>
      
        <content type="html"><![CDATA[<p>MarginRankingLoss,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">torch.manual_seed(1)</span><br><span class="line">refer: https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_44633882&#x2F;article&#x2F;details&#x2F;105595785</span><br><span class="line">from time import time</span><br><span class="line">batch &#x3D; 4</span><br><span class="line">clsnum &#x3D; 3</span><br><span class="line">y_true &#x3D; torch.randint(0,2,(batch,clsnum)) # true label</span><br><span class="line">print(y_true)</span><br><span class="line">y_pred &#x3D; torch.rand((batch,clsnum)) # predict label</span><br><span class="line">print(&quot;-----&quot;)</span><br><span class="line">print(y_pred)</span><br><span class="line"></span><br><span class="line">st &#x3D; time()</span><br><span class="line">sum_one &#x3D; 0</span><br><span class="line">for i in range(y_pred.size(0)):</span><br><span class="line">    true_index &#x3D; (y_true[i] &#x3D;&#x3D; 1.0).nonzero().flatten()</span><br><span class="line">    false_index &#x3D; (y_true[i] &#x3D;&#x3D; 0.0).nonzero().flatten()</span><br><span class="line">    for j in true_index:</span><br><span class="line">        for k in false_index:</span><br><span class="line">            val &#x3D; 1 - y_pred[i, j] + y_pred[i, k]</span><br><span class="line">            if val &gt; 0:</span><br><span class="line">                sum_one +&#x3D; val</span><br><span class="line">            else:</span><br><span class="line">                sum_one +&#x3D; val * 0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Obtaining the content from websites that need to be logged in</title>
      <link href="2020/08/19/Obtaining-the-content-from-websites-that-need-to-be-logged-in/"/>
      <url>2020/08/19/Obtaining-the-content-from-websites-that-need-to-be-logged-in/</url>
      
        <content type="html"><![CDATA[<p>Sometimes, we can not obtain the content from web by python request, because, the website need to be logged in, so in this blog, I will introduce a method about how to log in and obtian the content.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from selenium import webdriver</span><br><span class="line">def main():</span><br><span class="line">    url &#x3D; &quot;Your login website&quot; # such as,https:&#x2F;&#x2F;gitee.com&#x2F;login </span><br><span class="line">    driver &#x3D; webdriver.Chrome()</span><br><span class="line">    driver.get(url)</span><br><span class="line"></span><br><span class="line">    driver.find_element_by_id(&quot;LoginForm_email&quot;).send_keys(&quot;email&quot;)</span><br><span class="line"></span><br><span class="line">    driver.find_element_by_id(&quot;LoginForm_password&quot;).send_keys(&quot;password&quot;)</span><br><span class="line"></span><br><span class="line">    driver.find_element_by_css_selector(&quot;input[name&#x3D;&#39;yt0&#39;]&quot;).click()</span><br><span class="line">    time.sleep(30)</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    print(driver.current_url)</span><br><span class="line"></span><br><span class="line">    print(driver.page_source)</span><br><span class="line">    driver.quit()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="selenium"><a href="#selenium" class="headerlink" title="selenium"></a>selenium</h3><p>In the first, you should pip install selenium</p><h3 id="chromedriver-exe"><a href="#chromedriver-exe" class="headerlink" title="chromedriver.exe"></a>chromedriver.exe</h3><ul><li>In the first, you should check the version of your chrome, from <a href="chrome://version/">HERE</a>, and then according the chrome version, to choose the chromedriver.exe from <a href="http://chromedriver.storage.googleapis.com/index.html">HERE</a>. The progress, you can also refer <a href="https://www.cnblogs.com/it-tsz/p/11753800.html">HERE</a>.</li><li>Second, put chromedriver.exe in two files: (1) the file which includes python.exe, my path is “E:\bao\anaconda_bao\Aanconda3”. (2) “C:\Program Files (x86)\Google\Chrome\Application”</li></ul><h3 id="get-three-elements"><a href="#get-three-elements" class="headerlink" title="get three elements"></a>get three elements</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(&quot;LoginForm_email&quot;).send_keys(&quot;email&quot;)</span><br><span class="line"></span><br><span class="line">    driver.find_element_by_id(&quot;LoginForm_password&quot;).send_keys(&quot;password&quot;)</span><br><span class="line"></span><br><span class="line">    driver.find_element_by_css_selector(&quot;input[name&#x3D;&#39;yt0&#39;]&quot;).click()</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/qq_42780025/article/details/108026726">Refer</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>A system of Chinese medical multi-KBQA</title>
      <link href="2020/08/17/A-system-of-Chinese-medical-multi-KBQA/"/>
      <url>2020/08/17/A-system-of-Chinese-medical-multi-KBQA/</url>
      
        <content type="html"><![CDATA[<p>I build a Chinese medical knowledge grpah, and use more than 80 rules to answer the complex question. For detail, you can refer <a href="https://github.com/ToneLi/A-system-of-Chinese-medical-multi-KBQA">HERE</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Loss sum Error</title>
      <link href="2020/08/11/Loss-sum-Error/"/>
      <url>2020/08/11/Loss-sum-Error/</url>
      
        <content type="html"><![CDATA[<p>I want to record this error, because I spend many time to find it. The model output error usually a cuda tensor, such as “[tensor [3.433],device-cuda]”, if you add the loss in each batch, it wll cauce “CUDA out of memory”. So, you should do ,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for i_batch, mdata in enumerate(loader):</span><br><span class="line"> scores, test_loss, relation_index &#x3D; model.get_score_ranked(S)</span><br><span class="line"> loss_one_batch&#x3D;test_loss.item()  [right]</span><br><span class="line">loss_averay &#x3D; loss_averay + loss_one_batch</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>DGL in GCN classification</title>
      <link href="2020/08/07/DGL-in-GCN-classification/"/>
      <url>2020/08/07/DGL-in-GCN-classification/</url>
      
        <content type="html"><![CDATA[<p>This blog will describe how to use DGL in GCN classification.</p><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>Note: this description from Cora dataset-ReadME.  Cora dataset consists of Machine Learning papers. These papers are classified into one of the following seven classes:</p><ul><li>Case_Based</li><li>Genetic_Algorithms</li><li>Neural_Networks</li><li>Probabilistic_Methods</li><li>Reinforcement_Learning</li><li>Rule_Learning</li><li>Theory</li></ul><p>The papers were selected in a way such that in the final corpus every paper cites or is cited by at least one other paper. There are 2708 papers in the whole corpus.  After stemming and removing stopwords we were left with a vocabulary of size 1433 unique words. All words with document frequency less than 10 were removed.</p><p>THE DIRECTORY CONTAINS TWO FILES:<br>The .content file contains descriptions of the papers in the following format:</p><p><paper_id> <word_attributes>+ <class_label >, word_attributes is the bag_of_word</p><p>The .cites file contains the citation graph of the corpus. Each line describes a link in the following format:</p><p><font color=orange><ID of cited paper> <ID of citing paper></font></p><p>Each line contains two paper IDs. The first entry is the ID of the paper being cited and the second ID stands for the paper which contains the citation. The direction of the link is from right to left. If a line is represented by “paper1 paper2” then the link is “paper2-&gt;paper1”. </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Open question in multi-KBQA</title>
      <link href="2020/07/26/The-important-issues-in-multi-KBQA/"/>
      <url>2020/07/26/The-important-issues-in-multi-KBQA/</url>
      
        <content type="html"><![CDATA[<p>In multi-KBQA, there are some issues need to be solved, these problems are also the open questions in many work. In the next, I will describe these issues and the related work. Some classical model, you can refer <a href="https://iamlimingchen.com/2020/06/04/The-summary-of-the-classical-model-for-multi-KBQA/">HERE</a></p><ul><li>Incomplete in knowledge graph   </li><li>Interpretable reasoning path</li><li>How to infer, more power inference</li><li>How to stop, what’s the maximum of hops.</li><li>How to obtain the topic entity.</li></ul><h3 id="Incomplete-in-knowledge-graph"><a href="#Incomplete-in-knowledge-graph" class="headerlink" title="Incomplete in knowledge graph"></a><font color=green>Incomplete in knowledge graph</font></h3><p>For a question, we assue we all know the answer, but this answer is not excit in knowledge graph, KG is a semantic network which is built by human. It’s a very normal things which have low coverage. EmbedKGQA<br>uses knowlege graph embedding method to solve this issues, while  PullNet and GraftNet choose a method which combine the text corpus-Wikipidia to handle this. Unfortunately, these work can not solve other issues, such as Interpretable, EmbedKGQA does not have reasoning ability.</p><h3 id="Interpretable-reasoning-path"><a href="#Interpretable-reasoning-path" class="headerlink" title="Interpretable reasoning path"></a><font color=green>Interpretable reasoning path</font></h3><p>IRN-An Interpretable Reasoning Network for Multi-Relation Question Answering proposed a method base memeory netwok to get the path relation.  In 2019 [Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering] is appear, authors enhance KV-memeory network to get the inference path.</p><h3 id="How-to-infer-more-power-inference"><a href="#How-to-infer-more-power-inference" class="headerlink" title="How to infer, more power inference"></a><font color=green>How to infer, more power inference</font></h3><p>Memory networks, GCN, R-GCN</p><h3 id="How-to-stop"><a href="#How-to-stop" class="headerlink" title="How to stop "></a><font color=green>How to stop </font></h3><p>In U-Hop,  authors proposed an unrestricted-hop relation extraction framework to relax restrictions on candidate path length.</p><h3 id="How-to-obtain-the-topic-entity"><a href="#How-to-obtain-the-topic-entity" class="headerlink" title="How to obtain the topic entity"></a><font color=green>How to obtain the topic entity</font></h3><ul><li>The dataset have the label of topic entity, such as MetaQA, WebQSP.</li><li>By entity linking method in KG.</li><li>Combining the topic entity obtaining model and inference model.</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The source of Deep Graph Library (DGL)</title>
      <link href="2020/07/26/The-source-of-Deep-Graph-Library-DGL/"/>
      <url>2020/07/26/The-source-of-Deep-Graph-Library-DGL/</url>
      
        <content type="html"><![CDATA[<p>DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.</p><p>I have found two source:</p><ul><li><a href="https://github.com/dmlc/dgl">Deep Graph Library (DGL)</a></li><li><a href="https://github.com/dmlc/dgl/tree/master/examples/pytorch">Source code about different GCN version</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hyperparameters Optimization in DL</title>
      <link href="2020/07/24/Hyperparameters-Optimization-in-DL/"/>
      <url>2020/07/24/Hyperparameters-Optimization-in-DL/</url>
      
        <content type="html"><![CDATA[<p>reference blog <a href="https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/">Practical Guide to Hyperparameters Optimization for Deep Learning Models</a>, this blog also give the random search excample <a href="https://blog.csdn.net/qq_37430422/article/details/103637299">HERE</a><br>The author gived four types method:</p><ul><li>Babysitting (aka Trial &amp; Error)</li><li>Grid Search</li><li><font color=red>Random Search</font></li><li>Bayesian Optimization</li></ul><p>In summary: Don’t use Grid Search if your searching space contains more than 3 to 4 dimensions. Instead, use Random Search, which provides a really good baseline for each searching task.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Two version ComplEx code</title>
      <link href="2020/07/23/Two-version-ComplEx-code/"/>
      <url>2020/07/23/Two-version-ComplEx-code/</url>
      
        <content type="html"><![CDATA[<p>The source code of <a href="https://github.com/malllabiisc/EmbedKGQA">EmbedKGQA</a> and <a href="https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding">ROTATE</a> give the describtion about ComplEx.</p><h3 id="EmbedKGQA"><a href="#EmbedKGQA" class="headerlink" title="EmbedKGQA"></a>EmbedKGQA</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def ComplEx(self, head, relation):</span><br><span class="line">    eps &#x3D; 1e-05</span><br><span class="line">  </span><br><span class="line">    head &#x3D; torch.stack(list(torch.chunk(head, 2, dim&#x3D;1)), dim&#x3D;1)</span><br><span class="line">    head &#x3D; self.bn0(head)</span><br><span class="line">    </span><br><span class="line">    head &#x3D; self.ent_dropout(head)</span><br><span class="line">    relation &#x3D; self.rel_dropout(relation)</span><br><span class="line">    head &#x3D; head.permute(1, 0, 2)</span><br><span class="line">    re_head &#x3D; head[0]</span><br><span class="line">    im_head &#x3D; head[1]</span><br><span class="line"></span><br><span class="line">    re_relation, im_relation &#x3D; torch.chunk(relation, 2, dim&#x3D;1)</span><br><span class="line">    re_tail, im_tail &#x3D; torch.chunk(self.embedding.weight, 2, dim &#x3D;1)</span><br><span class="line"></span><br><span class="line">    re_score &#x3D; re_head * re_relation - im_head * im_relation</span><br><span class="line">    im_score &#x3D; re_head * im_relation + im_head * re_relation</span><br><span class="line"></span><br><span class="line">    score &#x3D; torch.stack([re_score, im_score], dim&#x3D;1)</span><br><span class="line">    score &#x3D; self.bn2(score)</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    score &#x3D; self.score_dropout(score)</span><br><span class="line">    score &#x3D; score.permute(1, 0, 2)</span><br><span class="line"></span><br><span class="line">    re_score &#x3D; score[0]</span><br><span class="line">    im_score &#x3D; score[1]</span><br><span class="line">    score &#x3D; torch.mm(re_score, re_tail.transpose(1,0)) + torch.mm(im_score, im_tail.transpose(1,0))</span><br><span class="line">    pred &#x3D; torch.sigmoid(score)</span><br><span class="line">    return pred</span><br></pre></td></tr></table></figure><p>In this script, authors used the entity embedding matrix as the tail. In the last they get the pred by Sigmoid|h◦r◦t|,  they regard pred=Sigmoid|h◦r◦t| as the presiction. The true label is answer’s one-hot representation, such as L=(1,1,0,0,0,1), the length of L is the length of the all entities. At last they used BCEloss(true label, pred) to get the loss, while in RotatE is different.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def ComplEx(self, head, relation, tail, mode):</span><br><span class="line">       re_head, im_head &#x3D; torch.chunk(head, 2, dim&#x3D;2)</span><br><span class="line">       re_relation, im_relation &#x3D; torch.chunk(relation, 2, dim&#x3D;2)</span><br><span class="line">       re_tail, im_tail &#x3D; torch.chunk(tail, 2, dim&#x3D;2)</span><br><span class="line"></span><br><span class="line">       if mode &#x3D;&#x3D; &#39;head-batch&#39;:</span><br><span class="line">           re_score &#x3D; re_relation * re_tail + im_relation * im_tail</span><br><span class="line">           im_score &#x3D; re_relation * im_tail - im_relation * re_tail</span><br><span class="line">           score &#x3D; re_head * re_score + im_head * im_score</span><br><span class="line">       else:</span><br><span class="line">           re_score &#x3D; re_head * re_relation - im_head * im_relation</span><br><span class="line">           im_score &#x3D; re_head * im_relation + im_head * re_relation</span><br><span class="line">           score &#x3D; re_score * re_tail + im_score * im_tail</span><br><span class="line"></span><br><span class="line">       score &#x3D; score.sum(dim &#x3D; 2)</span><br><span class="line">       return score</span><br></pre></td></tr></table></figure><p>They used head, relation, tail in a batch to get the score, this score is the loss value,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">positive_score &#x3D; ComplEx(positive_sample)</span><br><span class="line">negative_score &#x3D; F.logsigmoid(positive_score).squeeze(dim &#x3D; 1)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BCEloss in torch</title>
      <link href="2020/07/23/BCEloss-in-torch/"/>
      <url>2020/07/23/BCEloss-in-torch/</url>
      
        <content type="html"><![CDATA[<p>BCEloss is a Binary Cross Entropy Error Function, the math function is as below, p is the target label, q is the true label. w is the weight. log is ln in math.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Loss &#x3D; -w◦[p ◦log(q) + (1-p)◦log(1-q)]</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">m &#x3D; nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">loss &#x3D; nn.BCELoss(size_average&#x3D;False, reduce&#x3D;False)</span><br><span class="line">input &#x3D; torch.randn(4, requires_grad&#x3D;True)</span><br><span class="line">target &#x3D; torch.empty(4).random_(2)</span><br><span class="line">print(&quot;target&quot;,target)</span><br><span class="line">lossinput &#x3D; m(input)</span><br><span class="line">output &#x3D; loss(lossinput, target)</span><br><span class="line"></span><br><span class="line">print(&quot;the input:&quot;)</span><br><span class="line">print(lossinput)</span><br><span class="line">print(&quot;the input target:&quot;)</span><br><span class="line">print(target)</span><br><span class="line">print(&quot;loss result:&quot;)</span><br><span class="line">print(output)</span><br><span class="line">print(&quot;first loss by own computation：&quot;)</span><br><span class="line">print(-(target[0]*math.log(lossinput[0])+(1-target[0])*math.log(1-lossinput[0])))</span><br><span class="line"></span><br><span class="line">---------result----------</span><br><span class="line">target tensor([0., 0., 1., 1.])</span><br><span class="line">the input:</span><br><span class="line">tensor([0.3643, 0.2608, 0.7570, 0.5007], grad_fn&#x3D;&lt;SigmoidBackward&gt;)</span><br><span class="line">the input target:</span><br><span class="line">tensor([0., 0., 1., 1.])</span><br><span class="line">loss result:</span><br><span class="line">tensor([0.4531, 0.3022, 0.2784, 0.6917], grad_fn&#x3D;&lt;BinaryCrossEntropyBackward&gt;)</span><br><span class="line">first loss by own computation：</span><br><span class="line">tensor(0.4531)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Code moudle Inerpretation in RotatE</title>
      <link href="2020/07/22/Code-moudle-Inerpretation-in-RotatE/"/>
      <url>2020/07/22/Code-moudle-Inerpretation-in-RotatE/</url>
      
        <content type="html"><![CDATA[<p>In this blog, I will introduce the RotatE model (How to use complex vector space). Euler’s formula is used in this model, e^iθ=cosθ+isinθ, </p><h3 id="Why-ri-1"><a href="#Why-ri-1" class="headerlink" title="Why |ri|=1"></a><font color=orange>Why |ri|=1</font></h3><p>ri=e^iθ=cosθ+isinθ</p><p>(ri)^2=(cosθ)^2+(sinθ)^2=1, same as , z=a+bi, z^2=a^2+b^2</p><p>So, |ri|=1,<br>Because, h*r=t, |r|=1, h=t, this situation is symmetric and antisymmetric,<br>“1” refers same.</p><h3 id="Code-analiyses-in-RotaE"><a href="#Code-analiyses-in-RotaE" class="headerlink" title="Code analiyses in RotaE"></a><font color=orange>Code analiyses in RotaE</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def RotatE(self, head, relation, tail, mode):</span><br><span class="line">    pi &#x3D; 3.14159265358979323846</span><br><span class="line">    </span><br><span class="line">    re_head, im_head &#x3D; torch.chunk(head, 2, dim&#x3D;2)</span><br><span class="line">    re_tail, im_tail &#x3D; torch.chunk(tail, 2, dim&#x3D;2)</span><br><span class="line"></span><br><span class="line">    #Make phases of relations uniformly distributed in [-pi, pi]</span><br><span class="line"></span><br><span class="line">    phase_relation &#x3D; relation&#x2F;(self.embedding_range.item()&#x2F;pi)</span><br><span class="line"></span><br><span class="line">    re_relation &#x3D; torch.cos(phase_relation)</span><br><span class="line">    im_relation &#x3D; torch.sin(phase_relation)</span><br><span class="line"></span><br><span class="line">    if mode &#x3D;&#x3D; &#39;head-batch&#39;:</span><br><span class="line">        re_score &#x3D; re_relation * re_tail + im_relation * im_tail</span><br><span class="line">        im_score &#x3D; re_relation * im_tail - im_relation * re_tail</span><br><span class="line">        re_score &#x3D; re_score - re_head</span><br><span class="line">        im_score &#x3D; im_score - im_head</span><br><span class="line">    else:</span><br><span class="line">        re_score &#x3D; re_head * re_relation - im_head * im_relation</span><br><span class="line">        im_score &#x3D; re_head * im_relation + im_head * re_relation</span><br><span class="line">        re_score &#x3D; re_score - re_tail</span><br><span class="line">        im_score &#x3D; im_score - im_tail</span><br><span class="line"></span><br><span class="line">    score &#x3D; torch.stack([re_score, im_score], dim &#x3D; 0)</span><br><span class="line">    score &#x3D; score.norm(dim &#x3D; 0)</span><br><span class="line"></span><br><span class="line">    score &#x3D; self.gamma.item() - score.sum(dim &#x3D; 2)</span><br><span class="line">    return score</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>By chunk, head and tail is divided in to real part (re) and  imaginary part (im).</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">e_head, im_head &#x3D; torch.chunk(head, 2, dim&#x3D;2)</span><br><span class="line">re_tail, im_tail &#x3D; torch.chunk(tail, 2, dim&#x3D;2)</span><br></pre></td></tr></table></figure><p>By /pi, the relation is changed into angle, so, phase_relation is the angle,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re_relation &#x3D; torch.cos(phase_relation)</span><br><span class="line">im_relation &#x3D; torch.sin(phase_relation)</span><br></pre></td></tr></table></figure><p>In the last, because, h*r=t<br>h=h_re+i◦h_im</p><p>r=r_re+i◦r_im</p><p>t=t_re+i◦t_im</p><p>h◦r=(h_re+i◦h_im)◦(r_re+i◦r_im)</p><p>h◦r    =h_re◦r_re+h_re◦r_im◦i+h_im◦r_re◦i-h_im◦r_im</p><p>h◦r    =<font color=red>h_re◦r_re-h_im◦r_im</font>+<font color=green>(h_im◦r_re+h_re◦r_im)◦i</font></p><p>re_score=real part=<font color=red>h_re◦r_re-h_im◦r_im</font>-t_re</p><p>im_score= imaginary part=<font color=green>h_im◦r_re+h_re◦r_im-t_im</font></p><p>h◦r-t=re_score+im_score</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score &#x3D; torch.stack([re_score, im_score], dim &#x3D; 0)</span><br><span class="line">score &#x3D; score.norm(dim &#x3D; 0)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>multi-QA GCN</title>
      <link href="2020/07/19/multi-QA-GCN/"/>
      <url>2020/07/19/multi-QA-GCN/</url>
      
        <content type="html"><![CDATA[<h3 id="Graph-Convolutional-Network-GCN"><a href="#Graph-Convolutional-Network-GCN" class="headerlink" title="Graph Convolutional Network (GCN)"></a>Graph Convolutional Network (GCN)</h3><ul><li>Graph Convolutional Networks for Text Classification-2019</li><li><a href="https://github.com/yao8839836/text_gcn">Practice 1-the source code of this above paper</a></li><li><a href="https://www.jiqizhixin.com/articles/2019-05-22-4">Practice 2</a>, <a href="https://github.com/plkmo/Bible_Text_GCN">The website of project</a></li><li> GraftNet: Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text-2018</li><li> Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader-2019</li></ul><h3 id="Relational-Graph-Convolutional-Network-R-GCN"><a href="#Relational-Graph-Convolutional-Network-R-GCN" class="headerlink" title="Relational Graph Convolutional Network (R-GCN)"></a>Relational Graph Convolutional Network (R-GCN)</h3><ul><li>Modeling Relational Data with Graph Convolutional Networks-2018</li><li>BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering-2019</li></ul><h3 id="Hyper-Graph-Convolutional-Networks-HGCN"><a href="#Hyper-Graph-Convolutional-Networks-HGCN" class="headerlink" title="Hyper Graph Convolutional Networks (HGCN)"></a>Hyper Graph Convolutional Networks (HGCN)</h3><ul><li>Hypergraph Neural Networks-2019</li><li>Two-Phase Hypergraph Based Reasoning with Dynamic Relations for Multi-Hop KBQA-2020</li><li>Hypergraph Convolutional Network for Multi-Hop Knowledge Base Question Answering (Student Abstract)-2020</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GNN papers</title>
      <link href="2020/07/18/GNN-papers/"/>
      <url>2020/07/18/GNN-papers/</url>
      
        <content type="html"><![CDATA[<ul><li>From Tinghua, reference list about GNN. <a href="https://github.com/thunlp/GNNPapers">Here</a></li><li><a href="https://arxiv.org/pdf/1812.08434.pdf">GNN-Graph Neural Networks: A Review of Methods and Applications</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>multi-KBQA in IJCAI2020</title>
      <link href="2020/07/15/multi-KBQA-in-IJCAI2020/"/>
      <url>2020/07/15/multi-KBQA-in-IJCAI2020/</url>
      
        <content type="html"><![CDATA[<p>The complex question answering in IJCAI2020</p><h3 id="QA-with-Meta-Learning"><a href="#QA-with-Meta-Learning" class="headerlink" title="QA with Meta Learning"></a>QA with Meta Learning</h3><ul><li><a href="https://www.ijcai.org/Proceedings/2020/509">Retrieve, Program, Repeat: Complex Knowledge Base Question Answering via Alternate Meta-learning</a></li></ul><h3 id="Hypergraph-convolutional-networks-HGCN"><a href="#Hypergraph-convolutional-networks-HGCN" class="headerlink" title="Hypergraph convolutional networks (HGCN)"></a>Hypergraph convolutional networks (HGCN)</h3><ul><li><a href="https://www.ijcai.org/Proceedings/2020/500">Two-Phase Hypergraph Based Reasoning with Dynamic Relations fo rMulti-HopKBQA</a></li></ul><h3 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h3><ul><li><a href="https://www.ijcai.org/Proceedings/2020/519">Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Highlight in ACL2020</title>
      <link href="2020/07/13/Highlight-in-ACL2020/"/>
      <url>2020/07/13/Highlight-in-ACL2020/</url>
      
        <content type="html"><![CDATA[<p>Vered Shwartz published Highlights of ACL 2020 in July/11/2020<a href="https://medium.com/@vered1986/highlights-of-acl-2020-4ef9f27a4f0c">Here</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>No elmo.py in allennlp</title>
      <link href="2020/07/12/No-elmo-py-in-allennlp/"/>
      <url>2020/07/12/No-elmo-py-in-allennlp/</url>
      
        <content type="html"><![CDATA[<p>When I run the code <font color=red>allennlp.commands.elmo import ElmoEmbedder</font> in “BAG: Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering”, there is an error <font color=red>“No moudle named commands.elmo”</font>, so I decide to find the elmo.py file in their project <a href="https://github.com/allenai/allennlp/tree/master/allennlp/commands">allennlp</a>, Suprise! there is no elmo.py.  The author told me “We removed the Elmo command in version 1.0, you can run version 0.9 of AllenNLP”. So now (2020/7/12), the version of allennlp in github is 1.0, if I want to use the elmo command, I should run version 0.9. In the next, I will describe how to install it in detail.</p><ul><li>In the first, you need download <font color=green>allennlp-0.9.0-py3-none-any.whl</font> in <a href="https://mirrors.cloud.tencent.com/pypi/simple/allennlp/">Here</a></li><li>Second, you should enter the file which has allennlp-0.9.0-py3-none-any.whl, and run <font color=red>pip install allennlp-0.9.0-py3-none-any.whl</font> </li><li>Ok, done!</li></ul><p>Note: install this tool depends on your device, if you can pass it, you should try install it by the source package, such as allennlp-0.9.0.tar.gz, elmo.py in it.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Knowledge in ACL2020</title>
      <link href="2020/07/11/Knowledge-in-ACL2020/"/>
      <url>2020/07/11/Knowledge-in-ACL2020/</url>
      
        <content type="html"><![CDATA[<p>I list the papers in ACL 2020 which about the knowledge, and use these paper’s title to generate the word cloud in my note.</p><ul><li>Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information</li><li>Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction</li><li>Connecting Embeddings for Knowledge Graph Entity Typing</li><li>Distilling Knowledge Learned in BERT for Text Generation</li><li>Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness</li><li>Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge</li><li>Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy</li><li>Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs</li><li>Improving Event Detection via Open-domain Trigger Knowledge</li><li>Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings</li><li>Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge</li><li>KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation</li><li>KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis</li><li>Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation</li><li>Knowledge Graph Embedding Compression</li><li>Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward</li><li>Low-Dimensional Hyperbolic Knowledge Graph Embeddings</li><li>NeuInfer: Knowledge Inference on N-ary Facts</li><li>Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding</li><li>R^3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge</li><li>ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding</li><li>Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation</li><li>SEEK: Segmented Embedding of Knowledge Graphs</li><li>SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</li><li>Structure-Level Knowledge Distillation For Multilingual Sequence Labeling</li><li>Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer</li><li>The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents</li><li>TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories</li><li>WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge</li><li>A Re-evaluation of Knowledge Graph Completion Methods</li><li>Efficient strategies for hierarchical text classification: external knowledge and auxiliary tasks</li><li><a href="https://www.aclweb.org/anthology/2020.acl-main.538/">Incorporating External Knowledge through Pre-training for Natural Language to Code Generation</a></li><li>Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms</li><li>Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge</li><li>Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases</li><li>GAIA: A Fine-grained Multimedia Knowledge Extraction System</li><li>MixingBoard: a Knowledgeable Stylized Integrated Text Generation Platform</li><li>TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing</li><li>Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources</li><li>Let’s be Humorous: Knowledge Enhanced Humor Generation</li><li>Why is penguin more similar to polar bear than to sea gull? Analyzing conceptual knowledge in distributional models</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from wordcloud import WordCloud</span><br><span class="line">with open(&quot;KG_tittle.txt&quot; ,encoding&#x3D;&quot;utf-8&quot;)as file:</span><br><span class="line">    #read text</span><br><span class="line">    text&#x3D;file.read()</span><br><span class="line">    wordcloud&#x3D;WordCloud(font_path&#x3D;&quot;C:&#x2F;Windows&#x2F;Fonts&#x2F;Tahoma.ttf&quot;,</span><br><span class="line">    background_color&#x3D;&quot;black&quot;,width&#x3D;600,</span><br><span class="line">    height&#x3D;300,max_words&#x3D;50).generate(text)</span><br><span class="line">    #generate picture</span><br><span class="line">    image&#x3D;wordcloud.to_image()</span><br><span class="line">    #show picture</span><br><span class="line">    image.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Get n-gram for a sentence</title>
      <link href="2020/07/09/Get-n-gram-for-a-sentence/"/>
      <url>2020/07/09/Get-n-gram-for-a-sentence/</url>
      
        <content type="html"><![CDATA[<p>we want to use n-gram get the different semantic units from a sentence, in this blog, I give a script to obtain the n-gram from a sentence, English or Chinese.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def zu(list):</span><br><span class="line">    #[&quot;w&quot;,&quot;e&quot;]--&gt;[w e]</span><br><span class="line">    s&#x3D;&quot;&quot;</span><br><span class="line">    for word in list:</span><br><span class="line">        s&#x3D;s+&quot; &quot;+word</span><br><span class="line">    return s.strip()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_ngrams(sentence,n&#x3D;3):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    get the n-gram for a sentence  default n is 3</span><br><span class="line">    :param sentence:   movies are about</span><br><span class="line">    :return:  sets&#x3D;[movies, are, about,movies are, are about, movies are about]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    print(sentence)</span><br><span class="line">    tempSentence &#x3D; sentence.split(&quot; &quot;)</span><br><span class="line"></span><br><span class="line">    n_grams&#x3D;[]</span><br><span class="line">    for j in range(2,n+1):</span><br><span class="line">        n_gram &#x3D; []</span><br><span class="line">        [n_gram.append(zu(tempSentence[i:i + j]) )if len(tempSentence[i:i + j]) &#x3D;&#x3D; j else 0 for i in range(0, len(tempSentence))]</span><br><span class="line">        n_grams.append(n_gram)</span><br><span class="line">    end_gram &#x3D; [num for elem in n_grams for num in elem]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Note:  vec1 &#x3D; [[1, 2, 3], [4, 5, 6], [7, 8, 9]], </span><br><span class="line">    s1 &#x3D; [num for elem in vec1 for num in elem]  #elem&#x3D;[1, 2, 3],[4, 5, 6]...</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    return  end_gram</span><br><span class="line"></span><br><span class="line">sen&#x3D;&quot;movies are about&quot;</span><br><span class="line">print(get_ngrams(sen)) #[&#39;movies are&#39;, &#39;are about&#39;, &#39;movies are about&#39;]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Can we use reasoning path to answer the question in multi-KBQA</title>
      <link href="2020/07/04/Can-we-use-reasoning-path-to-answer-the-question-in-multi-KBQA/"/>
      <url>2020/07/04/Can-we-use-reasoning-path-to-answer-the-question-in-multi-KBQA/</url>
      
        <content type="html"><![CDATA[<p>Question rising, I found some datasets have reasoning path in the train and test data. When we run the test data, we can use reasoning path to get the answer directly in knowledge graph, why so many works explore how to do the multi-KBQA task? May be this question is to naive, but some simple things will rise a great idea. I think the reasons as follow： </p><ul><li>Yes, you can get the answer by reasoning path in the test data, but the test data is used to test your model. In the real scenario, the client just tell machine a question, the machine must answer this question. <font color=red>If we use a question which does not occur in test data (sure do not have reasoning path), what should we do? </font>, so give a question, we should let machine to know the reasoning path (entity-relation).</li><li>Ok, if there is a sentence, we have got its relations (reasoning path 1-hop, 2-hop, 3-hop), can we use these relations get the answer? <font color=red>We all know the knowledge graph is incomplete, the answer is excit, but it is not in KG</font>, so just by reasoing path, we can not get the answer, it wll return “None”</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BiLSTM+self_attention in Pytorch</title>
      <link href="2020/07/03/BiLSTM-self-attention-in-Pytorch/"/>
      <url>2020/07/03/BiLSTM-self-attention-in-Pytorch/</url>
      
        <content type="html"><![CDATA[<p>The structure in Pytorch is simple than tensorflow, in this blog, I give an excample about how to use pytorch in lstm+self_attention. The dataset is IMDB, some code model you can refer <a href="https://pytorch.org/text/datasets.html">HERE</a>, raw code from <a href="https://github.com/uhauha2929/examples/blob/master/self-attention.ipynb">uhauha2929</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">from torchtext import data</span><br><span class="line">from torchtext import datasets</span><br><span class="line">from torchtext.vocab import GloVe</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"></span><br><span class="line">device &#x3D; torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)</span><br><span class="line">batch_size &#x3D; 64</span><br><span class="line">embedding_dim &#x3D; 200</span><br><span class="line">hidden_dim &#x3D; 200</span><br><span class="line">epochs &#x3D; 5</span><br><span class="line"></span><br><span class="line"># define Field, filed define the way about how to progess this data</span><br><span class="line">#tokenize: text--&gt;token,</span><br><span class="line">#sequential: if you segmente the data</span><br><span class="line">#ReversibleField is a type of Filed (description: an extension of the filed that allows reverse mapping of word ids to words)</span><br><span class="line"># ReversibleField, use case: text filed  if you want to map the integers back to natural language (such as in the case of language modeling)</span><br><span class="line">TEXT &#x3D; data.ReversibleField(lower&#x3D;True, include_lengths&#x3D;True)</span><br><span class="line">LABEL &#x3D; data.Field(sequential&#x3D;False)</span><br><span class="line"># make splits for data</span><br><span class="line">#Note： if you cannot download IMDB, you can run wget &quot;http:&#x2F;&#x2F;ai.stanford.edu&#x2F;~amaas&#x2F;data&#x2F;sentiment&#x2F;aclImdb_v1.tar.gz&quot; in your server</span><br><span class="line">train, test &#x3D; datasets.IMDB.splits(TEXT, LABEL)</span><br><span class="line"># build the vocabulary</span><br><span class="line">TEXT.build_vocab(train, vectors&#x3D;GloVe(name&#x3D;&#39;6B&#39;, dim&#x3D;embedding_dim))</span><br><span class="line">LABEL.build_vocab(train)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">.data\imdb\aclImdb\</span><br><span class="line">|- README</span><br><span class="line">|-- test</span><br><span class="line">|-- train</span><br><span class="line">|- imdb.vocab</span><br><span class="line">|-- imdbEr.txt</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># define the Iterator</span><br><span class="line">train_iter, test_iter &#x3D; data.BucketIterator.splits(</span><br><span class="line">        (train, test), sort_key&#x3D;lambda x:len(x.text),</span><br><span class="line">        sort_within_batch&#x3D;True,</span><br><span class="line">        batch_size&#x3D;batch_size, device&#x3D;device,</span><br><span class="line">        repeat&#x3D;False)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SelfAttention(nn.Module):</span><br><span class="line">        def __init__(self, hidden_dim):</span><br><span class="line">                super().__init__()</span><br><span class="line">                self.hidden_dim &#x3D; hidden_dim</span><br><span class="line">                self.projection &#x3D; nn.Sequential(</span><br><span class="line">                        nn.Linear(hidden_dim, 64),</span><br><span class="line">                        nn.ReLU(True),</span><br><span class="line">                        nn.Linear(64, 1)</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        def forward(self, encoder_outputs):</span><br><span class="line">                batch_size &#x3D; encoder_outputs.size(0)</span><br><span class="line">                # (B, L, H) -&gt; (B , L, 1)</span><br><span class="line">                energy &#x3D; self.projection(encoder_outputs)</span><br><span class="line">                weights &#x3D; F.softmax(energy.squeeze(-1), dim&#x3D;1)</span><br><span class="line">                # (B, L, H) * (B, L, 1) -&gt; (B, H)</span><br><span class="line">                outputs &#x3D; (encoder_outputs * weights.unsqueeze(-1)).sum(dim&#x3D;1)</span><br><span class="line">                return outputs, weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class AttnClassifier(nn.Module):</span><br><span class="line">        def __init__(self, input_dim, embedding_dim, hidden_dim):</span><br><span class="line">                super().__init__()</span><br><span class="line">                self.input_dim &#x3D; input_dim</span><br><span class="line">                self.embedding_dim &#x3D; embedding_dim</span><br><span class="line">                self.hidden_dim &#x3D; hidden_dim</span><br><span class="line">                self.embedding &#x3D; nn.Embedding(input_dim, embedding_dim)</span><br><span class="line">                self.lstm &#x3D; nn.LSTM(embedding_dim, hidden_dim, bidirectional&#x3D;True)</span><br><span class="line">                self.attention &#x3D; SelfAttention(hidden_dim)</span><br><span class="line">                self.fc &#x3D; nn.Linear(hidden_dim, 1)</span><br><span class="line"></span><br><span class="line">        def set_embedding(self, vectors):</span><br><span class="line">                self.embedding.weight.data.copy_(vectors)</span><br><span class="line"></span><br><span class="line">        def forward(self, inputs, lengths):</span><br><span class="line">                print(&quot;--get in to forward--&quot;)</span><br><span class="line">                batch_size &#x3D; inputs.size(1)</span><br><span class="line">                # (L, B)</span><br><span class="line">                embedded &#x3D; self.embedding(inputs)</span><br><span class="line">                # (L, B, E)</span><br><span class="line">                packed_emb &#x3D; nn.utils.rnn.pack_padded_sequence(embedded, lengths)</span><br><span class="line">                out, hidden &#x3D; self.lstm(packed_emb)</span><br><span class="line">                out &#x3D; nn.utils.rnn.pad_packed_sequence(out)[0]</span><br><span class="line">                out &#x3D; out[:, :, :self.hidden_dim] + out[:, :, self.hidden_dim:]</span><br><span class="line">                # (L, B, H)</span><br><span class="line">                embedding, attn_weights &#x3D; self.attention(out.transpose(0, 1))</span><br><span class="line">                # (B, HOP, H)</span><br><span class="line">                outputs &#x3D; self.fc(embedding.view(batch_size, -1))</span><br><span class="line">                # (B, 1)</span><br><span class="line">                return outputs, attn_weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train(train_iter, model, optimizer, criterion):</span><br><span class="line">    print(&quot;---get in train----&quot;)</span><br><span class="line">    model.train()</span><br><span class="line">    epoch_loss &#x3D; 0</span><br><span class="line">    bar &#x3D; tqdm(total&#x3D;len(train_iter))</span><br><span class="line">    b_ix &#x3D; 1</span><br><span class="line">    for batch in train_iter:</span><br><span class="line">        (x, x_l), y &#x3D; batch.text, batch.label - 1</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs, _ &#x3D; model(x, x_l)</span><br><span class="line">    #</span><br><span class="line">        loss &#x3D; criterion(outputs.view(-1), y.float())</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        epoch_loss +&#x3D; loss.item()</span><br><span class="line">        if b_ix % 10 &#x3D;&#x3D; 0:</span><br><span class="line">            bar.update(10)</span><br><span class="line">            bar.set_description(&#39;current loss:&#123;:.4f&#125;&#39;.format(epoch_loss &#x2F; b_ix))</span><br><span class="line">        b_ix +&#x3D; 1</span><br><span class="line">    bar.update((b_ix - 1) % 10)</span><br><span class="line">    bar.close()</span><br><span class="line">    return epoch_loss &#x2F; len(train_iter)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">        model &#x3D; AttnClassifier(len(TEXT.vocab), embedding_dim, hidden_dim).to(device)</span><br><span class="line">        model.set_embedding(TEXT.vocab.vectors)</span><br><span class="line">        # optim</span><br><span class="line">        optimizer &#x3D; optim.Adam(model.parameters())</span><br><span class="line">        criterion &#x3D; nn.BCEWithLogitsLoss().to(device)</span><br><span class="line">        # train model</span><br><span class="line">        print(&quot;---------begin------epoch--&quot;)</span><br><span class="line">        for epoch in range(epochs):</span><br><span class="line">                train(train_iter, model, optimizer, criterion)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How to use RoBERT to get word embedding</title>
      <link href="2020/07/02/How-to-use-RoBERT-to-get-word-embedding/"/>
      <url>2020/07/02/How-to-use-RoBERT-to-get-word-embedding/</url>
      
        <content type="html"><![CDATA[<h3 id="There-is-a-funking-error-OSError-Model-name-‘roberta-base’-was-not-foun"><a href="#There-is-a-funking-error-OSError-Model-name-‘roberta-base’-was-not-foun" class="headerlink" title="There is a funking error: OSError: Model name ‘roberta-base’ was not foun"></a><font color=orange>There is a funking error: OSError: Model name ‘roberta-base’ was not foun</font></h3><p><font color=red>Error:</font> OSError: Model name ‘roberta-base’ was not found in tokenizers model name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). We assumed ‘roberta-base’ was a path, a model identifier, or url to a directory containing vocabulary files named [‘vocab.json’, ‘merges.txt’] but couldn’t find such vocabulary files at this path or url.</p><p>Environment: win 10.<br>Solution:</p><ul><li>Step1: I download the relevant file into  pretrained_models/roberta-base<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p pretrained_models&#x2F;roberta-base</span><br><span class="line">$ wget https:&#x2F;&#x2F;s3.amazonaws.com&#x2F;models.huggingface.co&#x2F;bert&#x2F;roberta-base-pytorch_model.bin -O pretrained_models&#x2F;roberta-base&#x2F;pytorch_model.bin</span><br><span class="line">$ wget https:&#x2F;&#x2F;s3.amazonaws.com&#x2F;models.huggingface.co&#x2F;bert&#x2F;roberta-base-config.json -O pretrained_models&#x2F;roberta-base&#x2F;config.json</span><br><span class="line">$ wget https:&#x2F;&#x2F;s3.amazonaws.com&#x2F;models.huggingface.co&#x2F;bert&#x2F;roberta-base-vocab.json -O pretrained_models&#x2F;roberta-base&#x2F;vocab.json</span><br><span class="line">$ wget https:&#x2F;&#x2F;s3.amazonaws.com&#x2F;models.huggingface.co&#x2F;bert&#x2F;roberta-base-merges.txt -O pretrained_models&#x2F;roberta-base&#x2F;merges.txt</span><br><span class="line">$ ls pretrained_models&#x2F;roberta-base</span><br><span class="line">$ config.json  merges.txt  pytorch_model.bin  vocab.json</span><br></pre></td></tr></table></figure></li><li>Step2: And then, this funking error is solved:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from transformers import RobertaModel, RobertaConfig, RobertaTokenizer</span><br><span class="line"></span><br><span class="line">tokeniser &#x3D; RobertaTokenizer.from_pretrained(&#39;pretrained_models&#x2F;roberta-base&#39;)</span><br><span class="line">encoder &#x3D; RobertaModel.from_pretrained(&#39;pretrained_models&#x2F;roberta-base&#39;)</span><br><span class="line">print(len(tokeniser))</span><br><span class="line">print(encoder.embeddings.word_embeddings.weight.shape)</span><br><span class="line"></span><br><span class="line">results:</span><br><span class="line">50265</span><br><span class="line">torch.Size([50265, 768])</span><br></pre></td></tr></table></figure>Note: if you feel it’s difficult to download this file in China, you can download it in my <a href="https://pan.baidu.com/s/1P86rrwTagLQ1-PN4cIgaOw">BaiduYun</a>, code: 2rag.</li></ul><h3 id="Obtain-the-word-id-in-RoBERT"><a href="#Obtain-the-word-id-in-RoBERT" class="headerlink" title="Obtain the word id in RoBERT"></a><font color=orange>Obtain the word id in RoBERT</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from transformers import RobertaTokenizer</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">def pad_sequence(arr, max_len&#x3D;128):</span><br><span class="line">        num_to_add &#x3D; max_len - len(arr)</span><br><span class="line">        for _ in range(num_to_add):</span><br><span class="line">            arr.append(&#39;&lt;pad&gt;&#39;)</span><br><span class="line">        return arr</span><br><span class="line">def toke(question):</span><br><span class="line">    tokenizer &#x3D; RobertaTokenizer.from_pretrained(&#39;pretrained_models&#x2F;roberta-base&#39;)</span><br><span class="line">    question &#x3D; &quot;&lt;s&gt; &quot; + question + &quot; &lt;&#x2F;s&gt;&quot;</span><br><span class="line">    question_tokenized &#x3D; tokenizer.tokenize(question)</span><br><span class="line">    print(&quot;question_tokenized1&quot;,question_tokenized)</span><br><span class="line">    question_tokenized &#x3D; pad_sequence(question_tokenized, 64)</span><br><span class="line">    print(&quot;question_tokenized2&quot;,question_tokenized)</span><br><span class="line">    question_tokenized &#x3D; torch.tensor(tokenizer.encode(question_tokenized, add_special_tokens&#x3D;False))</span><br><span class="line">    print(&quot;question_tokenized&quot;,question_tokenized)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    question&#x3D;&quot;apple computer&quot;</span><br><span class="line">    toke(question)</span><br><span class="line"></span><br><span class="line">result: </span><br><span class="line">question_tokenized1 [&#39;&lt;s&gt;&#39;, &#39;Ġapple&#39;, &#39;Ġcomputer&#39;, &#39;&lt;&#x2F;s&gt;&#39;]</span><br><span class="line">question_tokenized2 [&#39;&lt;s&gt;&#39;, &#39;Ġapple&#39;, &#39;Ġcomputer&#39;, &#39;&lt;&#x2F;s&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;&lt;pad&gt;&#39;]</span><br><span class="line">question_tokenized tensor([    0, 15162,  3034,     2,     1,     1,     1,     1,     1,     1,</span><br><span class="line">            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,</span><br><span class="line">            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,</span><br><span class="line">            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,</span><br><span class="line">            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,</span><br><span class="line">            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,</span><br><span class="line">            1,     1,     1,     1])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Note: I do not understand the different between Ġapple and apple, I find there are many Ġapple in Robert’s vocab, I do not know the choose method.</p><h3 id="Full-Script"><a href="#Full-Script" class="headerlink" title="Full Script"></a><font color=orange>Full Script</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">from transformers import RobertaTokenizer,RobertaModel</span><br><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line">def pad_sequence(arr, max_len&#x3D;128):</span><br><span class="line">        num_to_add &#x3D; max_len - len(arr)</span><br><span class="line">        for _ in range(num_to_add):</span><br><span class="line">            arr.append(&#39;&lt;pad&gt;&#39;)</span><br><span class="line">        return arr</span><br><span class="line">def toke(question):</span><br><span class="line">    tokenizer &#x3D; RobertaTokenizer.from_pretrained(&#39;pretrained_models&#x2F;roberta-base&#39;)</span><br><span class="line">    question &#x3D; &quot;&lt;s&gt; &quot; + question + &quot; &lt;&#x2F;s&gt;&quot;</span><br><span class="line">    question_tokenized &#x3D; tokenizer.tokenize(question)</span><br><span class="line">    print(&quot;question_tokenized1&quot;,question_tokenized)</span><br><span class="line">    question_tokenized &#x3D; pad_sequence(question_tokenized,10)</span><br><span class="line">    print(&quot;question_tokenized2&quot;,question_tokenized)</span><br><span class="line">    question_tokenized &#x3D; torch.tensor(tokenizer.encode(question_tokenized, add_special_tokens&#x3D;False))</span><br><span class="line">    print(&quot;question_tokenized&quot;,question_tokenized)</span><br><span class="line">    attention_mask &#x3D; []</span><br><span class="line">    for q in question_tokenized:</span><br><span class="line">        # 1 means padding token</span><br><span class="line">        if q &#x3D;&#x3D; 1:</span><br><span class="line">            attention_mask.append(0)</span><br><span class="line">        else:</span><br><span class="line">            attention_mask.append(1)</span><br><span class="line">    return question_tokenized, torch.LongTensor(attention_mask)</span><br><span class="line"></span><br><span class="line">def buid_model():</span><br><span class="line">    question &#x3D; [&quot;apple chen computer&quot;,&quot;are you ok&quot;] # I assume the batch_size&#x3D;&#x3D;2, so the input of RobertaModel.from_pretrained must a batch</span><br><span class="line">    question_tokenizeds&#x3D;[]</span><br><span class="line">    attention_masks&#x3D;[]</span><br><span class="line">    for sub_question in question:</span><br><span class="line">        question_tokenized,attention_mask&#x3D;toke(sub_question)</span><br><span class="line">        question_tokenizeds.append(list(np.array(question_tokenized)))</span><br><span class="line">        attention_masks.append(list(np.array(attention_mask)))</span><br><span class="line"></span><br><span class="line">    question_tokenizeds&#x3D;torch.LongTensor(question_tokenizeds)</span><br><span class="line">    attention_masks &#x3D; torch.LongTensor(attention_masks)</span><br><span class="line"></span><br><span class="line">    roberta_model &#x3D; RobertaModel.from_pretrained(&#39;pretrained_models&#x2F;roberta-base&#39;)</span><br><span class="line"></span><br><span class="line">    data_shape&#x3D;(roberta_model.embeddings.word_embeddings.weight.shape)#torch.Size([50265, 768])</span><br><span class="line"></span><br><span class="line">    roberta_last_hidden_states &#x3D; roberta_model(question_tokenizeds,attention_masks)[0]#roberta_last_hidden_states.size()torch.Size([2, 10, 768]) &#123;batch_size,sentence_length,embedding_dim &#125;</span><br><span class="line"></span><br><span class="line">    states &#x3D; roberta_last_hidden_states.transpose(1,0) #torch.Size([10, 2, 768]) &#123;sentence_length,batch_size,embedding_dim&#125;</span><br><span class="line">    cls_embedding &#x3D; states[0] # I do not know why authors use states[0] </span><br><span class="line">    question_embedding &#x3D; cls_embedding #torch.Size([2, 768])</span><br><span class="line">    print(question_embedding)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    buid_model()</span><br><span class="line">    print(&quot;--------&quot;)</span><br><span class="line">    # print(attention_mask)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Why-use-states-0-as-the-question-emedding"><a href="#Why-use-states-0-as-the-question-emedding" class="headerlink" title="Why use states[0] as the question emedding? "></a><font color=orange>Why use states[0] as the question emedding? </font></h3><p>RoBert is same as BERT, from BERT paper, you will find  states[0]  refers to CLS token, BERT author’s describtion as follow: </p><ul><li>The ﬁrst token of every sequence is always a special classiﬁcation token ([CLS]). The ﬁnal hidden state corresponding to this token is used as the aggregate sequence representation for classiﬁcation tasks</li></ul><p>Note: You can refer <a href="https://github.com/malllabiisc/EmbedKGQA/issues/2">Here</a>, the author have solved my issues.</p><h3 id="Summary-of-pre-training-model"><a href="#Summary-of-pre-training-model" class="headerlink" title="Summary of pre-training model"></a><font color=orange>Summary of pre-training model</font></h3><p>*BERT</p><p>*RoBERTa</p><p>*GPT-2</p><p>*XLNet</p><p>*DistilBERT</p><h3 id="Speical-token-G-in-RoBERT"><a href="#Speical-token-G-in-RoBERT" class="headerlink" title="Speical token Ġ in RoBERT"></a><font color=orange>Speical token Ġ in RoBERT</font></h3><p><a href="https://blog.csdn.net/weixin_43791511/article/details/106269047?fps=1&locationNum=2">refer1</a></p><p><a href="https://huggingface.co/blog/how-to-train">refer2</a>:<br>注意这个BPE和Bert原有的不太一样，原有的BPE在处理英文或者法文这样类似的文本时，可以当成一样的，但是在处理中文日文等，其是word-level的，也就是不再以字节为单位进行成对出现的(一个汉字一般占两个字节)，这样的编码方式在语料过多时，会使得词典变得很大(10-100K)，所以其采用了按Byte进行配对，无论中英文还是别的，只要是文本，那就按照其存储的字节进行BPE编码，这样大致可以使得字典数缩减到一个可以接受的值(50K)。这样左还有一个好处，它对于任何输入文本，基本都能进行编码，50K的这样的词典，基本不需要UNK</p><p><a href="https://blog.csdn.net/qq_28168421/article/details/104489455">refer3</a></p><p>In my view it’s a Byte-Pair <a href="https://arxiv.org/pdf/1508.07909.pdf">Encoding(BPE)</a>, apple–&gt;Ġapple</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Iterator in RotatE</title>
      <link href="2020/07/01/Iterator-in-RotatE/"/>
      <url>2020/07/01/Iterator-in-RotatE/</url>
      
        <content type="html"><![CDATA[<p>In its training progress, for step in (all step), I found that, one step refers to one batch. This is different for my previous experience (model need to train all data in one step)</p><p>In the first, they create dataloader in pytorch,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_dataloader_head &#x3D; DataLoader(</span><br><span class="line">            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size, &#39;head-batch&#39;),</span><br><span class="line">            batch_size&#x3D;args.batch_size,</span><br><span class="line">            shuffle&#x3D;True,</span><br><span class="line">            num_workers&#x3D;max(1, args.cpu_num&#x2F;&#x2F;2),</span><br><span class="line">            collate_fn&#x3D;TrainDataset.collate_fn</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        train_dataloader_tail &#x3D; DataLoader(</span><br><span class="line">            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size, &#39;tail-batch&#39;),</span><br><span class="line">            batch_size&#x3D;args.batch_size,</span><br><span class="line">            shuffle&#x3D;True,</span><br><span class="line">            num_workers&#x3D;max(1, args.cpu_num&#x2F;&#x2F;2),</span><br><span class="line">            collate_fn&#x3D;TrainDataset.collate_fn</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>Second, to make a iterator</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">train_iterator &#x3D; BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BidirectionalOneShotIterator(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, dataloader_head, dataloader_tail):</span><br><span class="line">        self.iterator_head &#x3D; self.one_shot_iterator(dataloader_head)</span><br><span class="line">        # print(&quot;bb&quot;,next(self.iterator_head))  #一个batch的</span><br><span class="line">        self.iterator_tail &#x3D; self.one_shot_iterator(dataloader_tail)</span><br><span class="line">        self.step &#x3D; 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def __next__(self):</span><br><span class="line">        self.step +&#x3D; 1</span><br><span class="line"></span><br><span class="line">        if self.step % 2 &#x3D;&#x3D; 0:</span><br><span class="line">            data &#x3D; next(self.iterator_head)</span><br><span class="line">        else:</span><br><span class="line">            data &#x3D; next(self.iterator_tail)</span><br><span class="line">        print(&quot;self.step&quot;, self.step)</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    @staticmethod</span><br><span class="line">    def one_shot_iterator(dataloader):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        Transform a PyTorch Dataloader into python iterator</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        while True:</span><br><span class="line">            for data in dataloader:</span><br><span class="line">                yield data</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this code, self.one_shot_iterator(dataloader_head) same as a iter in python, you need use next to run it, the data for next in def <strong>next</strong>(self) is diferent in every steps.</p><p>At last, get into loop:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> for step in range(init_step, 5):</span><br><span class="line">        log &#x3D; kge_model.train_step(kge_model, optimizer, train_iterator, args)</span><br><span class="line"></span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">ef train_step(model, optimizer, train_iterator, args):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        A single train step. Apply back-propation and return the loss</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        positive_sample, negative_sample, subsampling_weight, mode &#x3D; next(train_iterator)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>the next in  train_step just call the def <strong>next</strong>(self) in class BidirectionalOneShotIterator.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch install</title>
      <link href="2020/07/01/Pytorch-install/"/>
      <url>2020/07/01/Pytorch-install/</url>
      
        <content type="html"><![CDATA[<p>In the first, you can log the <a href="https://pytorch.org/">Pytorch_web</a> to get the Run this Command, such that, “pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f <a href="https://download.pytorch.org/whl/torch_stable.html&quot;">https://download.pytorch.org/whl/torch_stable.html&quot;</a>.</p><p>If this is faild, you can go to <a href="https://download.pytorch.org/whl/torch_stable.html">Download path</a>, to get the wheel. win, linux or mac..</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install torch-1.4.0+cpu-cp36-cp36m-win_amd64.whl</span><br><span class="line">conda install pytorch-1.2.0-py3.6_cpu_1.tar.bz2</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Install Torchvision：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Anaconda:</span><br><span class="line">conda install torchvision -c soumith</span><br><span class="line">or</span><br><span class="line">conda install torchvision -c pytorch</span><br><span class="line"></span><br><span class="line">pip:</span><br><span class="line">pip install torchvision</span><br></pre></td></tr></table></figure><p>TensorboardX</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir&#x3D;&#x3D;kge --host&#x3D;127.0.0.1  (host must be 127.0.0.1)</span><br><span class="line">kge  is the path</span><br><span class="line">you must enter the log path, </span><br><span class="line">tensorboard --logdir&#x3D;&#x3D;G:\Project_Li\2020_QA\Multi_hop_KGQA\repro_work\RotatE\KGERotatE_analyze_version1\runs\kge --host&#x3D;127.0.0.1</span><br><span class="line"></span><br><span class="line">There is a joke, I did not full the path(G:..), just kge, when I open &quot;http:&#x2F;&#x2F;127.0.0.1:6006&quot;, view &quot;tensorboard No scalar data was found&quot;, oh, fuck.</span><br><span class="line">Sometimes you need full the path.</span><br></pre></td></tr></table></figure><p>If you want to use TensorboardX, you should make sure you have installed tensorboard, tensorflow, pytorch,tensoboardX, The below is an excample</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import torch</span><br><span class="line"># import tensorflow as tf</span><br><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line">#</span><br><span class="line">writer1&#x3D;SummaryWriter(&quot;runs&#x2F;scal&quot;) # given a path, this path is used to save the log</span><br><span class="line">for i in range(10):</span><br><span class="line">    writer1.add_scalar(&quot;quadratic&quot;,i**2,global_step&#x3D;i)</span><br><span class="line">    writer1.add_scalar(&quot;exponentila&quot;,2**i,global_step&#x3D;i)</span><br><span class="line">print(torch.__version__)</span><br><span class="line">writer1.close() # it must be closed.</span><br></pre></td></tr></table></figure><p>Update scripy</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade scrapy -i http:&#x2F;&#x2F;pypi.douban.com&#x2F;simple&#x2F; --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>What&#39;s Natural Language Inference (NLI)</title>
      <link href="2020/06/14/What-s-Natural-Language-Inference-NLI/"/>
      <url>2020/06/14/What-s-Natural-Language-Inference-NLI/</url>
      
        <content type="html"><![CDATA[<p>This is subset about natural language processing. Can the premise (P) prove the inference of hypothesis (H)?  It’s can be seen as a classification problem.</p><ul><li>type1: Entailment (蕴含). P: 山东的沿海城市有青岛， 威海，烟台等。 H: 青岛是山东的一个沿海城市。</li><li>type2: Contradiction (矛盾). P: 硅谷在都柏林。 H: 人们传统称谓的硅谷位于美国加利福尼亚州.</li><li>type3：Neutral (独立). P: 谷歌是家高科技公司。 H: 百度是一家大型互联网公司。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today code model 20200613 Saturday</title>
      <link href="2020/06/13/Today-code-model-20200613-Saturday/"/>
      <url>2020/06/13/Today-code-model-20200613-Saturday/</url>
      
        <content type="html"><![CDATA[<h3 id="yml-python"><a href="#yml-python" class="headerlink" title="yml-python"></a><font color=orange>yml-python</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">webqsp.yml:</span><br><span class="line"> name: &#39;webqsp&#39;</span><br><span class="line">data_folder: &#39;datasets&#x2F;webqsp&#x2F;full&#x2F;&#39;</span><br><span class="line">train_data: &#39;train.json&#39;</span><br><span class="line">train_documents: &#39;documents.json&#39;</span><br><span class="line"></span><br><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import yaml</span><br><span class="line">import io</span><br><span class="line">with io.open(&quot;webqsp.yml&quot;,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as rf:</span><br><span class="line">    ss&#x3D;yaml.load(rf)</span><br><span class="line">    print(ss)</span><br><span class="line"></span><br><span class="line">result: &#123;&#39;name&#39;: &#39;webqsp&#39;, &#39;data_folder&#39;: &#39;datasets&#x2F;webqsp&#x2F;full&#x2F;&#39;, &#39;train_data&#39;: &#39;train.json&#39;, &#39;train_documents&#39;: &#39;documents.json&#39;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Wikipedia-in-GraftNet"><a href="#Wikipedia-in-GraftNet" class="headerlink" title="Wikipedia  in GraftNet"></a><font color=orange>Wikipedia  in GraftNet</font></h3><p>In GraftNet, the author used  Wikipedia as the corpus and retrieve text at the sentence level. This file is in documents.json–their source code.<br>file style:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&quot;document&quot;: </span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">1: &quot;entities&quot;: [&#123;&quot;text&quot;: &quot;&lt;fb:m.083jzs&gt;&quot;, &quot;end&quot;: 19, &quot;name&quot;: &quot;Where the Heart Is (UK TV series)&quot;, &quot;start&quot;: 15&#125;], </span><br><span class="line"></span><br><span class="line">2: &quot;text&quot;: &quot;She then signed on to play the lead role of a teenage mother in \&quot;Where the Heart Is\&quot;, which opened in April 2000.&quot;&#125;, </span><br><span class="line"></span><br><span class="line">3: &quot;documentId&quot;: 1070&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="np-full-x-y"><a href="#np-full-x-y" class="headerlink" title="np.full(x,y)"></a><font color=orange>np.full(x,y)</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">document_texts &#x3D; dict()</span><br><span class="line">document_texts[-1] &#x3D; np.full(4, 5, dtype&#x3D;int) # 5 is the element in this tuple</span><br><span class="line">print(document_texts) #&#123;-1: array([5, 5, 5, 5])&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today code model 20200612</title>
      <link href="2020/06/12/Today-code-model-20200612/"/>
      <url>2020/06/12/Today-code-model-20200612/</url>
      
        <content type="html"><![CDATA[<h3 id="Making-dict"><a href="#Making-dict" class="headerlink" title="Making dict"></a><font color=orange>Making dict</font></h3><p>question:<br>Input: WebQTrn-2280    <a href="fb:m.07t65">fb:m.07t65</a>=29320.8898392909,<a href="fb:m.07t65">fb:m.07t65</a>=10.7919105311015<br>Output: ‘WebQTrn-2280’: [‘<a href="fb:m.07t65">fb:m.07t65</a>‘, ‘<a href="fb:m.07t65">fb:m.07t65</a>‘]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def _read_seeds():</span><br><span class="line">    &quot;&quot;&quot;Return map from question ids to seed entities.&quot;&quot;&quot;</span><br><span class="line">    seed_map &#x3D; &#123;&#125;</span><br><span class="line">    with open(question_seeds) as f:</span><br><span class="line">        for line in f:</span><br><span class="line">            qId, seeds &#x3D; line.strip(&quot;\n&quot;).split(&quot;\t&quot;)</span><br><span class="line">            #seed_map[qId] &#x3D; [seeds.split(&quot;,&quot;, 1)[0].split(&quot;&#x3D;&quot;)[0]]</span><br><span class="line">            seed_map[qId] &#x3D; [seed.split(&quot;&#x3D;&quot;)[0]</span><br><span class="line">                             for seed in seeds.split(&quot;,&quot;) if seed]</span><br><span class="line">    return seed_map</span><br></pre></td></tr></table></figure><h3 id="csr-matrix"><a href="#csr-matrix" class="headerlink" title="csr_matrix"></a><font color=orange>csr_matrix</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">from scipy.sparse import csr_matrix</span><br><span class="line">import numpy as np</span><br><span class="line">row_ones&#x3D;[23,45]</span><br><span class="line">col_ones&#x3D;[43,65]</span><br><span class="line">num_entities&#x3D;100 # must big than the every element in row_ones and col_ones</span><br><span class="line">m &#x3D; csr_matrix(</span><br><span class="line">            (np.ones((len(row_ones),)), (np.array(row_ones), np.array(col_ones))),</span><br><span class="line">            shape&#x3D;(num_entities, num_entities))</span><br><span class="line">print(m)</span><br><span class="line">print(np.ones((len(row_ones),)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from scipy.sparse import csr_matrix</span><br><span class="line"></span><br><span class="line">row &#x3D; [0,0,0,1,1,1,2,2,2]#row index</span><br><span class="line">col &#x3D; [0,1,2,0,1,2,0,1,2]#col index</span><br><span class="line">data &#x3D; [1,10,1,23,1,4,1,34,8]#</span><br><span class="line">team &#x3D; csr_matrix((data,(row,col)),shape&#x3D;(3,3))</span><br><span class="line">print(team)</span><br><span class="line">print(team.todense())</span><br><span class="line"></span><br><span class="line">result:-----------------------</span><br><span class="line"> (0, 0)1</span><br><span class="line">  (0, 1)10</span><br><span class="line">  (0, 2)1</span><br><span class="line">  (1, 0)23</span><br><span class="line">  (1, 1)1</span><br><span class="line">  (1, 2)4</span><br><span class="line">  (2, 0)1</span><br><span class="line">  (2, 1)34</span><br><span class="line">  (2, 2)8</span><br><span class="line">[[ 1 10  1]</span><br><span class="line"> [23  1  4]</span><br><span class="line"> [ 1 34  8]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Choose-important-entities-in-GrafNet"><a href="#Choose-important-entities-in-GrafNet" class="headerlink" title="Choose important entities in GrafNet"></a><font color=orange>Choose important entities in GrafNet</font></h3><p>They run the Personalized PageRank(PPR) method around these seeds to identify other entities which might be an answer to the question. These seed are entity menthod which is computed by <a href="https://github.com/scottyih/STAGG">STAGG</a>. So the author used two files:  webquestions.examples.train.e2e.top10.filter.tsv train_links_raw and webquestions.examples.test.e2e.top10.filter.tsv test_links_raw.</p><p>After running PPR they retain the top E entities v1,…,vE by PPR score,along with any edges between them, and add them to Gq. Edge-weighted PPR is performed on the subset of each question starting from the seeds mentioned in that question to extract a subgraph with 500 most important entities. This PPR code is in step4_extract_subgraphs.py. This script describe the flow about how to use PPR to get the entities. I did not show the   step4_extract_subgraphs.py.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">realtion_map &#123;rel1:[[23,45](row_ones,head),[41,66](col_ones,tail)], rel2:[[2,4],[6,7]]&#125;</span><br><span class="line"></span><br><span class="line">s&#x3D;len(row_ones)&#x3D;2</span><br><span class="line">np.ones(s)&#x3D;[1,1]</span><br><span class="line">m:  (45,66)  1</span><br><span class="line">    (23,41)  1</span><br><span class="line"></span><br><span class="line">relation_map[rel1]: (45,66)  U(1)</span><br><span class="line">                    (23,41)  U(1)</span><br><span class="line">Note:U(x)&#x3D;normalize(x)*[(score)**2]</span><br><span class="line"></span><br><span class="line">multigraph_W&#x3D;adjmat:     (45,66)  B(1)</span><br><span class="line">                         (23,41)  B(1)</span><br><span class="line"> -----</span><br><span class="line">Note:B(x)&#x3D;U(x)&#x2F;len(relation_map)</span><br><span class="line"></span><br><span class="line">ppr &#x3D; _personalized_pagerank(seed, multigraph_W)</span><br><span class="line"></span><br><span class="line">seed &#x3D; np.zeros((multigraph_W.shape[0], 1))# multigraph_W.shape[0]: how many entity</span><br><span class="line">seed[entities] &#x3D; np.expand_dims(np.arange(len(entities), 0, -1),</span><br><span class="line">                                     axis&#x3D;1)</span><br><span class="line">seed &#x3D; seed &#x2F; seed.sum()</span><br><span class="line"> [0.]</span><br><span class="line"> [0.]</span><br><span class="line"> [0.]</span><br><span class="line"> [1.]</span><br><span class="line"></span><br><span class="line">________________________</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sorted_idx &#x3D; np.argsort(ppr)[::-1]</span><br><span class="line">extracted_ents &#x3D; sorted_idx[:MAX_ENT] #MAX_ENT&#x3D;500</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Personal Page Rank</title>
      <link href="2020/06/11/Personal%20Page%20Rank/"/>
      <url>2020/06/11/Personal%20Page%20Rank/</url>
      
        <content type="html"><![CDATA[<h3 id="Personal-Page-Rank"><a href="#Personal-Page-Rank" class="headerlink" title="Personal Page Rank"></a><font color=orange>Personal Page Rank</font></h3><p><a href="https://blog.csdn.net/harryhuang1990/article/details/10048383">refer</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># coding&#x3D;utf-8</span><br><span class="line"></span><br><span class="line">def PersonalRank(G, alpha, root, max_step):</span><br><span class="line">    rank &#x3D; &#123;x: 0 for x in G.keys()&#125;</span><br><span class="line"></span><br><span class="line">    rank[root] &#x3D; 1 #&#123;&#39;A&#39;: 1, &#39;B&#39;: 0, &#39;C&#39;: 0, &#39;a&#39;: 0, &#39;b&#39;: 0, &#39;c&#39;: 0, &#39;d&#39;: 0&#125;</span><br><span class="line"></span><br><span class="line">    for k in range(max_step):</span><br><span class="line">        tmp &#x3D; &#123;x: 0 for x in G.keys()&#125;</span><br><span class="line">        for i, ri in G.items():</span><br><span class="line">            for j, wij in ri.items():</span><br><span class="line">                tmp[j] +&#x3D; alpha * rank[i] &#x2F; (1.0 * len(ri))</span><br><span class="line">        tmp[root] +&#x3D; (1 - alpha)</span><br><span class="line">        rank &#x3D; tmp</span><br><span class="line"></span><br><span class="line">        # the weight of every node</span><br><span class="line">        print(&#39;iter: &#39; + str(k) + &quot;\t&quot;)</span><br><span class="line">        for key, value in rank.items():</span><br><span class="line">            print (&quot;%s:%.3f, \t&quot; % (key, value))</span><br><span class="line"></span><br><span class="line">    return rank</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    G &#x3D; &#123;&#39;A&#39;: &#123;&#39;a&#39;: 1, &#39;c&#39;: 1&#125;,</span><br><span class="line">         &#39;B&#39;: &#123;&#39;a&#39;: 1, &#39;b&#39;: 1, &#39;c&#39;: 1, &#39;d&#39;: 1&#125;,</span><br><span class="line">         &#39;C&#39;: &#123;&#39;c&#39;: 1, &#39;d&#39;: 1&#125;,</span><br><span class="line">         &#39;a&#39;: &#123;&#39;A&#39;: 1, &#39;B&#39;: 1&#125;,</span><br><span class="line">         &#39;b&#39;: &#123;&#39;B&#39;: 1&#125;,</span><br><span class="line">         &#39;c&#39;: &#123;&#39;A&#39;: 1, &#39;B&#39;: 1, &#39;C&#39;: 1&#125;,</span><br><span class="line">         &#39;d&#39;: &#123;&#39;B&#39;: 1, &#39;C&#39;: 1&#125;&#125;</span><br><span class="line"></span><br><span class="line">    PersonalRank(G, 0.5, &#39;A&#39;, 5)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today code model 20200610</title>
      <link href="2020/06/10/Today-code-model-20200610/"/>
      <url>2020/06/10/Today-code-model-20200610/</url>
      
        <content type="html"><![CDATA[<h3 id="reload-stopwords-from-NLTK"><a href="#reload-stopwords-from-NLTK" class="headerlink" title="reload stopwords from NLTK"></a><font color=orange>reload stopwords from NLTK</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from nltk.corpus import stopwords as SW</span><br><span class="line">stopwords &#x3D; set(SW.words(&quot;english&quot;))</span><br><span class="line"></span><br><span class="line">stopwords.add(&quot;&#39;s&quot;)</span><br><span class="line">print(stopwords)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">&#123;&quot;you&#39;ve&quot;, &#39;hasn&#39;, &#39;until&#39;, &#39;hadn&#39;, &quot;weren&#39;t&quot;, &#39;over&#39;, &#39;each&#39;, &#39;weren&#39;, &#39;o&#39;, &#39;d&#39;, &quot;hasn&#39;t&quot;, &quot;she&#39;s&quot;,...&#125;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><h3 id="Error-tokenizers-punkt-english-pickle’-not-found"><a href="#Error-tokenizers-punkt-english-pickle’-not-found" class="headerlink" title="Error: tokenizers/punkt/english.pickle’ not found"></a><font color=orange>Error: tokenizers/punkt/english.pickle’ not found</font></h3><p>There is an excample for word segment and remove the stopwords by using need NLTK</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import nltk</span><br><span class="line">from nltk.corpus import stopwords as SW</span><br><span class="line">stopwords &#x3D; set(SW.words(&quot;english&quot;))</span><br><span class="line">def extract_keywords(text):</span><br><span class="line">    &quot;&quot;&quot;Remove wh-words and stop words from text.&quot;&quot;&quot;</span><br><span class="line">    return u&quot; &quot;.join([token for token in nltk.word_tokenize(text)</span><br><span class="line">        if token not in stopwords])</span><br><span class="line"></span><br><span class="line">print(extract_keywords(&quot;This is a text for the test&quot;))</span><br></pre></td></tr></table></figure><p>You may need this error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Resource &#39;tokenizers&#x2F;punkt&#x2F;english.pickle&#39; not found.  Please</span><br><span class="line">  use the NLTK Downloader to obtain the resource:  &gt;&gt;&gt;</span><br><span class="line">  nltk.download()</span><br><span class="line">  Searched in:</span><br><span class="line">    - &#39;C:\\Users\\mcli2&#x2F;nltk_data&#39;</span><br><span class="line">    - &#39;C:\\nltk_data&#39;</span><br><span class="line">    - &#39;D:\\nltk_data&#39;</span><br></pre></td></tr></table></figure><p>You can use nltk.download() or nltk.download(“punkt”). But I found there still have error. The best way is to download the  “punkt” from <a href="http://www.nltk.org/nltk_data/">NLTK Corpora</a>, and put it in the path “Searched in…”. </p><h3 id="WebQuestionsSP-progress"><a href="#WebQuestionsSP-progress" class="headerlink" title="WebQuestionsSP progress"></a><font color=orange>WebQuestionsSP progress</font></h3><p>The source code from GraftNet–step0_preprocess_webqsp.py</p><ul><li>QuestionId: the ID of the question, such as: WebQTrn-0</li><li>QuestionText: They take the “ProcessedQuestion”, WebQuestionsSP also have RawQuestion. excample: ProcessedQuestion: “what is the name of justin bieber brother”, RawQuestion: “what is the name of justin bieber brother?”</li><li>QuestionKeywords:  the question is progressed by segment and remove stopword.</li><li><font color=blue>Answers-freebaseId</font>: the answer’s ID in freebase, the answer is a entity. Excample: m.0gxnnwq</li><li> <font color=blue>Answers-text=EntityName</font>: entity’s name, EntityName”: “Jaxon Bieber”. Freebase ID:  m.0gxnnwq.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import json</span><br><span class="line">import nltk</span><br><span class="line">import os</span><br><span class="line">import io</span><br><span class="line"></span><br><span class="line">from nltk.corpus import stopwords as SW</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def extract_keywords(text):</span><br><span class="line">    &quot;&quot;&quot;Remove wh-words and stop words from text.&quot;&quot;&quot;</span><br><span class="line">    return u&quot; &quot;.join([token for token in nltk.word_tokenize(text)</span><br><span class="line">        if token not in stopwords])</span><br><span class="line"></span><br><span class="line">def get_answers(question):</span><br><span class="line">    &quot;&quot;&quot;extract unique answers from question parses.&quot;&quot;&quot;</span><br><span class="line">    answers &#x3D; set()</span><br><span class="line">    for parse in question[&quot;Parses&quot;]:</span><br><span class="line">        for answer in parse[&quot;Answers&quot;]:</span><br><span class="line">            answers.add((answer[&quot;AnswerArgument&quot;],</span><br><span class="line">                answer[&quot;EntityName&quot;]))</span><br><span class="line">    return answers</span><br><span class="line"></span><br><span class="line">def get_entities(question):</span><br><span class="line">    &quot;&quot;&quot;extract oracle entities from question parses.&quot;&quot;&quot;</span><br><span class="line">    entities &#x3D; set()</span><br><span class="line">    for parse in question[&quot;Parses&quot;]:</span><br><span class="line">        if parse[&quot;TopicEntityMid&quot;] is not None:</span><br><span class="line">            entities.add((parse[&quot;TopicEntityMid&quot;], parse[&quot;TopicEntityName&quot;]))</span><br><span class="line">    return entities</span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    # out_json &#x3D; &quot;\webqsp_processed.json&quot;</span><br><span class="line">    base_path&#x3D;&quot;data\downloads\WebQSP\data&quot;</span><br><span class="line">    in_files &#x3D; [&quot;\WebQSP.train.json&quot;, &quot;\WebQSP.test.json&quot;]</span><br><span class="line">    out_json &#x3D; base_path+&quot;\webqsp_processed.json&quot;</span><br><span class="line">    stopwords &#x3D; set(SW.words(&quot;english&quot;))</span><br><span class="line">    stopwords.add(&quot;&#39;s&quot;)</span><br><span class="line">    questions &#x3D; []</span><br><span class="line"></span><br><span class="line">    for fil in in_files:</span><br><span class="line">        data &#x3D; json.load(open(base_path + fil,encoding&#x3D;&quot;utf-8&quot;))</span><br><span class="line">        for question in data[&quot;Questions&quot;]:</span><br><span class="line">            q_obj &#x3D; &#123;</span><br><span class="line">                &quot;QuestionId&quot;: question[&quot;QuestionId&quot;],</span><br><span class="line">                &quot;QuestionText&quot;: question[&quot;ProcessedQuestion&quot;],</span><br><span class="line">                &quot;QuestionKeywords&quot;: extract_keywords(question[&quot;ProcessedQuestion&quot;]),</span><br><span class="line">                &quot;OracleEntities&quot;: [</span><br><span class="line">                    &#123;&quot;freebaseId&quot;: &quot;&lt;fb:&quot; + entity[0] + &quot;&gt;&quot;, &quot;text&quot;: entity[1]&#125;</span><br><span class="line">                    for entity in get_entities(question)</span><br><span class="line">                ],</span><br><span class="line">                &quot;Answers&quot;: [</span><br><span class="line">                    &#123;&quot;freebaseId&quot;: &quot;&lt;fb:&quot; + answer[0] + &quot;&gt;&quot;</span><br><span class="line">                     if answer[0].startswith(&quot;m.&quot;) or answer[0].startswith(&quot;g.&quot;) else answer[0],</span><br><span class="line">                     &quot;text&quot;: answer[1]&#125;</span><br><span class="line">                    for answer in get_answers(question)</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">            questions.append(q_obj)</span><br><span class="line"></span><br><span class="line">    json.dump(questions, open(out_json, &quot;w&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="unscramble-step1-process-entity-links-py-in-GraftNet"><a href="#unscramble-step1-process-entity-links-py-in-GraftNet" class="headerlink" title="unscramble step1_process_entity_links.py in GraftNet"></a><font color=orange>unscramble step1_process_entity_links.py in GraftNet</font></h3><p>This is the script to get the entities of a question. The result:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">WebQTrn-1135&lt;fb:m.02lfw5&gt;&#x3D;22539.6397506544</span><br><span class="line">WebQTrn-1924&lt;fb:m.09c7w0&gt;&#x3D;12137.4751494713,&lt;fb:m.0868f&gt;&#x3D;16.0353132187641</span><br><span class="line">WebQTrn-1981&lt;fb:m.04k1p&gt;&#x3D;29722.1964697806</span><br><span class="line">WebQTrn-446&lt;fb:m.04wqr&gt;&#x3D;1039.59542839242</span><br><span class="line">WebQTrn-493&lt;fb:m.0154j&gt;&#x3D;25807.4905207448</span><br><span class="line">....</span><br></pre></td></tr></table></figure><p>The authors used <a href="https://github.com/scottyih/STAGG">STAGG</a> to filter the question’s entity. They use two files:  webquestions.examples.train.e2e.top10.filter.tsv train_links_raw and webquestions.examples.test.e2e.top10.filter.tsv test_links_raw.</p><p>The introduction about this two file:<br>file style: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">WebQTrn-5john noble1910&#x2F;m&#x2F;02fgm7John_Noble28691.6914572421</span><br><span class="line">WebQTrn-5lord of the rings3817&#x2F;m&#x2F;025h5sThe_Lord_of_the_Rings_(film_series)23.9769219225805</span><br><span class="line">WebQTrn-5lord of the rings3817&#x2F;m&#x2F;017jd9The_Lord_of_the_Rings%3a_The_Return_of_the_King17.6310243670667</span><br><span class="line">WebQTrn-5lord of the rings3817&#x2F;m&#x2F;07bz5The_Lord_of_the_Rings7.72448078871221</span><br><span class="line">WebQTrn-5lord of the rings3817&#x2F;m&#x2F;017gl1The_Lord_of_the_Rings%3a_The_Fellowship_of_the_Ring1.95900240303194</span><br><span class="line">WebQTrn-6joakim noah911&#x2F;m&#x2F;0c2yrfJoakim_Noah38026.6190248461</span><br></pre></td></tr></table></figure><p>The columns are: Question ID, Entity Mention, Start Position (of the normalized question), Length, Linked Entity ID, Entity Name, Entity Linking Score. We can see that in question WebQTrn-5 (this id just in data WebQuestionsSP), they are two entity mentions john noble and lord of the rings,  lord of the rings is a polysemy, it has four means which refer four different entities in freebase. </p><p>The authors in GraftNet used two method to filter the entity in a sentence.</p><ul><li>sccore &lt;10, remove, so above 7.72448078871221 is filterd. I really do not know why authors choose 10 as the mini-score, if I choose the mini-score is 8 or 15, what is the result.</li><li>remove the overlapping. code describtion.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def _overlap(span, spans):</span><br><span class="line">    for sp in spans:</span><br><span class="line">        if max(sp[0], span[0]) &lt; min(sp[1], span[1]):</span><br><span class="line">            return True</span><br><span class="line">    return False</span><br></pre></td></tr></table></figure>I give an excample to understand it, we have two entity mention in a sentence [A,B,C,D], the A is the first entity’s start position in this sentence, B is the end position. C is the second entity’s start position. max(A,C)&gt;min(B,D). Match this situation, the entity mention (except the first entity mention) is chosen.<br>Full code:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;Script to process STAGG links into format for retrieving subgraphs.</span><br><span class="line"></span><br><span class="line">Filters any non-top entity which has overlapping span with a higher ranked</span><br><span class="line">entity and any non-top entity which has score lower than MIN_SCORE.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">questions_file &#x3D; &quot;data&#x2F;scratch&#x2F;webqsp_processed.json&quot;</span><br><span class="line">link_files &#x3D; [&quot;data&#x2F;downloads&#x2F;train_links_raw.tsv&quot;, &quot;data&#x2F;downloads&#x2F;test_links_raw.tsv&quot;]</span><br><span class="line">entity_file &#x3D; &quot;data&#x2F;downloads&#x2F;freebase_prepro&#x2F;freebase_prepro&#x2F;freebase_2hops&#x2F;all_entities&quot;</span><br><span class="line">output &#x3D; &quot;data&#x2F;scratch&#x2F;stagg_linked_questions.txt&quot;</span><br><span class="line"></span><br><span class="line">MIN_SCORE &#x3D; 10.</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"></span><br><span class="line">def _convert_freebase_id(x):</span><br><span class="line">    return &quot;&lt;fb:&quot; + x[1:].replace(&quot;&#x2F;&quot;, &quot;.&quot;) + &quot;&gt;&quot;</span><br><span class="line"></span><br><span class="line">def _overlap(span, spans):</span><br><span class="line">    for sp in spans:</span><br><span class="line">        if max(sp[0], span[0]) &lt; min(sp[1], span[1]):</span><br><span class="line">            return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line">def _filter_links(links):</span><br><span class="line"> </span><br><span class="line">    f_links &#x3D; [links[0][:2]] #&#x2F;m&#x2F;02pg6z3&#39;, &#39;62402.2220055809</span><br><span class="line">    spans_covered &#x3D; [links[0][2:]] #[9, 917]</span><br><span class="line"></span><br><span class="line">    for item in links[1:]:</span><br><span class="line">        if float(item[1]) &lt; MIN_SCORE: continue #21133.5617340499</span><br><span class="line">        if _overlap(item[2:], spans_covered): continue</span><br><span class="line">        f_links.append(item[:2])</span><br><span class="line">        spans_covered.append(item[2:])</span><br><span class="line">    return f_links</span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    entities &#x3D; set()</span><br><span class="line">    with open(entity_file) as f:</span><br><span class="line">        for line in tqdm(f):</span><br><span class="line">            entities.add(line.strip())</span><br><span class="line"></span><br><span class="line">    question_ids &#x3D; set()</span><br><span class="line">    with open(questions_file) as f:</span><br><span class="line">        all_questions &#x3D; json.load(f)</span><br><span class="line">        question_ids.update([q[&quot;QuestionId&quot;] for q in all_questions])</span><br><span class="line"></span><br><span class="line">    entity_map &#x3D; &#123;qId: [] for qId in question_ids&#125; #&#123;&#39;WebQTrn-2038&#39;: [], &#39;WebQTrn-971&#39;: [], &#39;WebQTest-1427&#39;: []&#125;</span><br><span class="line"></span><br><span class="line">    f_out &#x3D; open(output, &quot;w&quot;)</span><br><span class="line">    for fil in link_files:</span><br><span class="line">        with open(fil) as f:</span><br><span class="line">            for line in f:</span><br><span class="line">                (qId, surface, start,</span><br><span class="line">                 length, fId, fsurface, score) &#x3D; line.strip().split(&quot;\t&quot;)</span><br><span class="line">                if _convert_freebase_id(fId) not in entities: continue</span><br><span class="line">                if qId in entity_map:</span><br><span class="line">                    entity_map[qId].append([fId, score, int(start), int(start) + int(length)])</span><br><span class="line">    </span><br><span class="line">    with open(output, &quot;w&quot;) as f_out:</span><br><span class="line">        for (qId, links) in entity_map.items():</span><br><span class="line">            if not links:</span><br><span class="line">                f_out.write(&quot;%s\t\n&quot; % qId)</span><br><span class="line">                continue</span><br><span class="line">            link_str &#x3D; &quot;,&quot;.join(</span><br><span class="line">                &quot;%s&#x3D;%s&quot; % (_convert_freebase_id(fId), score)</span><br><span class="line">                for fId, score in _filter_links(links))</span><br><span class="line">            f_out.write(&quot;%s\t%s\n&quot; % (qId, link_str))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Open-file"><a href="#Open-file" class="headerlink" title="Open file "></a><font color=orange>Open file </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with open(relations_file) as f:</span><br><span class="line">       for ii, line in enumerate(f):</span><br><span class="line">           relation &#x3D; line.strip()</span><br><span class="line">           print(relation)</span><br></pre></td></tr></table></figure><h3 id="line-strip-split-None-1"><a href="#line-strip-split-None-1" class="headerlink" title="line.strip().split(None, 1) "></a><font color=orange>line.strip().split(None, 1) </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">line&#x3D;&quot;d ddd ff ff&quot;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">s&#x3D;line.strip().split(None, 1)</span><br><span class="line">print(s) #[&#39;d&#39;, &#39;ddd ff ff&#39;]</span><br><span class="line">d&#x3D;line.strip().split(None, 2)</span><br><span class="line">print(d) #[&#39;d&#39;, &#39;ddd&#39;, &#39;ff ff&#39;]</span><br></pre></td></tr></table></figure></li></ul><h3 id="The-method-to-obtain-the-relation-embedding-from-GraftNet"><a href="#The-method-to-obtain-the-relation-embedding-from-GraftNet" class="headerlink" title="The method to obtain the relation embedding from GraftNet"></a><font color=orange>The method to obtain the relation embedding from GraftNet</font></h3><p>This script from the source  code in Graftnet. In the next I will give the relaiton compution progress, and point out the shortcoming (in my view), in the last, I will give the code.<br><font color=red>relaiton compution progress</font><br>In the first they made two dict by the relations in Freebase, dict word_to_relation and  relation_lens.</p><ul><li>(1) <font color=red>dict: word_to_relation</font>, it’s a key: value, I give the key=”football”, value as below:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">line1: (&#39;&lt;fb:american_football.football_coach.coaching_history&gt;&#39;, 1)</span><br><span class="line">line2: (&#39;&lt;fb:american_football.football_coach.coaching_history&gt;&#39;, 2)</span><br><span class="line">(&#39;&lt;fb:american_football.football_coach.current_team_head_coached&gt;&#39;, 1)</span><br><span class="line">(&#39;&lt;fb:american_football.football_coach.current_team_head_coached&gt;&#39;, 2)</span><br><span class="line">(&#39;&lt;fb:american_football.football_historical_coach_position.coach&gt;&#39;, 1)</span><br><span class="line">(&#39;&lt;fb:american_football.football_historical_coach_position.coach&gt;&#39;, 2)</span><br><span class="line">(&#39;&lt;fb:american_football.football_historical_coach_position.team&gt;&#39;, 1)</span><br><span class="line">(&#39;&lt;fb:american_football.football_historical_coach_position.team&gt;&#39;, 2)</span><br></pre></td></tr></table></figure>The value has two port: port 1: relation, port2: the number, such as, 1,2. (every relation can be divided by “.”, such as [american_football, football_coach, coaching_history], the author give the number [1,2,3]. In line1, the football’s number is 1, because its position is 1,  in line2, it’s number is 2. </li><li>(2)<font color=red>dict: relation_lens</font>, relation and the number which is computed  by the word number.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;&lt;fb:american_football.football_coach.coaching_history&gt;&#39;: 12, &#39;&lt;fb:american_football.football_coach.current_team_head_coached&gt;&#39;: 18&#125;</span><br></pre></td></tr></table></figure><p>as below:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def _add_word(word, t, v):</span><br><span class="line">    if word not in word_to_relation: word_to_relation[word] &#x3D; []</span><br><span class="line">    word_to_relation[word].append((v, t))</span><br><span class="line">    if v not in relation_lens: relation_lens[v] &#x3D; 0</span><br><span class="line">    relation_lens[v] +&#x3D; t</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    with open(relations_file) as f:</span><br><span class="line">        for ii, line in enumerate(f):</span><br><span class="line">            relation &#x3D; line.strip() #&lt;fb:american_football.football_player.position_s&gt;</span><br><span class="line">            # print(relation)</span><br><span class="line">            domain, typ, prop &#x3D; relation[4:-1].split(&quot;.&quot;)[-3:] # Why add [-3:], add and no add have the same situation</span><br><span class="line">            &quot;&quot;&quot;[&#39;visual_art&#39;, &#39;artwork&#39;, &#39;art_form&#39;]&quot;&quot;&quot;</span><br><span class="line">            for word in domain.split(&quot;_&quot;):</span><br><span class="line">                _add_word(word, 1, relation)</span><br><span class="line">            for word in typ.split(&quot;_&quot;):</span><br><span class="line">                _add_word(word, 2, relation)</span><br><span class="line">            for word in prop.split(&quot;_&quot;):</span><br><span class="line">                _add_word(word, 3, relation)</span><br></pre></td></tr></table></figure><ul><li>(3) At last, they used this way to compute the relation:<br>such as: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">line1: (&#39;&lt;fb:american_football.football_coach.coaching_history&gt;&#39;, 1)</span><br><span class="line">line2: (&#39;&lt;fb:american_football.football_coach.coaching_history&gt;&#39;, 2)</span><br></pre></td></tr></table></figure>the vector of “<a href="fb:american_football.football_coach.coaching_history">fb:american_football.football_coach.coaching_history</a>“ is computed by </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vec&#x3D;1*(glove(american)+glove(football)+glove(football)+glove(coach)+glove(coaching)+glove(history))</span><br><span class="line">and then Vec&#x3D;Vec&#x2F;relation_lens[relation]. </span><br></pre></td></tr></table></figure><p><font color=red>I do not think this is a good idea to compute the  relation embedding, it not includes semantic information, why multing the position number (1,2,3)? why dividing relation_lens[relation], a little decition can make big different </font>.</p><p><font color=red>Full code</font>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;Script to compute relation embeddings for each relation in given list.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">relations_file &#x3D; &quot;data&#x2F;downloads&#x2F;freebase_prepro&#x2F;freebase_prepro&#x2F;freebase_2hops&#x2F;relations&quot;</span><br><span class="line">embeddings_file &#x3D; &quot;data&#x2F;downloads&#x2F;glove.840B.300d.txt&quot;</span><br><span class="line">output_file &#x3D; &quot;data&#x2F;scratch&#x2F;relation_emb.pkl&quot;</span><br><span class="line">dim &#x3D; 300</span><br><span class="line"></span><br><span class="line">import pickle as pkl</span><br><span class="line">import numpy as np</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"></span><br><span class="line">word_to_relation &#x3D; &#123;&#125;</span><br><span class="line">&quot;&quot;&quot;word_to_relation: &#123;&#39;american&#39;: [(&#39;&lt;fb:american_football.football_coach.coaching_history&gt;&#39;, 1), (&#39;&lt;fb:american_football.football_coach.current_team_head_coached&gt;&#39;, 1),&quot;&quot;&quot;</span><br><span class="line">relation_lens &#x3D; &#123;&#125; # relation: the number of this relation</span><br><span class="line">&quot;&quot;&quot;relation_lens: &#123;&#39;&lt;fb:american_football.football_coach.coaching_history&gt;&#39;: 12, &#39;&lt;fb:american_football.football_coach.current_team_head_coached&gt;&#39;: 18,....&quot;&quot;&quot;</span><br><span class="line">def _add_word(word, t, v):</span><br><span class="line">    if word not in word_to_relation: word_to_relation[word] &#x3D; []</span><br><span class="line">    word_to_relation[word].append((v, t))</span><br><span class="line">    if v not in relation_lens: relation_lens[v] &#x3D; 0</span><br><span class="line">    relation_lens[v] +&#x3D; t</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    with open(relations_file,encoding&#x3D;&quot;utf-8&quot;) as f:</span><br><span class="line">        for ii, line in enumerate(f):</span><br><span class="line">            relation &#x3D; line.strip() #&lt;fb:american_football.football_player.position_s&gt;</span><br><span class="line">            domain, typ, prop &#x3D; relation[4:-1].split(&quot;.&quot;)[-3:] # Why add [-3:], add and no add have the same situation</span><br><span class="line">            &quot;&quot;&quot;[&#39;visual_art&#39;, &#39;artwork&#39;, &#39;art_form&#39;]&quot;&quot;&quot;</span><br><span class="line">            for word in domain.split(&quot;_&quot;):</span><br><span class="line">                _add_word(word, 1, relation)</span><br><span class="line">            for word in typ.split(&quot;_&quot;):</span><br><span class="line">                _add_word(word, 2, relation)</span><br><span class="line">            for word in prop.split(&quot;_&quot;):</span><br><span class="line">                _add_word(word, 3, relation)</span><br><span class="line"></span><br><span class="line">    relation_emb &#x3D; &#123;r: np.zeros((dim,)) for r in relation_lens&#125;</span><br><span class="line">    with open(embeddings_file,encoding&#x3D;&quot;utf-8&quot;) as f:</span><br><span class="line">        for line in tqdm(f):</span><br><span class="line">            word_vec &#x3D; line.strip().split(&quot; &quot;) # the source code used word, vec &#x3D; line.strip().split(None, 1), but it can make mistake</span><br><span class="line">            word&#x3D;word_vec[0]</span><br><span class="line">            vec&#x3D;word_vec[1:]</span><br><span class="line"></span><br><span class="line">            if word in word_to_relation:</span><br><span class="line">                for rid, typ in word_to_relation[word]:</span><br><span class="line">                    relation_emb[rid] +&#x3D; typ * np.array(</span><br><span class="line">                        [float(vv) for vv in vec])</span><br><span class="line">    for relation in relation_emb:</span><br><span class="line">        relation_emb[relation] &#x3D; relation_emb[relation] &#x2F; relation_lens[relation]</span><br><span class="line">    print(relation_emb)</span><br><span class="line"></span><br><span class="line">    pkl.dump(relation_emb, open(output_file, &quot;wb&quot;))</span><br></pre></td></tr></table></figure><h3 id="The-method-to-obtain-the-question-embedding-from-GraftNet"><a href="#The-method-to-obtain-the-question-embedding-from-GraftNet" class="headerlink" title="The method to obtain the question embedding from GraftNet"></a><font color=orange>The method to obtain the question embedding from GraftNet</font></h3><p>This method is same as above, this script from <font color=red>step3_question_embeddings.py</font>, the source code of GraftNet. I just modify a little thing to let it run in my machine.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;Script to compute question embeddings for given questions.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">questions_file &#x3D; &quot;data&#x2F;scratch&#x2F;webqsp_processed.json&quot;</span><br><span class="line">embeddings_file &#x3D;&quot;data&#x2F;downloads&#x2F;glove.840B.300d.txt&quot;</span><br><span class="line">output_file &#x3D; &quot;data&#x2F;scratch&#x2F;webqsp_embeddings.pkl&quot;</span><br><span class="line">dim &#x3D; 300</span><br><span class="line"></span><br><span class="line">import pickle as pkl</span><br><span class="line">import numpy as np</span><br><span class="line">import json</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"></span><br><span class="line">word_to_question &#x3D; &#123;&#125;</span><br><span class="line">question_lens &#x3D; &#123;&#125;</span><br><span class="line">def _add_word(word, v):</span><br><span class="line">    if word not in word_to_question: word_to_question[word] &#x3D; []</span><br><span class="line">    word_to_question[word].append(v)</span><br><span class="line">    if v not in question_lens: question_lens[v] &#x3D; 0</span><br><span class="line">    question_lens[v] +&#x3D; 1</span><br><span class="line"></span><br><span class="line">with open(questions_file) as f:</span><br><span class="line">    questions &#x3D; json.load(f)</span><br><span class="line">    for ii, question in enumerate(questions):</span><br><span class="line">        qId, question_text &#x3D; question[&quot;QuestionId&quot;], question[&quot;QuestionKeywords&quot;]</span><br><span class="line">        for word in question_text.split():</span><br><span class="line">            _add_word(word, qId)</span><br><span class="line"></span><br><span class="line">question_emb &#x3D; &#123;r: np.zeros((dim,)) for r in question_lens&#125;</span><br><span class="line">with open(embeddings_file,encoding&#x3D;&quot;utf-8&quot;) as f:</span><br><span class="line">    for line in tqdm(f):</span><br><span class="line">        word_vec &#x3D; line.strip().split(&quot; &quot;)  # the source code used word, vec &#x3D; line.strip().split(None, 1), but it can make mistake</span><br><span class="line">        word &#x3D; word_vec[0]</span><br><span class="line">        vec &#x3D; word_vec[1:]</span><br><span class="line">        if word in word_to_question:</span><br><span class="line">            for qid in word_to_question[word]:</span><br><span class="line">                question_emb[qid] +&#x3D; np.array([float(vv) for vv in vec])</span><br><span class="line"></span><br><span class="line">for question in question_emb:</span><br><span class="line">    question_emb[question] &#x3D; question_emb[question] &#x2F; question_lens[question]</span><br><span class="line"></span><br><span class="line">pkl.dump(question_emb, open(output_file, &quot;wb&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today Break 20200608</title>
      <link href="2020/06/08/Today-Break-20200608/"/>
      <url>2020/06/08/Today-Break-20200608/</url>
      
        <content type="html"><![CDATA[<p>This is a wonderful day, I found a basketball court in my city-QIHE, and played with some baseketball lovers. Last week, I have read and run the source code about <font color=blue>EMbedGQA</font>. Although there are some issues in it, I have learnt many skills about progressing data, building the model by using the  <font color=blue>ComplEx and RoBERTa (pytorch)</font>. In this week, I will read and analyse the paper about <font color=blue>GRAFT-Net</font>, it’s a model about multi-hop KBQA, and was proposed by Carnegie Mellon University in 2018. I wish I can complete my model and model paper before 2020.08. So, come on!</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today code and error model Sunday</title>
      <link href="2020/06/07/Today-code-and-error-model-sunday/"/>
      <url>2020/06/07/Today-code-and-error-model-sunday/</url>
      
        <content type="html"><![CDATA[<h3 id="what’s-requires-grad-in-pytorch"><a href="#what’s-requires-grad-in-pytorch" class="headerlink" title="what’s requires_grad in pytorch"></a><font color=orange>what’s requires_grad in pytorch</font></h3><p>We can by tensor.requires_grad to check if this tensor ineed to be token a derivative. In a model, many parameters (such as weight, bias) should take a derivative,  by the input  and Ground Truth (right label) should not take this.<br>look this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">input &#x3D; torch.randn(7, 3, 25, 50)</span><br><span class="line">print(input.requires_grad)</span><br><span class="line">import torch.nn as nn</span><br><span class="line">- False</span><br><span class="line"></span><br><span class="line">net &#x3D; nn.Sequential(nn.Conv2d(3, 8, 3, 1),</span><br><span class="line">                    nn.Conv2d(8, 16, 3, 1))</span><br><span class="line">for param in net.named_parameters():</span><br><span class="line">    param[1].requires_grad &#x3D; False</span><br><span class="line">    print(param[0], param[1].requires_grad)  </span><br><span class="line">- 0.weight True</span><br><span class="line">- 0.bias True</span><br><span class="line">- 1.weight True</span><br><span class="line">- 1.bias True</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>if we set requires_grad = False, what’s situation? I think there are two aspets:</p><ul><li>If we use the pre-trained model, we assume already have this model, we just use this trained model to do something, such as roBERT, the parameters is the best, we should not train them again, so we should set requires_grad = False.</li><li>We can freeze some layers not to update, just train one or two layers, as bellow:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model &#x3D; torchvision.models.resnet18(pretrained&#x3D;True)</span><br><span class="line">for param in model.parameters():</span><br><span class="line">    param.requires_grad &#x3D; False</span><br><span class="line"></span><br><span class="line"># use a new fc to replace the full-connected layer</span><br><span class="line"># in the new fc layer, requires_grad&#x3D;True (default)</span><br><span class="line">model.fc &#x3D; nn.Linear(512, 100)</span><br><span class="line"></span><br><span class="line"># just update the parameters in fc layers</span><br><span class="line">optimizer &#x3D; optim.SGD(model.fc.parameters(), lr&#x3D;1e-2, momentum&#x3D;0.9)</span><br><span class="line"></span><br><span class="line"># by this, we freeze  the layers before fc in resnet, just update the paremeters in fc layers in training</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a><font color=orange>tqdm</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from tqdm import  tqdm</span><br><span class="line">loader&#x3D;[1,2,3,4,5,6]</span><br><span class="line">loader&#x3D;tqdm(loader, total&#x3D;len(loader),unit&#x3D;&quot;batches&quot;)</span><br><span class="line">for i, a in enumerate(loader):</span><br><span class="line">    print(a)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">100%|██████████| 6&#x2F;6 [00:00&lt;00:00, 5994.72batches&#x2F;s]</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="use-pytorch-to-make-one-hot"><a href="#use-pytorch-to-make-one-hot" class="headerlink" title="use pytorch to make one-hot"></a><font color=orange>use pytorch to make one-hot</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">vec_len&#x3D;10</span><br><span class="line">one_hot &#x3D; torch.FloatTensor(vec_len)</span><br><span class="line">print(one_hot)</span><br><span class="line">one_hot.zero_()</span><br><span class="line">print(one_hot)</span><br><span class="line">indices&#x3D;[1,2,3]</span><br><span class="line">indices &#x3D; torch.LongTensor(indices)</span><br><span class="line">one_hot.scatter_(0, indices, 1)</span><br><span class="line">print(one_hot)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">scatter(dim, index, src)</span><br><span class="line"></span><br><span class="line">dim：Along which dimension to index</span><br><span class="line">index：parameters to index </span><br><span class="line">src：the source parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result:</span><br><span class="line">tensor([9.1837e-39, 8.7245e-39, 8.9082e-39, 8.4490e-39, 1.0102e-38, 9.0919e-39,</span><br><span class="line">        1.0102e-38, 8.9082e-39, 8.4489e-39, 9.6429e-39])</span><br><span class="line">tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])</span><br><span class="line">tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a><font color=orange>Label Smoothing</font></h3><p>to avoid over-fiting for one-hot vector in label. I do not know why one-hot can make over-fiting.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">s&#x3D;[[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3]]</span><br><span class="line">one &#x3D; torch.tensor(s)</span><br><span class="line">print(one)</span><br><span class="line">print(one.size(0))</span><br><span class="line"></span><br><span class="line">actual &#x3D; ((1.0-self.label_smoothing)*actual) + (1.0&#x2F;actual.size(1))</span><br></pre></td></tr></table></figure><h3 id="torch-norm"><a href="#torch-norm" class="headerlink" title="torch.norm"></a><font color=orange>torch.norm</font></h3><p><a href="https://www.cnblogs.com/wanghui-garcia/p/11266298.html">refer</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.arange(9, dtype&#x3D;torch.float) - 4</span><br><span class="line"># a tensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])</span><br><span class="line">b &#x3D; a.reshape(3,3)</span><br><span class="line"># b&#x3D;tensor([[-4., -3., -2.],</span><br><span class="line">#         [-1.,  0.,  1.],</span><br><span class="line">#         [ 2.,  3.,  4.]])</span><br><span class="line">print(torch.norm(a,p&#x3D;2)) #tensor(7.7460),  p is the power exponent, when p&#x3D;2, (4*4)</span><br><span class="line"></span><br><span class="line">print(torch.norm(b)) #tensor(7.7460)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">torch.norm(input, p&#x3D;&#39;fro&#39;, dim&#x3D;None, keepdim&#x3D;False, out&#x3D;None, dtype&#x3D;None)</span><br><span class="line">if not set p, it&#39;s fro (default)</span><br><span class="line">7.7460 &#x3D; √（16*2 + 9*2 +4*2 + 1*2）</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="nn-Embedding-from-pretrained-torch-from-numpy-words-vector-wv-vectors"><a href="#nn-Embedding-from-pretrained-torch-from-numpy-words-vector-wv-vectors" class="headerlink" title="nn.Embedding.from_pretrained(torch.from_numpy(words_vector.wv.vectors))"></a><font color=orange>nn.Embedding.from_pretrained(torch.from_numpy(words_vector.wv.vectors))</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">import torch</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">nn.Embedding.from_pretrained(torch.from_numpy(words_vector.wv.vectors))</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">embedding &#x3D; nn.Embedding(6, 7)</span><br><span class="line">print(embedding.weight)</span><br><span class="line">input &#x3D; torch.LongTensor([[0,1],[3,5]])</span><br><span class="line">print(embedding(input))</span><br><span class="line">result:</span><br><span class="line">-----------------</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.5125,  0.3232, -0.2582, -0.3287, -0.6066,  0.3146,  0.7395],</span><br><span class="line">        [ 0.2703,  1.3907,  0.2815,  0.0629, -1.4006,  0.7667,  1.5393],</span><br><span class="line">        [-0.3242,  0.4823,  0.1822, -1.3310, -1.1742, -0.2926, -0.2241],</span><br><span class="line">        [ 0.5598,  0.7581,  0.2994, -0.2932,  1.2944,  1.6914,  0.3852],</span><br><span class="line">        [ 0.1087,  0.3037, -1.9757, -0.8317, -0.7715, -2.5496, -0.1095],</span><br><span class="line">        [ 0.5891,  0.1701,  1.3025, -0.6028,  0.7422, -1.7674, -0.4233]],</span><br><span class="line">       requires_grad&#x3D;True)</span><br><span class="line">tensor([[[-0.5125,  0.3232, -0.2582, -0.3287, -0.6066,  0.3146,  0.7395],</span><br><span class="line">         [ 0.2703,  1.3907,  0.2815,  0.0629, -1.4006,  0.7667,  1.5393]],</span><br><span class="line"></span><br><span class="line">        [[ 0.5598,  0.7581,  0.2994, -0.2932,  1.2944,  1.6914,  0.3852],</span><br><span class="line">         [ 0.5891,  0.1701,  1.3025, -0.6028,  0.7422, -1.7674, -0.4233]]],</span><br><span class="line">       grad_fn&#x3D;&lt;EmbeddingBackward&gt;)</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="torch–unsqueeze"><a href="#torch–unsqueeze" class="headerlink" title="torch–unsqueeze"></a><font color=orange>torch–unsqueeze</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">import torch</span><br><span class="line">a&#x3D;torch.arange(0,6)</span><br><span class="line">a&#x3D;a.view(2,3)</span><br><span class="line">print(a.shape)</span><br><span class="line">b&#x3D;a.unsqueeze(0)</span><br><span class="line">print(b)</span><br><span class="line">print(b.shape)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Chat with Dataloader in pytorch (NLP)</title>
      <link href="2020/06/06/Chat-with-Dataloader-in-pytorch-NLP/"/>
      <url>2020/06/06/Chat-with-Dataloader-in-pytorch-NLP/</url>
      
        <content type="html"><![CDATA[<p>This is a very interested model in pytorch to progress the data, in the next I will give an excmple to describe it’s flow:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.utils.data as Data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BATCH_SIZE &#x3D; 5</span><br><span class="line">x &#x3D; torch.linspace(1, 10, 10)  # x: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])</span><br><span class="line">y &#x3D; torch.linspace(10, 1, 10)  # y: tensor([10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.])</span><br><span class="line"></span><br><span class="line">torch_dataset &#x3D; Data.TensorDataset(x, y)</span><br><span class="line">print(torch_dataset[0])  # (tensor(1.), tensor(10.))</span><br><span class="line">print(torch_dataset[1])  # (tensor(2.), tensor(9.))</span><br><span class="line">print(torch_dataset[2]) #(tensor(3.), tensor(8.))</span><br><span class="line">print(torch_dataset[3]) # (tensor(4.), tensor(7.))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>by defining the fucntion Data.TensorDataset, and print the torch_dataset[0], index=0, find the result are (tensor(1.), tensor(10.), so why is it?  let us see the class TensorDataset</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class TensorDataset(Dataset):</span><br><span class="line">    r&quot;&quot;&quot;Dataset wrapping tensors.</span><br><span class="line"></span><br><span class="line">    Each sample will be retrieved by indexing tensors along the first dimension.</span><br><span class="line"></span><br><span class="line">    Arguments:</span><br><span class="line">        *tensors (Tensor): tensors that have the same size of the first dimension.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, *tensors):</span><br><span class="line">        assert all(tensors[0].size(0) &#x3D;&#x3D; tensor.size(0) for tensor in tensors)</span><br><span class="line">        self.tensors &#x3D; tensors</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        return tuple(tensor[index] for tensor in self.tensors)</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return self.tensors[0].size(0)</span><br></pre></td></tr></table></figure><p>the key part in this class is  <font color=red><strong>getitem</strong></font>,  in def <strong>getitem</strong>(self, index), by print the self.result, we can see:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.tensors (tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]), tensor([10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>let talk about the <strong>getitem</strong>, there is an excample: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class DataTest:</span><br><span class="line">    def __init__(self, id, address):</span><br><span class="line">        self.id &#x3D; id</span><br><span class="line">        self.address &#x3D; address</span><br><span class="line">        self.d &#x3D; &#123;self.id: 1,</span><br><span class="line">                  self.address: &quot;192.168.1.1&quot;</span><br><span class="line">                  &#125;</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, key):</span><br><span class="line">        print(key)</span><br><span class="line">        return &quot;hello&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data &#x3D; DataTest(1, &quot;192.168.2.11&quot;)</span><br><span class="line">print(data.__getitem__(2))</span><br><span class="line"># hello</span><br><span class="line">print(data[2])</span><br></pre></td></tr></table></figure><p>So, if <strong>getitem</strong> in your class,  <font color=red>the result data.<strong>getitem</strong>(2)) and data[2] are same. So, in torch_dataset = Data.TensorDataset(x, y), torch_dataset[0] is same as  torch_dataset.<strong>getitem</strong>(0)</font></p><p>Ok, and then we should put dataset into Dataloader:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">oader &#x3D; Data.DataLoader(</span><br><span class="line">    dataset&#x3D;torch_dataset,  - torch TensorDataset format</span><br><span class="line">    batch_size&#x3D;BATCH_SIZE,  - mini batch size</span><br><span class="line">    shuffle&#x3D;True,  </span><br><span class="line">    # num_workers&#x3D;2,             - subprocesses for loading data</span><br><span class="line">)</span><br><span class="line">for epoch in range(3):  # train entire dataset 3 times</span><br><span class="line">    for step, (batch_x, batch_y) in enumerate(loader):  # for each training step</span><br><span class="line">        # print(batch_x,batch_y)</span><br><span class="line">        # train your data...</span><br><span class="line">        print(&#39;Epoch: &#39;, epoch, &#39;| Step: &#39;, step, &#39;| batch x: &#39;,</span><br><span class="line">              batch_x.numpy(), &#39;| batch y: &#39;, batch_y.numpy())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Get the result:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Epoch:  0 | Step:  0 | batch x:  [3. 7. 2. 8. 5.] | batch y:  [8. 4. 9. 3. 6.]</span><br><span class="line">Epoch:  0 | Step:  1 | batch x:  [ 1.  6.  9. 10.  4.] | batch y:  [10.  5.  2.  1.  7.]</span><br><span class="line">Epoch:  1 | Step:  0 | batch x:  [8. 6. 4. 7. 5.] | batch y:  [3. 5. 7. 4. 6.]</span><br><span class="line">Epoch:  1 | Step:  1 | batch x:  [ 2. 10.  1.  9.  3.] | batch y:  [ 9.  1. 10.  2.  8.]</span><br><span class="line">Epoch:  2 | Step:  0 | batch x:  [7. 4. 2. 6. 3.] | batch y:  [4. 7. 9. 5. 8.]</span><br><span class="line">Epoch:  2 | Step:  1 | batch x:  [ 8.  1. 10.  5.  9.] | batch y:  [ 3. 10.  1.  6.  2.]</span><br></pre></td></tr></table></figure><p>Of cource, you can define your dataset class by your own.There is an excample from pytorch tutorial:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">lass myDataset(Dataset):</span><br><span class="line">    def __init__(self, csv_file, txt_file, root_dir, other_file):</span><br><span class="line">        self.csv_data &#x3D; pd.read_csv(csv_file)</span><br><span class="line">        with open(txt_file, &#39;r&#39;) as f:</span><br><span class="line">            data_list &#x3D; f.readlines()</span><br><span class="line">        self.txt_data &#x3D; data_list</span><br><span class="line">        self.root_dir &#x3D; root_dir</span><br><span class="line"> </span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.csv_data)</span><br><span class="line"> </span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        data &#x3D; (self.csv_data[idx], self.txt_data[idx])</span><br><span class="line">        return data</span><br><span class="line"> </span><br><span class="line">dataiter &#x3D; DataLoader(myDataset, batch_size&#x3D;32, shuffle&#x3D;True)</span><br></pre></td></tr></table></figure><p><font color=red>Ok, in the next I give other excample from the source code of paper “Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings”, I extract the relevant code from it,</font></p><p><font color=green>main.py</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">from dataloader import DatasetMetaQA</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line">import numpy as np</span><br><span class="line">def preprocess_entities_relations(entity_dict, relation_dict, entities, relations):</span><br><span class="line">    e &#x3D; &#123;&#125;</span><br><span class="line">    r &#x3D; &#123;&#125;</span><br><span class="line">    f &#x3D; open(entity_dict, &#39;r&#39;)</span><br><span class="line">    for line in f:</span><br><span class="line">        line &#x3D; line[:-1].split(&#39;\t&#39;)</span><br><span class="line">        ent_id &#x3D; int(line[0])</span><br><span class="line">        ent_name &#x3D; line[1]</span><br><span class="line">        e[ent_name] &#x3D; entities[ent_id]</span><br><span class="line">    f.close()</span><br><span class="line">    f &#x3D; open(relation_dict,&#39;r&#39;)</span><br><span class="line">    for line in f:</span><br><span class="line">        line &#x3D; line.strip().split(&#39;\t&#39;)</span><br><span class="line">        rel_id &#x3D; int(line[0])</span><br><span class="line">        rel_name &#x3D; line[1]</span><br><span class="line">        r[rel_name] &#x3D; relations[rel_id]</span><br><span class="line">    f.close()</span><br><span class="line">    return e,r</span><br><span class="line"></span><br><span class="line">def prepare_embeddings(embedding_dict):</span><br><span class="line">    entity2idx &#x3D; &#123;&#125;</span><br><span class="line">    idx2entity &#x3D; &#123;&#125;</span><br><span class="line">    i &#x3D; 0</span><br><span class="line">    embedding_matrix &#x3D; []</span><br><span class="line">    for key, entity_vector in embedding_dict.items():</span><br><span class="line">        entity2idx[key] &#x3D; i</span><br><span class="line">        idx2entity[i] &#x3D; key</span><br><span class="line">        i +&#x3D; 1</span><br><span class="line">        embedding_matrix.append(entity_vector)</span><br><span class="line">    return entity2idx, idx2entity, embedding_matrix</span><br><span class="line"></span><br><span class="line">def process_text_file(text_file, split&#x3D;False):</span><br><span class="line">    data_file &#x3D; open(text_file, &#39;r&#39;)</span><br><span class="line">    data_array &#x3D; []</span><br><span class="line">    for data_line in data_file.readlines():</span><br><span class="line">        data_line &#x3D; data_line.strip()</span><br><span class="line">        if data_line &#x3D;&#x3D; &#39;&#39;:</span><br><span class="line">            continue</span><br><span class="line">        data_line &#x3D; data_line.strip().split(&#39;\t&#39;)</span><br><span class="line">        # if no answer</span><br><span class="line">        if len(data_line) !&#x3D; 2:</span><br><span class="line">            continue</span><br><span class="line">        question &#x3D; data_line[0].split(&#39;[&#39;)</span><br><span class="line">        question_1 &#x3D; question[0]</span><br><span class="line">        question_2 &#x3D; question[1].split(&#39;]&#39;)</span><br><span class="line">        head &#x3D; question_2[0].strip()</span><br><span class="line">        question_2 &#x3D; question_2[1]</span><br><span class="line">        question &#x3D; question_1+&#39;NE&#39;+question_2</span><br><span class="line">        ans &#x3D; data_line[1].split(&#39;|&#39;)</span><br><span class="line">        data_array.append([head, question.strip(), ans])</span><br><span class="line">    if split&#x3D;&#x3D;False:</span><br><span class="line">        return data_array</span><br><span class="line">    else:</span><br><span class="line">        data &#x3D; []</span><br><span class="line">        for line in data_array:</span><br><span class="line">            head &#x3D; line[0]</span><br><span class="line">            question &#x3D; line[1]</span><br><span class="line">            tails &#x3D; line[2]</span><br><span class="line">            for tail in tails:</span><br><span class="line">                data.append([head, question, tail])</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    &quot;&quot;&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;reload the relevent path&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;&quot;&quot;</span><br><span class="line">    model_folder_name &#x3D; &quot;MetaQA&#x2F;ComplEx&quot;</span><br><span class="line">    entity_embedding_path &#x3D; model_folder_name + &#39;&#x2F;E.npy&#39;</span><br><span class="line">    relation_embedding_path &#x3D; model_folder_name + &#39;&#x2F;R.npy&#39;</span><br><span class="line">    entity_path &#x3D; entity_embedding_path</span><br><span class="line">    relation_path &#x3D; relation_embedding_path</span><br><span class="line">    entity_dict &#x3D; model_folder_name + &#39;&#x2F;entities.dict&#39;</span><br><span class="line">    relation_dict &#x3D;model_folder_name + &#39;&#x2F;relations.dict&#39;</span><br><span class="line">    hops &#x3D; &quot;1&quot; + &#39;hops&#39;</span><br><span class="line">    data_path &#x3D; &#39;MetaQA&#x2F;qa_train_&#39; + hops + &#39;.txt&#39;</span><br><span class="line">    batch_size&#x3D;64</span><br><span class="line">    num_workers&#x3D;15</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    entities &#x3D; np.load(entity_path)</span><br><span class="line">    relations &#x3D; np.load(relation_path)</span><br><span class="line">    print(&#39;------Loaded entities and relations----over--&#39;)</span><br><span class="line">    e,r &#x3D; preprocess_entities_relations(entity_dict, relation_dict, entities, relations) ## &#123;&#39;gorilla&#39;: array([-1.66980512e-02,2.75684539e-02, -2.20887363e-02, -1.55121172e-02&#125;</span><br><span class="line">    entity2idx, idx2entity, embedding_matrix &#x3D; prepare_embeddings(e)</span><br><span class="line">    data &#x3D; process_text_file(data_path, split&#x3D;False) #[&#39;ginger rogers&#39;, &#39;what movies are about NE&#39;, [&#39;Top Hat&#39;, &#39;Kitty Foyle&#39;, &#39;The Barkleys of Broadway&#39;]]  (topic entity, question, answer)</span><br><span class="line"></span><br><span class="line">    dataset &#x3D; DatasetMetaQA(data&#x3D;data, relations&#x3D;r, entities&#x3D;e, entity2idx&#x3D;entity2idx)</span><br><span class="line">    # print(dataset[0])</span><br><span class="line">    data_loader &#x3D; DataLoader(dataset, batch_size&#x3D;batch_size, shuffle&#x3D;True, num_workers&#x3D;num_workers) # num_workers: the number of thread</span><br><span class="line">    # print(dataset.__getitem__())</span><br><span class="line">    # for i_batch, a in enumerate(data_loader):</span><br><span class="line">    #     print(a[0])</span><br></pre></td></tr></table></figure><p><font color=green>dataload.py, it’s the class DatasetMetaQA</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import random</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line">from collections import defaultdict</span><br><span class="line">import os</span><br><span class="line">import unicodedata</span><br><span class="line">import re</span><br><span class="line">import time</span><br><span class="line">from collections import defaultdict</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">import numpy as np</span><br><span class="line">from transformers import *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DatasetMetaQA(Dataset):  # DatasetMetaQA inherit the method and property from dataset</span><br><span class="line">    def __init__(self, data, relations, entities, entity2idx):</span><br><span class="line">        self.data &#x3D; data</span><br><span class="line">        self.relations &#x3D; relations</span><br><span class="line">        self.entities &#x3D; entities</span><br><span class="line">        self.entity2idx &#x3D; entity2idx</span><br><span class="line">        self.pos_dict &#x3D; defaultdict(list)</span><br><span class="line">        self.neg_dict &#x3D; defaultdict(list)</span><br><span class="line">        self.index_array &#x3D; list(self.entities.keys())</span><br><span class="line">        self.tokenizer_class &#x3D; RobertaTokenizer</span><br><span class="line">        self.pretrained_weights &#x3D; &#39;pretrained_models&#x2F;roberta-base&#39;</span><br><span class="line">        self.tokenizer &#x3D; self.tokenizer_class.from_pretrained(self.pretrained_weights, cache_dir&#x3D;&#39;.&#39;)</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.data)</span><br><span class="line">    </span><br><span class="line">    def pad_sequence(self, arr, max_len&#x3D;128):</span><br><span class="line">        num_to_add &#x3D; max_len - len(arr)</span><br><span class="line">        for _ in range(num_to_add):</span><br><span class="line">            arr.append(&#39;&lt;pad&gt;&#39;)</span><br><span class="line">        return arr</span><br><span class="line"></span><br><span class="line">    def toOneHot(self, indices):</span><br><span class="line">        indices &#x3D; torch.LongTensor(indices) #tensor([35949, 18682, 31800])</span><br><span class="line">        batch_size &#x3D; len(indices)</span><br><span class="line"></span><br><span class="line">        vec_len &#x3D; len(self.entity2idx) #43234</span><br><span class="line">        one_hot &#x3D; torch.FloatTensor(vec_len)</span><br><span class="line">        one_hot.zero_()</span><br><span class="line">        one_hot.scatter_(0, indices, 1)</span><br><span class="line">        return one_hot</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        data_point &#x3D; self.data[index]</span><br><span class="line">        question_text &#x3D; data_point[1]</span><br><span class="line">        question_tokenized, attention_mask &#x3D; self.tokenize_question(question_text)</span><br><span class="line">        head_id &#x3D; self.entity2idx[data_point[0].strip()]</span><br><span class="line">        tail_ids &#x3D; []  # [23,4,5]</span><br><span class="line">        for tail_name in data_point[2]:</span><br><span class="line">            tail_name &#x3D; tail_name.strip()</span><br><span class="line">        #     #TODO: dunno if this is right way of doing things</span><br><span class="line">            if tail_name in self.entity2idx:</span><br><span class="line">                tail_ids.append(self.entity2idx[tail_name])</span><br><span class="line">        tail_onehot &#x3D; self.toOneHot(tail_ids)</span><br><span class="line">        return question_tokenized, attention_mask, head_id, tail_onehot</span><br><span class="line"></span><br><span class="line">    def tokenize_question(self, question):</span><br><span class="line">        question &#x3D; &quot;&lt;s&gt; &quot; + question + &quot; &lt;&#x2F;s&gt;&quot;</span><br><span class="line">        question_tokenized &#x3D; self.tokenizer.tokenize(question)</span><br><span class="line">        question_tokenized &#x3D; self.pad_sequence(question_tokenized, 64)</span><br><span class="line">        question_tokenized &#x3D; torch.tensor(self.tokenizer.encode(question_tokenized, add_special_tokens&#x3D;False))</span><br><span class="line">        attention_mask &#x3D; []</span><br><span class="line">        for q in question_tokenized:</span><br><span class="line">            # 1 means padding token</span><br><span class="line">            if q &#x3D;&#x3D; 1:</span><br><span class="line">                attention_mask.append(0)</span><br><span class="line">            else:</span><br><span class="line">                attention_mask.append(1)</span><br><span class="line">        return question_tokenized, torch.tensor(attention_mask, dtype&#x3D;torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DataLoaderMetaQA(DataLoader):</span><br><span class="line"></span><br><span class="line">    def __init__(self, *args, **kwargs):</span><br><span class="line">        print(&quot;init--------dataloadermetaQA&quot;)</span><br><span class="line">        super(DataLoaderMetaQA, self).__init__(*args, **kwargs)</span><br><span class="line">        # self.collate_fn &#x3D; _collate_fn</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Code model in Saturday 20200606</title>
      <link href="2020/06/06/Code-model-in-Saturday-20200606/"/>
      <url>2020/06/06/Code-model-in-Saturday-20200606/</url>
      
        <content type="html"><![CDATA[<h3 id="I-want-to-find-the-vector-of-the-entity"><a href="#I-want-to-find-the-vector-of-the-entity" class="headerlink" title="I want to find the vector of the entity"></a><font color=orange>I want to find the vector of the entity</font></h3><p>I have a entity dict, a relation dict, entity matrix, relation matrix, I want to make a dict{key:value}, key is a entity or relation, value is its vector, like this: {‘gorilla’: array([-1.66980512e-02,2.75684539e-02, -2.20887363e-02, -1.55121172e-02}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def preprocess_entities_relations(entity_dict, relation_dict, entities, relations):</span><br><span class="line">    e &#x3D; &#123;&#125;</span><br><span class="line">    r &#x3D; &#123;&#125;</span><br><span class="line">    f &#x3D; open(entity_dict, &#39;r&#39;)</span><br><span class="line">    for line in f:</span><br><span class="line">        line &#x3D; line[:-1].split(&#39;\t&#39;)</span><br><span class="line">        ent_id &#x3D; int(line[0])</span><br><span class="line">        ent_name &#x3D; line[1]</span><br><span class="line">        e[ent_name] &#x3D; entities[ent_id]</span><br><span class="line">    f.close()</span><br><span class="line">    f &#x3D; open(relation_dict,&#39;r&#39;)</span><br><span class="line">    for line in f:</span><br><span class="line">        line &#x3D; line.strip().split(&#39;\t&#39;)</span><br><span class="line">        rel_id &#x3D; int(line[0])</span><br><span class="line">        rel_name &#x3D; line[1]</span><br><span class="line">        r[rel_name] &#x3D; relations[rel_id]</span><br><span class="line">    f.close()</span><br><span class="line">    return e,r</span><br></pre></td></tr></table></figure><h3 id="MetaQA-progress"><a href="#MetaQA-progress" class="headerlink" title="MetaQA progress"></a><font color=orange>MetaQA progress</font></h3><p>A: what movies did [Temuera Morrison] act in    Once Were Warriors|Tracker|River Queen</p><p>————-&gt;</p><p>B:[‘Temuera Morrison’, ‘what movies did NE act in’, [‘Once Were Warriors’, ‘Tracker’, ‘River Queen’]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def process_text_file(text_file, split&#x3D;False):</span><br><span class="line">    data_file &#x3D; open(text_file, &#39;r&#39;)</span><br><span class="line">    data_array &#x3D; []</span><br><span class="line">    for data_line in data_file.readlines():</span><br><span class="line">        data_line &#x3D; data_line.strip()</span><br><span class="line">        if data_line &#x3D;&#x3D; &#39;&#39;:</span><br><span class="line">            continue</span><br><span class="line">        data_line &#x3D; data_line.strip().split(&#39;\t&#39;)</span><br><span class="line">        # if no answer</span><br><span class="line">        if len(data_line) !&#x3D; 2:</span><br><span class="line">            continue</span><br><span class="line">        question &#x3D; data_line[0].split(&#39;[&#39;)</span><br><span class="line">        question_1 &#x3D; question[0]</span><br><span class="line">        question_2 &#x3D; question[1].split(&#39;]&#39;)</span><br><span class="line">        entity_mention &#x3D; question_2[0].strip()</span><br><span class="line"></span><br><span class="line">        question_2 &#x3D; question_2[1]</span><br><span class="line">        question &#x3D; question_1+&#39;NE&#39;+question_2</span><br><span class="line">        ans &#x3D; data_line[1].split(&#39;|&#39;)</span><br><span class="line">        data_array.append([entity_mention, question.strip(), ans])</span><br><span class="line">    if split&#x3D;&#x3D;False:</span><br><span class="line">        return data_array</span><br><span class="line">    else:</span><br><span class="line">        data &#x3D; []</span><br><span class="line">        for line in data_array:</span><br><span class="line">            head &#x3D; line[0]</span><br><span class="line">            question &#x3D; line[1]</span><br><span class="line">            tails &#x3D; line[2]</span><br><span class="line">            for tail in tails:</span><br><span class="line">                data.append([head, question, tail])</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">s&#x3D;process_text_file(&quot;demo.txt&quot;)</span><br><span class="line">print(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="MetaQA-get-word-dic"><a href="#MetaQA-get-word-dic" class="headerlink" title="MetaQA get word dic"></a><font color=orange>MetaQA get word dic</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def get_vocab(data):</span><br><span class="line">    word_to_ix &#x3D; &#123;&#125;</span><br><span class="line">    maxLength &#x3D; 0</span><br><span class="line">    idx2word &#x3D; &#123;&#125;</span><br><span class="line">    for d in data:</span><br><span class="line">            sent &#x3D; d[1]</span><br><span class="line">            for word in sent.split():</span><br><span class="line">                if word not in word_to_ix:</span><br><span class="line">                    idx2word[len(word_to_ix)] &#x3D; word</span><br><span class="line">                    word_to_ix[word] &#x3D; len(word_to_ix)</span><br><span class="line">                    </span><br><span class="line">            length &#x3D; len(sent.split())</span><br><span class="line">            if length &gt; maxLength:</span><br><span class="line">                maxLength &#x3D; length</span><br><span class="line"></span><br><span class="line">    return word_to_ix, idx2word, maxLength</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today code model 20200605</title>
      <link href="2020/06/05/Today-code-model-20200605/"/>
      <url>2020/06/05/Today-code-model-20200605/</url>
      
        <content type="html"><![CDATA[<h3 id="ModuleNotFoundError-No-module-named-‘tqdm-auto’"><a href="#ModuleNotFoundError-No-module-named-‘tqdm-auto’" class="headerlink" title="ModuleNotFoundError: No module named ‘tqdm.auto’"></a><font color=orange>ModuleNotFoundError: No module named ‘tqdm.auto’</font></h3><p>It’s the vrsion issue, you just “pip install –upgrade tqdm”.</p><h3 id="torch-sort"><a href="#torch-sort" class="headerlink" title="torch.sort"></a><font color=orange>torch.sort</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">s&#x3D;[[1,2,3,4],[5,6,78,8]]</span><br><span class="line">s&#x3D;torch.LongTensor(s)</span><br><span class="line">d&#x3D;torch.sort(s, dim&#x3D;1, descending&#x3D;True)</span><br><span class="line">print(s)</span><br><span class="line">print(d)</span><br><span class="line">tensor([[ 1,  2,  3,  4],</span><br><span class="line">        [ 5,  6, 78,  8]])</span><br><span class="line">torch.return_types.sort(</span><br><span class="line">values&#x3D;tensor([[ 4,  3,  2,  1],</span><br><span class="line">        [78,  8,  6,  5]]),</span><br><span class="line">indices&#x3D;tensor([[3, 2, 1, 0],</span><br><span class="line">        [2, 3, 1, 0]]))</span><br></pre></td></tr></table></figure><h3 id="np-where"><a href="#np-where" class="headerlink" title="np.where()"></a><font color=orange>np.where()</font></h3><p>when np.where just has one argument.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">s&#x3D;np.where(5&#x3D;&#x3D;1)</span><br><span class="line">print(s)</span><br><span class="line">a&#x3D;np.where(5&#x3D;&#x3D;5)</span><br><span class="line">print(a)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">s: (array([], dtype&#x3D;int64),)</span><br><span class="line">a: (array([0], dtype&#x3D;int64),)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">a1&#x3D;np.where(5&#x3D;&#x3D;5)[0][0]&#x3D;0</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">s&#x3D;np.array([1,2,3,6,7,8])</span><br><span class="line">s&#x3D;np.where(s&#x3D;&#x3D;6)</span><br><span class="line">print(s)</span><br><span class="line"></span><br><span class="line">return the number location in a array</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="evaluate-in-EmbedKGQA-Embedding"><a href="#evaluate-in-EmbedKGQA-Embedding" class="headerlink" title="evaluate in EmbedKGQA-Embedding"></a><font color=orange>evaluate in EmbedKGQA-Embedding</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">def evaluate(self, model, data):</span><br><span class="line">    model.eval()</span><br><span class="line">    hits &#x3D; []</span><br><span class="line">    ranks &#x3D; []</span><br><span class="line">    for i in range(10):</span><br><span class="line">        hits.append([])</span><br><span class="line"></span><br><span class="line">    test_data_idxs &#x3D; self.get_data_idxs(data)  # (9432, 20, 8804), (12249, 159, 1716), (10191, 101, 2806),</span><br><span class="line">    er_vocab &#x3D; self.get_er_vocab(self.get_data_idxs(d.data))  #(1780, 137): [7722, 7716, 13499, 10708]</span><br><span class="line"></span><br><span class="line">    print(&quot;Number of data points: %d&quot; % len(test_data_idxs))</span><br><span class="line">    for i in tqdm(range(0, len(test_data_idxs), self.batch_size)):</span><br><span class="line">        data_batch, _ &#x3D; self.get_batch(er_vocab, test_data_idxs, i)</span><br><span class="line">        e1_idx &#x3D; torch.LongTensor(data_batch[:,0])</span><br><span class="line">        r_idx &#x3D; torch.LongTensor(data_batch[:,1])</span><br><span class="line">        e2_idx &#x3D; torch.tensor(data_batch[:,2])</span><br><span class="line"></span><br><span class="line">        if self.cuda:</span><br><span class="line">            e1_idx &#x3D; e1_idx.cuda()</span><br><span class="line">            r_idx &#x3D; r_idx.cuda()</span><br><span class="line">            e2_idx &#x3D; e2_idx.cuda()</span><br><span class="line">        predictions &#x3D; model.forward(e1_idx, r_idx)   # 调用训练好的model</span><br><span class="line">        # following lines commented means RAW evaluation (not filtered)</span><br><span class="line">        for j in range(data_batch.shape[0]):</span><br><span class="line">            filt &#x3D; er_vocab[(data_batch[j][0], data_batch[j][1])]  # tail answer #(1780, 137): tail:[7722, 7716, 13499, 10708]</span><br><span class="line">            target_value &#x3D; predictions[j,e2_idx[j]].item() # e2_idx[j] is a number ,such as 56, predictions[0,4]&#x3D;tensor(0.3505, grad_fn&#x3D;&lt;SelectBackward&gt;), predictions[0,4].items()&#x3D;0.3505</span><br><span class="line">            predictions[j, filt] &#x3D; 0.0</span><br><span class="line">            predictions[j, e2_idx[j]] &#x3D; target_value</span><br><span class="line">    #</span><br><span class="line">        sort_values, sort_idxs &#x3D; torch.sort(predictions, dim&#x3D;1, descending&#x3D;True)</span><br><span class="line"></span><br><span class="line">        sort_idxs &#x3D; sort_idxs.cpu().numpy()</span><br><span class="line"></span><br><span class="line">        for j in range(data_batch.shape[0]):</span><br><span class="line">            # sort_idxs[j]: [10416  9084  4488 ...  5365 11847  9656]</span><br><span class="line">            rank &#x3D; np.where(sort_idxs[j]&#x3D;&#x3D;e2_idx[j].item())[0][0]  #  预测  相等就是0， 不等就是空</span><br><span class="line">            ranks.append(rank+1)</span><br><span class="line"></span><br><span class="line">            for hits_level in range(10):</span><br><span class="line">                if rank &lt;&#x3D; hits_level:</span><br><span class="line">                    hits[hits_level].append(1.0)</span><br><span class="line">                else:</span><br><span class="line">                    hits[hits_level].append(0.0)</span><br><span class="line">    hitat10 &#x3D; np.mean(hits[9])</span><br><span class="line">    hitat3 &#x3D; np.mean(hits[2])</span><br><span class="line">    hitat1 &#x3D; np.mean(hits[0])</span><br><span class="line">    meanrank &#x3D; np.mean(ranks)</span><br><span class="line">    mrr &#x3D; np.mean(1.&#x2F;np.array(ranks))</span><br><span class="line">    print(&#39;Hits @10: &#123;0&#125;&#39;.format(hitat10))</span><br><span class="line">    print(&#39;Hits @3: &#123;0&#125;&#39;.format(hitat3))</span><br><span class="line">    print(&#39;Hits @1: &#123;0&#125;&#39;.format(hitat1))</span><br><span class="line">    print(&#39;Mean rank: &#123;0&#125;&#39;.format(meanrank))</span><br><span class="line">    print(&#39;Mean reciprocal rank: &#123;0&#125;&#39;.format(mrr))</span><br><span class="line">    return [mrr, meanrank, hitat10, hitat3, hitat1]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The summary of the classical model for multi-KBQA</title>
      <link href="2020/06/04/The-summary-of-the-classical-model-for-multi-KBQA/"/>
      <url>2020/06/04/The-summary-of-the-classical-model-for-multi-KBQA/</url>
      
        <content type="html"><![CDATA[<p>In this blog I will give some classical models for multi-KBQA， and the performance in MetaQA and WebQSP, the detail about the data, you can find it in <a href="https://iamlimingchen.com/2020/05/04/Dataset-for-multi-hops-KBQA/">Dataset for multi-hops KBQA</a></p><ul><li><font color=orange>1: VRN</font>: Variational Reasoning for Question Answering with Knowledge Graph, 2018. By Georgia Institute of Technology . <a href="https://arxiv.org/abs/1709.04071">link</a></li><li><font color=orange>2: GraftNet</font>: Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text, 2018. By Carnegie Mellon University. <a href="https://arxiv.org/abs/1809.00782">link</a></li><li><font color=orange>3: PullNet</font>: PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text, 2019. By Google Research.<a href="https://arxiv.org/abs/1904.09537?context=cs.CL">link</a> </li><li><font color=orange>4: KV-Mem</font>: Key-Value Memory Networks for Directly Reading Documents, 2016. By Facebook AI Research. <a href="https://arxiv.org/abs/1606.03126v1">link</a></li><li><font color=orange>5: EmbedGQA</font>: Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings, 2020. By Indian Institute of Science. </li><li><font color=orange>6: ISM</font>. Multi-hop Knowledge Base Question Answering with an Iterative Sequence Matching Model, 2019. By Singapore Management University. <a href="https://ieeexplore.ieee.org/document/8970943">link</a></li><li><font color=orange>7: IRN</font>.An Interpretable Reasoning Network for Multi-Relation Question Answering, 2018. By Tsinghua University.<a href="https://arxiv.org/abs/1801.04726v1">link</a></li><li><font color=orange>8: RSF</font>. Differentiable Representations For Multihop Inference Rules, 2019. By Google Research. <a href="https://arxiv.org/abs/1905.10417?context=cs.LG">link</a></li></ul><table><thead><tr><th>method</th><th>1-hop</th><th>2-hop</th><th>3-hop</th><th>WebbQSP</th></tr></thead><tbody><tr><td>VRN</td><td>97.5</td><td>89.9</td><td>62.5</td><td>null</td></tr><tr><td>GraftNet</td><td>97</td><td>94.8</td><td>77.7</td><td>66.4</td></tr><tr><td>PullNet</td><td>97</td><td>99.9</td><td>91.4</td><td>68.1</td></tr><tr><td>KV-Mem</td><td>96.2</td><td>82.7</td><td>48.9</td><td>46.7</td></tr><tr><td>EmbedGQA</td><td>97.5</td><td>98.8</td><td>94.8</td><td>66.6</td></tr><tr><td>ISM</td><td>96.3</td><td>99.1</td><td>99.6</td><td>null</td></tr><tr><td>IRN</td><td>9</td><td>8.3</td><td>31.2</td><td>null</td></tr><tr><td>RSF</td><td>96.2</td><td>81.1</td><td>72.3</td><td>52.7</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today codel model 20200604</title>
      <link href="2020/06/04/Today-codel-model-20200604/"/>
      <url>2020/06/04/Today-codel-model-20200604/</url>
      
        <content type="html"><![CDATA[<h3 id="np-array-w"><a href="#np-array-w" class="headerlink" title="np.array(w)"></a><font color=orange>np.array(w)</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">w&#x3D;[(12303, 178), (3450, 382), (5896, 382)]</span><br><span class="line"></span><br><span class="line">s&#x3D;(np.array(w))</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">[[12303   178]</span><br><span class="line"> [ 3450   382]</span><br><span class="line"> [ 5896   382]]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">print(s)</span><br><span class="line">print(s[:,0]) # [12303  3450  5896]</span><br></pre></td></tr></table></figure><h3 id="torch-big-of-word"><a href="#torch-big-of-word" class="headerlink" title="torch big_of_word"></a><font color=orange>torch big_of_word</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">s&#x3D;torch.zeros([5,10], dtype&#x3D;torch.float32)</span><br><span class="line">print(s)</span><br><span class="line">for i in range(3):</span><br><span class="line">    s[i,[2,3,5]]&#x3D;1</span><br><span class="line">print(s)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])</span><br><span class="line">tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1., 1., 0., 1., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1., 1., 0., 1., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><h3 id="optimizer-zero-grad"><a href="#optimizer-zero-grad" class="headerlink" title="optimizer.zero_grad()"></a><font color=orange>optimizer.zero_grad()</font></h3><p>make the grad is zero at the beginning</p><h3 id="Error-RuntimeError-Expected-tensor-for-argument"><a href="#Error-RuntimeError-Expected-tensor-for-argument" class="headerlink" title=" Error: RuntimeError: Expected tensor for argument "></a><font color=orange> Error: RuntimeError: Expected tensor for argument </font></h3><p>There is an error: RuntimeError: Expected tensor for argument #1 ‘indices’ to have scalar type Long; but got torch.IntTensor instead (while checking arguments for embedding)<br>You just:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_ids &#x3D; torch.tensor(…)</span><br><span class="line">to--------&gt;</span><br><span class="line">input_ids &#x3D; torch.LongTensor()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today code model-20200603</title>
      <link href="2020/06/03/Today-code-model-20200603/"/>
      <url>2020/06/03/Today-code-model-20200603/</url>
      
        <content type="html"><![CDATA[<h3 id="use-os-in-python-to-get-the-file-path"><a href="#use-os-in-python-to-get-the-file-path" class="headerlink" title="use os in python to get the file path"></a><font color=orange>use os in python to get the file path</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"> </span><br><span class="line">--current directory--</span><br><span class="line">print (os.getcwd())</span><br><span class="line">print (os.path.abspath(os.path.dirname(__file__)))</span><br><span class="line"> </span><br><span class="line">-- up directory</span><br><span class="line">print (os.path.abspath(os.path.dirname(os.path.dirname(__file__))))</span><br><span class="line">print (os.path.abspath(os.path.dirname(os.getcwd())))</span><br><span class="line">print (os.path.abspath(os.path.join(os.getcwd(), &quot;..&quot;)))</span><br><span class="line"> </span><br><span class="line">--up up directory</span><br><span class="line">print (os.path.abspath(os.path.join(os.getcwd(), &quot;..&#x2F;..&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="np-random-seed-20"><a href="#np-random-seed-20" class="headerlink" title="np.random.seed(20)"></a><font color=orange>np.random.seed(20)</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">if we  set the same seed, the random value is same, if we do not set the same seed or do not set seed. the random value</span><br><span class="line">is different</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">np.random.seed(20)</span><br><span class="line">L1 &#x3D; np.random.randn(3, 3)</span><br><span class="line">np.random.seed(20)</span><br><span class="line">L2 &#x3D; np.random.randn(3, 3)</span><br><span class="line">print(L1)</span><br><span class="line">print(L2)</span><br></pre></td></tr></table></figure><h3 id="torch-cuda-manual-seed-all-seed"><a href="#torch-cuda-manual-seed-all-seed" class="headerlink" title="torch.cuda.manual_seed_all(seed)"></a><font color=orange>torch.cuda.manual_seed_all(seed)</font></h3><p>At the begining of the training, Parameter initialization is random, we need let the Parameter initialization is same at each training. So we should set the random seed. Maybe you can refer<a href="https://blog.csdn.net/zlrai5895/article/details/84947307">CSDN blog</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(args.seed)#set seed for CPU</span><br><span class="line">if cuda:</span><br><span class="line">    torch.cuda.manual_seed(seed)# set GPU for current GPU</span><br><span class="line">    torch.cuda.manual_seed_all(seed)#set all GPU for current GPU</span><br></pre></td></tr></table></figure><h3 id="Kg-entities-and-relation"><a href="#Kg-entities-and-relation" class="headerlink" title="Kg-entities and relation"></a><font color=orange>Kg-entities and relation</font></h3><p>question: If there is a KG file (triple), such as FB15k-237, MovieQA. I want to make a entity set and relation set, so what should I do?</p><p>You can do it by this way</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class Data:</span><br><span class="line"></span><br><span class="line">    def __init__(self, data_dir&#x3D;&quot;data&#x2F;FB15k-237&#x2F;&quot;, reverse&#x3D;False):</span><br><span class="line">        self.train_data &#x3D; self.load_data(data_dir, &quot;train&quot;, reverse&#x3D;reverse)</span><br><span class="line">        self.valid_data &#x3D; self.load_data(data_dir, &quot;valid&quot;, reverse&#x3D;reverse)</span><br><span class="line">        self.test_data &#x3D; self.load_data(data_dir, &quot;test&quot;, reverse&#x3D;reverse)</span><br><span class="line">        self.data &#x3D; self.train_data + self.valid_data + self.test_data</span><br><span class="line">        self.entities &#x3D; self.get_entities(self.data)</span><br><span class="line">        self.train_relations &#x3D; self.get_relations(self.train_data)</span><br><span class="line">        self.valid_relations &#x3D; self.get_relations(self.valid_data)</span><br><span class="line">        self.test_relations &#x3D; self.get_relations(self.test_data)</span><br><span class="line">        self.relations &#x3D; self.train_relations + [i for i in self.valid_relations \</span><br><span class="line">                if i not in self.train_relations] + [i for i in self.test_relations \</span><br><span class="line">                if i not in self.train_relations]</span><br><span class="line"></span><br><span class="line">    def load_data(self, data_dir, data_type&#x3D;&quot;train&quot;, reverse&#x3D;False):</span><br><span class="line">        with open(&quot;%s\\%s.txt&quot; % (data_dir, data_type), &quot;r&quot;) as f:</span><br><span class="line">            data &#x3D; f.read().strip().split(&quot;\n&quot;)</span><br><span class="line">            data &#x3D; [i.split(&#39;\t&#39;) for i in data]</span><br><span class="line">            # print(data)</span><br><span class="line">            if reverse:</span><br><span class="line">                data +&#x3D; [[i[2], i[1]+&quot;_reverse&quot;, i[0]] for i in data]</span><br><span class="line">            # print(data)</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">    def get_relations(self, data):</span><br><span class="line">        relations &#x3D; sorted(list(set([d[1] for d in data])))</span><br><span class="line">        return relations</span><br><span class="line"></span><br><span class="line">    def get_entities(self, data):</span><br><span class="line">        entities &#x3D; sorted(list(set([d[0] for d in data]+[d[2] for d in data])))</span><br><span class="line">        return entities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> d &#x3D; Data(data_dir&#x3D;data_dir, reverse&#x3D;True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="torch-set-num-threads"><a href="#torch-set-num-threads" class="headerlink" title="torch.set_num_threads"></a><font color=orange>torch.set_num_threads</font></h3><p>When we use the pytorch, the liunx and windows will start many progresses (进程) to handle our training data, so this progress will comsume big memory  and CPU, it will influence other work, so should set the number of progress. so you can need </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">torch.set_num_threads(N)</span><br><span class="line">M&#x3D;NUll  cpu51%，time 15ms</span><br><span class="line">M&#x3D;2  cpu17%，time 15ms--&gt;25ms</span><br><span class="line">M&#x3D;4  cpu34%，time 17ms</span><br><span class="line">M&#x3D;8  cpu67%，</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    torch.set_num_threads(4)</span><br><span class="line">    arr&#x3D; np.zeros((100,4, 400, 400), dtype&#x3D;np.float32)</span><br><span class="line">    aaa&#x3D;torch.from_numpy(arr)</span><br><span class="line">    start &#x3D; time.time()</span><br><span class="line">    aaa.add_(-0.406)</span><br><span class="line">    print(&#39;time2&#39;,time.time() - start)</span><br><span class="line">    time.sleep(0.007)</span><br></pre></td></tr></table></figure><h3 id="args-and-kwargs"><a href="#args-and-kwargs" class="headerlink" title="*args and **kwargs "></a><font color=orange>*args and **kwargs </font></h3><p>*args is a tuple, **kwargs is a dict.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def f(d1, d2, d3):</span><br><span class="line">    print(&quot;d1: &quot;, d1)</span><br><span class="line">    print(&quot;d2: &quot;, d2)</span><br><span class="line">    print(&quot;d3: &quot;, d3)</span><br><span class="line"></span><br><span class="line">args &#x3D; (&quot;one&quot;, 2, 3)</span><br><span class="line">f(*args)</span><br><span class="line">print(&quot;----------------&quot;)</span><br><span class="line">kwargs &#x3D; &#123;&quot;d3&quot;: &quot;one&quot;, &quot;d2&quot;: 2, &quot;d1&quot;: 3&#125;</span><br><span class="line">f(**kwargs)</span><br></pre></td></tr></table></figure><h3 id="super-init"><a href="#super-init" class="headerlink" title="super().init() "></a><font color=orange>super().<strong>init</strong>() </font></h3><pre><code>class Root(object):    def __init__(self):        self.x = &#39;I am a robot&#39;    def fun(self):        print(&#39;I am fun&#39;)class A(Root):    def __init__(self):        print(&#39;I am a child of Root&#39;)test = A()test.fun()test.x  we can meet this error:  test.x  #AttributeError: &#39;A&#39; object has no attribute &#39;x&#39;we can see that child A can inherit the method (fun()) of its father Root, but it can not inherit the property &quot;self.x = &#39;I am a robot&#39;&quot; so we should use super().__init__()  in this code:class Root(object):    def __init__(self):        self.x = &#39;I am a robot&#39; # this is property    def fun(self):        print(self.x)        print(&#39;I am fun&#39;) # this is methodclass A(Root):    def __init__(self):        super(A, self).__init__()        print(&#39;I am a child of Root&#39;)test = A()test.fun()test.x      ~result:~ I am a child of Root~ I am a robot~ I am fun</code></pre><h3 id="E-torch-nn-Embedding-len-d-entities-d1-multiplier-padding-idx-0"><a href="#E-torch-nn-Embedding-len-d-entities-d1-multiplier-padding-idx-0" class="headerlink" title="E=torch.nn.Embedding(len(d.entities), d1 * multiplier, padding_idx=0)"></a><font color=orange>E=torch.nn.Embedding(len(d.entities), d1 * multiplier, padding_idx=0)</font></h3><p>It produce a random matrix, it follows Normal distribution<br>E.weight=E.weght.data, it return a true value for the word embedding.</p><pre><code>E: Embedding(14541, 400, padding_idx=0)E.weight tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],        [-1.6463, -1.0449, -0.4470,  ..., -0.8321,  0.7408, -1.2943],        [-0.3093, -1.5920, -0.5301,  ...,  0.3698, -0.7990,  0.9958],        ...,</code></pre><h3 id="initialization-function-xavier-normal-in-pytorch"><a href="#initialization-function-xavier-normal-in-pytorch" class="headerlink" title="initialization function ( xavier_normal_()) in pytorch"></a><font color=orange>initialization function ( xavier_normal_()) in pytorch</font></h3><p>you can refer this <a href="https://www.cnblogs.com/jfdwd/p/11269622.html">xavier_normal</a></p><pre><code>torch.nn.init.xavier_normal_(tensor, gain=1)xavier follows Normal distribution.mean=0,std = gain * sqrt(2/fan_in + fan_out)</code></pre><h3 id="learning-rate-adjustment-strategy"><a href="#learning-rate-adjustment-strategy" class="headerlink" title="learning rate adjustment strategy "></a><font color=orange>learning rate adjustment strategy </font></h3><p>you can refer this <a href="https://blog.csdn.net/shanglianlm/article/details/85143614">six strategy</a></p><pre><code>torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)To use exponential damping to adjust the learning rate ,lr=lr∗gamma∗∗epoch</code></pre><h3 id="Pose-question"><a href="#Pose-question" class="headerlink" title="Pose question"></a><font color=orange>Pose question</font></h3><p>if we hace (1,2,4),(1,2,5),(1,2,7)how can I get {(4,5,7):(1,2)}<br>you can </p><pre><code>data=[(1,2,4),(1,2,5),(1,2,7)] def get_er_vocab(data):        er_vocab = defaultdict(list)        for triple in data:            er_vocab[(triple[0], triple[1])].append(triple[2])        return er_vocab</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today C++----2</title>
      <link href="2020/06/02/Today-C/"/>
      <url>2020/06/02/Today-C/</url>
      
        <content type="html"><![CDATA[<h3 id="lsb-release-command-not-found"><a href="#lsb-release-command-not-found" class="headerlink" title="lsb_release: command not found"></a><font color=orange>lsb_release: command not found</font></h3><p>lsb_release is a tool to view the server version<br>just: lsb_release -a<br>To solve the above problem you need “yum install redhat-lsb -y”</p><h3 id="g-command-not-found-in-linux"><a href="#g-command-not-found-in-linux" class="headerlink" title="g++: command not found in linux"></a><font color=orange>g++: command not found in linux</font></h3><p>you should “yum -y install gcc+”<br>when you input  this command “”yum -y install gcc+””<br>error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ip-172-29-0-155 node-v10.15.3]# yum -y install gcc+</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: mirrors.aliyun.com</span><br><span class="line"> * extras: mirrors.aliyun.com</span><br><span class="line"> * updates: mirrors.aliyun.com</span><br><span class="line">No package gcc+ available.</span><br></pre></td></tr></table></figure><p>you should:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum search &quot;gcc-c++&quot;</span><br></pre></td></tr></table></figure><p>you can find: gcc-c++.x86_64 : C++ support for GCC<br>So: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++.x86_64</span><br></pre></td></tr></table></figure><h3 id="run-a-cpp-file-in-linux"><a href="#run-a-cpp-file-in-linux" class="headerlink" title="run a .cpp file in linux"></a><font color=orange>run a .cpp file in linux</font></h3><p>there is a abc.cpp</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;&quot;hello world&quot;&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>you should “g++ -o abc abc.cpp” and “./abc”</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Today C++ -----1</title>
      <link href="2020/05/29/Today-C-1/"/>
      <url>2020/05/29/Today-C-1/</url>
      
        <content type="html"><![CDATA[<p>When I want to repro some work, I find some authors like use C++ to run their code. For learning their work in a better way, I decided to review  my C++ from little by little. My aim is to change their work into python script or some DL frameworks. So, let’s do it!!!!</p><h3 id="AddArray"><a href="#AddArray" class="headerlink" title="AddArray"></a><font color=orange>AddArray</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt; &#x2F;&#x2F;ostream</span><br><span class="line">using namespace std; &#x2F;&#x2F; C++标准库所使用的所有标识符（即类，函数，对象等名称） 都是在同一个特殊的名字空间（std） 中定义的</span><br><span class="line">int addArray (int array[], int n); </span><br><span class="line"></span><br><span class="line">int main ()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; sum</span><br><span class="line">int data[]&#x3D;&#123;0,1,2,3,4,5&#125;;</span><br><span class="line">int size&#x3D;sizeof(data)&#x2F;sizeof(data[0]); &#x2F;&#x2F; sizeof(data)   4*6  &#x3D;24 是说一个int型占了4个字节的内存</span><br><span class="line">cout &lt;&lt; &quot;the result is:&quot; &lt;&lt; addArray(data,size) &lt;&lt; endl; &#x2F;&#x2F; 是basic_osteam 类的对象</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;getchar(); &#x2F;&#x2F;(解决cmd闪退) 让系统等待字符的输入，来实现程序的暂停</span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int addArray(int array[], int n)  &#x2F;&#x2F; 把array当成指针， 数组第一个元素的地址  , 按着指针往下走就ok了</span><br><span class="line">&#x2F;&#x2F;int addArray(int *array, int n)</span><br><span class="line">&#123;</span><br><span class="line">int sum&#x3D;0;</span><br><span class="line">int i;</span><br><span class="line"></span><br><span class="line">for (i&#x3D;0;i&lt;n;i++)</span><br><span class="line">&#123;</span><br><span class="line">    sum+&#x3D;array[i];  &#x2F;&#x2F;sum+&#x3D;*array++;</span><br><span class="line">&#125;</span><br><span class="line">return sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="AddArray2"><a href="#AddArray2" class="headerlink" title="AddArray2 "></a><font color=orange>AddArray2 </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt; </span><br><span class="line">using namespace std;</span><br><span class="line">&#x2F;&#x2F;编写一个程序，要求用户输入一串数字和任意数目的空格，这些整数必须位于同一行中，</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">int sum&#x3D;0;</span><br><span class="line">int i;</span><br><span class="line">while (cin&gt;&gt;i) &#x2F;&#x2F; cin流对象， 知道如何从用户终端读取数据. 如果 到达了文件尾或者提取操作符遇到一个非法值，这个返回值为false</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    sum+&#x3D;i;</span><br><span class="line"></span><br><span class="line">    while (cin.peek() &#x3D;&#x3D;&#39; &#39;) &#x2F;&#x2F;  peek挑选出 空格  是空格执行下面</span><br><span class="line">    &#123;</span><br><span class="line">        cin.get(); &#x2F;&#x2F;跳过空格字符</span><br><span class="line">    &#125;</span><br><span class="line">    if (cin.peek()&#x3D;&#x3D;&#39;\n&#39;)</span><br><span class="line">    &#123;</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cout &lt;&lt;&quot;the result is：&quot;&lt;&lt;sum&lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ignore"><a href="#ignore" class="headerlink" title="ignore "></a><font color=orange>ignore </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">int a,b,c;</span><br><span class="line">cout&lt;&lt;&quot;input a:&quot;;</span><br><span class="line">cin&gt;&gt;a;</span><br><span class="line">cin.ignore(1024, &#39;\n&#39;);</span><br><span class="line">cout&lt;&lt;&quot;input b:&quot;;</span><br><span class="line">cin&gt;&gt;b;</span><br><span class="line">cin.ignore(1024, &#39;\n&#39;);</span><br><span class="line">cout&lt;&lt;&quot;input c:&quot;;</span><br><span class="line">cin&gt;&gt;c;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;&quot;\t&quot;&lt;&lt;b&lt;&lt;&quot;\t&quot;&lt;&lt;c&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="gcount"><a href="#gcount" class="headerlink" title="gcount() "></a><font color=orange>gcount() </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">const int SIZE&#x3D;10;</span><br><span class="line">char buf[SIZE];</span><br><span class="line">cout &lt;&lt; &quot;please enter a text&quot;;</span><br><span class="line">cin .read(buf, 20);  &#x2F;&#x2F;  cin  输入  cout输出</span><br><span class="line">cout &lt;&lt;&quot;the number of char is:&quot;&lt;&lt; cin.gcount() &lt;&lt; endl; &#x2F;&#x2F;并返回上一次输入操作被读入的字符的数目</span><br><span class="line">cout &lt;&lt; &quot;the text is:&quot;;</span><br><span class="line">cout.write (buf,20);   &#x2F;&#x2F;  从缓冲区输出20个字符</span><br><span class="line">cout &lt;&lt;endl;</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="peek"><a href="#peek" class="headerlink" title="peek() "></a><font color=orange>peek() </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"> char p;</span><br><span class="line"> cout &lt;&lt;&quot;please enter a text: \n&quot;;</span><br><span class="line"></span><br><span class="line"> while (cin.peek()!&#x3D;&#39;\n&#39;)   &#x2F;&#x2F;从cin中挑去一个字符，等于“\n” 就继续，不等于跳出循环</span><br><span class="line"> &#123;</span><br><span class="line">     cout&lt;&lt;(p&#x3D;cin.get()); &#x2F;&#x2F;  获取字符给p, 再传给cout.</span><br><span class="line">     </span><br><span class="line"> &#125;</span><br><span class="line"> cout&lt;&lt;endl;</span><br><span class="line"> return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="cin-getline-x-y"><a href="#cin-getline-x-y" class="headerlink" title="cin.getline(x.y) "></a><font color=orange>cin.getline(x.y) </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main ()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">char buf[20];</span><br><span class="line">cin.ignore(7);  &#x2F;&#x2F;忽略前7个字符</span><br><span class="line">cin.getline(buf,10);  &#x2F;&#x2F; buf 一共19个字符，获取10个到这里面存放</span><br><span class="line">cout&lt;&lt; buf&lt;&lt;endl;</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="temple-change–IO-demo"><a href="#temple-change–IO-demo" class="headerlink" title="temple change–IO demo"></a><font color=orange>temple change–IO demo</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 编写一个  温度单位转换程序， 提示用户以【xx.x C】 或 【xx.xF】 的格式</span><br><span class="line">&#x2F;&#x2F; 如果用户输入的是34.2 C程序将自动转换尾90.32 F并输出</span><br><span class="line"></span><br><span class="line">int  main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F;  华氏温度&#x3D;摄氏温度*9.0 &#x2F;5.0 +32</span><br><span class="line">    const unsigned short ADD_SUBTRACT&#x3D;32;</span><br><span class="line">    const double RATIO&#x3D;9.0&#x2F;5.0;</span><br><span class="line"></span><br><span class="line">    double templein,templeout;</span><br><span class="line">    char typeIn, typeOut;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    std::cout&lt;&lt;&quot; please use this type [xx.x C] or[xx.xF]:&quot;;</span><br><span class="line">    std::cin&gt;&gt;templein&gt;&gt;typeIn;</span><br><span class="line">    cin.ignore(100,&#39;\n&#39;); &#x2F;&#x2F;忽略 \n  在100长的字节里省去 &#39;\n&#39;</span><br><span class="line">    </span><br><span class="line">    std::cout&lt;&lt;&#39;\n&#39;;</span><br><span class="line">  </span><br><span class="line">    switch (typeIn)</span><br><span class="line">    &#123;</span><br><span class="line">    case &#39;C&#39;:</span><br><span class="line">    case &#39;c&#39;:</span><br><span class="line">        templeout&#x3D;templein *RATIO+ADD_SUBTRACT;</span><br><span class="line">        typeOut&#x3D;&#39;F&#39;;</span><br><span class="line">        typeIn&#x3D;&#39;C&#39;;</span><br><span class="line">        break;</span><br><span class="line">    case &#39;F&#39;:</span><br><span class="line">    case &#39;f&#39;:</span><br><span class="line">        templeout&#x3D;(templein-ADD_SUBTRACT)&#x2F;RATIO;</span><br><span class="line">        typeOut&#x3D;&#39;C&#39;;</span><br><span class="line">        typeIn&#x3D;&#39;F&#39;;</span><br><span class="line">        break;</span><br><span class="line">        </span><br><span class="line">    default:</span><br><span class="line">        typeOut&#x3D;&#39;E&#39;;</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (typeOut!&#x3D;&#39;E&#39;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout&lt;&lt;templein&lt;&lt;typeIn&lt;&lt;&quot;&#x3D;&quot;&lt;&lt;templeout&lt;&lt;typeOut&lt;&lt;&quot;\n\n&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout&lt;&lt;&quot;input is wrong&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Y-N–IO-demo"><a href="#Y-N–IO-demo" class="headerlink" title="Y/N–IO demo"></a><font color=orange>Y/N–IO demo</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;向用户提出一个“Y&#x2F;N”问题, 然后把用户输入的值赋值给answer变量</span><br><span class="line">&#x2F;&#x2F;  针对用户输入y,Y,N,进行过滤</span><br><span class="line"> int main()</span><br><span class="line"> &#123;</span><br><span class="line">     char answer;</span><br><span class="line">     std::cout&lt;&lt; &quot;are you a Chinese? [Y&#x2F;N]&quot; &lt;&lt; &quot;\n&quot;;</span><br><span class="line">     std::cin&gt;&gt;answer;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">     switch ( answer)</span><br><span class="line">     &#123;</span><br><span class="line">     case &#39;Y&#39;:</span><br><span class="line">     case &#39;y&#39;:</span><br><span class="line">       std::cout&lt;&lt; &quot;it is wrong&quot; &lt;&lt; &quot;\n&quot;;</span><br><span class="line">       break;</span><br><span class="line">    case &#39;N&#39;:</span><br><span class="line">    case &#39;n&#39;:</span><br><span class="line">        </span><br><span class="line">         std::cout&lt;&lt; &quot;it is right&quot; &lt;&lt; &quot;\n&quot;;</span><br><span class="line">         break;</span><br><span class="line">    default:</span><br><span class="line">        std::cout&lt;&lt; &quot;all wrong!!!!!!&quot; &lt;&lt; &quot;\n&quot;;</span><br><span class="line">        return 0;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F;std::cout&lt;&lt;&quot;pause&quot;&lt;&lt;&quot;\n&quot;;</span><br><span class="line">  &#x2F;&#x2F;std::cin.get();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="read-and-write"><a href="#read-and-write" class="headerlink" title="read and write"></a><font color=orange>read and write</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;fstream&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">fstream fp(&quot;text.txt&quot;, ios::in | ios::out);</span><br><span class="line">if (!fp)</span><br><span class="line">&#123;</span><br><span class="line">cerr&lt;&lt; &quot;open file failed&quot; &lt;&lt; endl;</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">fp&lt;&lt;&quot;I loce dc&quot;;</span><br><span class="line">static char str[10];</span><br><span class="line"></span><br><span class="line">fp.seekg(ios::beg); &#x2F;&#x2F;使得文件指针指向文件头 ios::end则是文件尾</span><br><span class="line">fp&gt;&gt;str;</span><br><span class="line">cout&lt;&lt;str&lt;&lt;endl;</span><br><span class="line">fp.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="readfile"><a href="#readfile" class="headerlink" title="readfile"></a><font color=orange>readfile</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;fstream&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">ifstream in;  &#x2F;&#x2F; 文件输入流的类,   文件读取类ifstream</span><br><span class="line">in.open(&quot;text.txt&quot;);</span><br><span class="line">if (!in)</span><br><span class="line">&#123;</span><br><span class="line">  cerr&lt;&lt;&quot;open file failed&quot; &lt;&lt; endl;</span><br><span class="line">  return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">char x;</span><br><span class="line">while (in &gt;&gt; x) &#x2F;&#x2F; 文件中的数据从in 流向 x再从x流向cout.</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;x;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">cout&lt;&lt;endl;</span><br><span class="line">in.close();</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="writetofile"><a href="#writetofile" class="headerlink" title="writetofile"></a><font color=orange>writetofile</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;fstream&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    ofstream out;   &#x2F;&#x2F;ofstream 这个类整了 out对象</span><br><span class="line">    out.open(&quot;text.txt&quot;,ios::app); &#x2F;&#x2F; </span><br><span class="line">    if (!out)</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line">        cerr&lt;&lt;&quot;open file failed&quot;&lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    for (int i&#x3D;0;i&lt;23;i++)</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">        out&lt;&lt;i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    out &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    out.close();</span><br><span class="line">    return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="array-and-pointer-数组和指针"><a href="#array-and-pointer-数组和指针" class="headerlink" title="array and pointer 数组和指针"></a><font color=orange>array and pointer 数组和指针</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">&#x2F;&#x2F; 如果我们想通过指针访问其他数组元素，应该怎么办？</span><br><span class="line">&#x2F;&#x2F; 指针运算 的奇妙之处在于，并不是将地址值简单+1处理，他是按着指向的数组的数据类型来递增的，也就是+sizeof(int)</span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">const unsigned short ITEMS&#x3D;5;</span><br><span class="line">int intArray[ITEMS]&#x3D;&#123;1,2,3,4,5&#125;;</span><br><span class="line">char charArray[ITEMS]&#x3D;&#123;&#39;F&#39;,&#39;i&#39;&#125;;</span><br><span class="line"></span><br><span class="line">int *intPtr&#x3D;intArray;</span><br><span class="line">char *charPtr&#x3D;charArray;</span><br><span class="line"></span><br><span class="line">std::cout &lt;&lt;&quot;output: &quot;&lt;&lt;&quot;\n&quot;;</span><br><span class="line">for (int i&#x3D;0;i&lt;ITEMS;i++)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt;*intPtr &lt;&lt;&quot; at &quot;&lt;&lt; reinterpret_cast &lt;unsigned long&gt;(intPtr) &lt;&lt;&#39;\n&#39;;</span><br><span class="line">    intPtr++;  &#x2F;&#x2F; 地址加+1指向下一个元素</span><br><span class="line">&#125;</span><br><span class="line">std::cout &lt;&lt;&quot;output: &quot;&lt;&lt;&quot;\n&quot;;</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Class-car"><a href="#Class-car" class="headerlink" title="Class-car"></a><font color=orange>Class-car</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">class Car</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">std::string color;</span><br><span class="line">std::string engine;</span><br><span class="line">float gas_tank;</span><br><span class="line">unsigned int wheel;</span><br><span class="line"></span><br><span class="line">void fill_tank(float liter);</span><br><span class="line">void running(void);</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void Car::fill_tank(float liter)  &#x2F;&#x2F; &quot;::&quot;  作用域解析操作符，作用是告诉编译器这个方法存在于何处，或者是属于哪一个类</span><br><span class="line">&#123;</span><br><span class="line">    gas_tank+&#x3D;liter;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="pointer–Change-age"><a href="#pointer–Change-age" class="headerlink" title="pointer–Change age"></a><font color=orange>pointer–Change age</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">void changeAge(int *age, int newAge);</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">int age&#x3D;24;</span><br><span class="line">std::cout&lt;&lt;&quot; my age is:&quot; &lt;&lt;age&lt;&lt;&quot;\n&quot;;</span><br><span class="line"></span><br><span class="line">changeAge(&amp;age,age+1);</span><br><span class="line"></span><br><span class="line">std::cout&lt;&lt;&quot; my age is:&quot; &lt;&lt;age&lt;&lt;&quot;\n&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void changeAge(int *age, int newAge)</span><br><span class="line">&#123;&#x2F;&#x2F; 如果传入的是地址，在函数中必须要通过 “*”  对指针进行解引用，除非有其他用途</span><br><span class="line"></span><br><span class="line">*age&#x3D;newAge;</span><br><span class="line">std::cout&lt;&lt;&quot; my age is:&quot; &lt;&lt;*age&lt;&lt;&quot;\n&quot;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="array1"><a href="#array1" class="headerlink" title="array1"></a><font color=orange>array1</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">#define ITEM 10</span><br><span class="line">&#x2F;&#x2F;定义一个 数组容纳10个整数，这些整数来组用户输入，我们将这些值累加和，平均值输出</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  </span><br><span class="line">    int num[ITEM];</span><br><span class="line">    cout&lt;&lt;&quot; please input 10 &quot;;</span><br><span class="line"></span><br><span class="line">    for (int i&#x3D;0;i&lt;ITEM;i++)</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line">        std::cout&lt;&lt; &quot;please input:&quot;&lt;&lt;i+1&lt;&lt;&quot; data:&quot;;</span><br><span class="line">        while( !(std::cin&gt;&gt;num[i]))</span><br><span class="line">        &#123;</span><br><span class="line">            std::cin.clear();</span><br><span class="line">            cin.ignore(100,&#39;\n&#39;); &#x2F;&#x2F; 清空缓冲值</span><br><span class="line">            std::cout&lt;&lt;&quot;please enter right number&quot;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int total&#x3D;0;</span><br><span class="line">    for (int j&#x3D;0;j&lt;ITEM;j++)</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">        total+&#x3D;num[j];</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout&lt;&lt;&quot;the sum is:&quot;&lt;&lt; total&lt;&lt;&quot;\n&quot;;</span><br><span class="line">    std::cout &lt;&lt;&quot; the averacy is：&quot;&lt;&lt;total&#x2F;ITEM;</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="array2"><a href="#array2" class="headerlink" title="array2"></a><font color=orange>array2</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;string&gt;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">std::string str;  &#x2F;&#x2F; 定义一个字符变量叫str</span><br><span class="line">std::cout&lt;&lt;&quot; please input string:&quot;;</span><br><span class="line">std::getline(std::cin,str);</span><br><span class="line">std::cout&lt;&lt;str;</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="reload-重载"><a href="#reload-重载" class="headerlink" title="reload 重载"></a><font color=orange>reload 重载</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 编写一个  温度单位转换程序， 提示用户以【xx.x C】 或 【xx.xF】 的格式</span><br><span class="line">&#x2F;&#x2F; 如果用户输入的是34.2 C程序将自动转换尾90.32 F并输出</span><br><span class="line">&#x2F;&#x2F;重载，只能通过不同参数进行重载</span><br><span class="line">void convertTemperature(double templein, char typein);</span><br><span class="line">void convertTemperature(int templein, char typein);</span><br><span class="line"></span><br><span class="line">int  main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F;  华氏温度&#x3D;摄氏温度*9.0 &#x2F;5.0 +32</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    double tempIn;</span><br><span class="line">    int tempInInt;</span><br><span class="line">    char typeIn;</span><br><span class="line"></span><br><span class="line">    std::cout&lt;&lt;&quot; please use this type [xx.x C] or[xx.xF]:&quot;;</span><br><span class="line">    std::cin&gt;&gt;tempIn&gt;&gt;typeIn;</span><br><span class="line">    cin.ignore(100,&#39;\n&#39;); &#x2F;&#x2F;忽略 \n  在100长的字节里省去 &#39;\n&#39;</span><br><span class="line">    std::cout&lt;&lt;&#39;\n&#39;;</span><br><span class="line">    convertTemperature(tempIn,typeIn);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    std::cout&lt;&lt;&quot; please use this type [xxC] or[xxF]:&quot;;</span><br><span class="line">    std::cin&gt;&gt; tempInInt&gt;&gt;typeIn;</span><br><span class="line">    cin.ignore(100,&#39;\n&#39;); &#x2F;&#x2F;忽略 \n  在100长的字节里省去 &#39;\n&#39;</span><br><span class="line">   </span><br><span class="line">    std::cout&lt;&lt;&#39;\n&#39;;</span><br><span class="line">    convertTemperature(tempInInt,typeIn);</span><br><span class="line">    </span><br><span class="line">    return 0;</span><br><span class="line">  </span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void convertTemperature(double templein, char typein)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    const unsigned short ADD_SUBTRACT&#x3D;32;</span><br><span class="line">    const double RATIO&#x3D;9.0&#x2F;5.0;</span><br><span class="line"></span><br><span class="line">    double templeout;</span><br><span class="line">    char typeOut;</span><br><span class="line">    switch (typein)</span><br><span class="line">    &#123;</span><br><span class="line">    case &#39;C&#39;:</span><br><span class="line">    case &#39;c&#39;:</span><br><span class="line">        templeout&#x3D;templein *RATIO+ADD_SUBTRACT;</span><br><span class="line">        typeOut&#x3D;&#39;F&#39;;</span><br><span class="line">        typein&#x3D;&#39;C&#39;;</span><br><span class="line">        break;</span><br><span class="line">    case &#39;F&#39;:</span><br><span class="line">    case &#39;f&#39;:</span><br><span class="line">        templeout&#x3D;(templein-ADD_SUBTRACT)&#x2F;RATIO;</span><br><span class="line">        typeOut&#x3D;&#39;C&#39;;</span><br><span class="line">        typein&#x3D;&#39;F&#39;;</span><br><span class="line">        break;</span><br><span class="line">        </span><br><span class="line">    default:</span><br><span class="line">        typeOut&#x3D;&#39;E&#39;;</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (typeOut!&#x3D;&#39;E&#39;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout&lt;&lt;templein&lt;&lt;typein&lt;&lt;&quot;&#x3D;&quot;&lt;&lt;templeout&lt;&lt;typeOut&lt;&lt;&quot;\n\n&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout&lt;&lt;&quot;input is wrong&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void convertTemperature(int templein, char typein)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    const unsigned short ADD_SUBTRACT&#x3D;32;</span><br><span class="line">    const double RATIO&#x3D;9.0&#x2F;5.0;</span><br><span class="line"></span><br><span class="line">    int tempout;</span><br><span class="line">    char typeOut;</span><br><span class="line">    switch (typein)</span><br><span class="line">    &#123;</span><br><span class="line">    case &#39;C&#39;:</span><br><span class="line">    case &#39;c&#39;:</span><br><span class="line">        tempout&#x3D;templein *RATIO+ADD_SUBTRACT;</span><br><span class="line">        typeOut&#x3D;&#39;F&#39;;</span><br><span class="line">        typein&#x3D;&#39;C&#39;;</span><br><span class="line">        break;</span><br><span class="line">    case &#39;F&#39;:</span><br><span class="line">    case &#39;f&#39;:</span><br><span class="line">        tempout&#x3D;(templein-ADD_SUBTRACT)&#x2F;RATIO;</span><br><span class="line">        typeOut&#x3D;&#39;C&#39;;</span><br><span class="line">        typein&#x3D;&#39;F&#39;;</span><br><span class="line">        break;</span><br><span class="line">        </span><br><span class="line">    default:</span><br><span class="line">        typeOut&#x3D;&#39;E&#39;;</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (typeOut!&#x3D;&#39;E&#39;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout&lt;&lt;templein&lt;&lt;typein&lt;&lt;&quot;&#x3D;&quot;&lt;&lt;tempout&lt;&lt;typeOut&lt;&lt;&quot;\n\n&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout&lt;&lt;&quot;input is wrong&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="union-联合"><a href="#union-联合" class="headerlink" title="union 联合"></a><font color=orange>union 联合</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">&#x2F;&#x2F; 联合  与结构有很多相同之处，来拟合也可以容纳多种不同的类型的值， 但是它每次只能存储这些值的某一个</span><br><span class="line">&#x2F;&#x2F; eg：我们要定义一个变量来存放某种密码，我们可以选择是G的生日，身份证后4位或者也是你养宠物的名字</span><br><span class="line">&#x2F;&#x2F; 这时 联合类型是个好选择</span><br><span class="line">union mima</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">unsigned long  birthday;</span><br><span class="line">unsigned short ssn;</span><br><span class="line">char*  pet;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">int  main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">mima  mima_l;</span><br><span class="line">mima_l.birthday&#x3D;19988065;</span><br><span class="line">std:: cout&lt;&lt; mima_l.birthday&lt;&lt;&quot;\n&quot;;</span><br><span class="line"></span><br><span class="line">mima_l.pet&#x3D;&quot;Chaozai&quot;;&#x2F;&#x2F; 这个联合 将把chaozai 存入到 mima_1联合的pet成员， 并丢弃birthday  成员里的值</span><br><span class="line">std:: cout&lt;&lt;mima_l.pet&lt;&lt;&quot;\n&quot;;</span><br><span class="line">std::cout &lt;&lt;mima_l.birthday&lt;&lt;&quot;\n&quot;; &#x2F;&#x2F; “Chaozai”  首字母的地址</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="enum-枚举"><a href="#enum-枚举" class="headerlink" title="enum 枚举"></a><font color=orange>enum 枚举</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">&#x2F;&#x2F; 枚举</span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">enum weekdays&#123;Monday, Tueesday,Wednesday, Thursday, Friday&#125;;</span><br><span class="line">weekdays today;</span><br><span class="line"></span><br><span class="line">today&#x3D;Monday;</span><br><span class="line"></span><br><span class="line">std::cout&lt;&lt;today&lt;&lt;&quot;\n&quot;;  &#x2F;&#x2F; 返回 0 在weekdays&#123;Monday, Tueesday,Wednesday, Thursday, Friday&#125;中的位置</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="swap"><a href="#swap" class="headerlink" title="swap"></a><font color=orange>swap</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">void swap(int *a,int *b);</span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">        int x&#x3D;3;</span><br><span class="line">        int y&#x3D;4;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        std::cout&lt;&lt; &quot;please input two value: &quot;;</span><br><span class="line">        &#x2F;&#x2F; std::cin&gt;&gt;x&gt;&gt;y;</span><br><span class="line"></span><br><span class="line">        swap(&amp;x,&amp;y);</span><br><span class="line">        std::cout &lt;&lt;&quot;after change: &quot;&lt;&lt; x&lt;&lt;&#39; &#39;&lt;&lt;y&lt;&lt;&quot;\n\n&quot;;</span><br><span class="line">        return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void swap(int *a,int *b)</span><br><span class="line">&#123;</span><br><span class="line">    int temp;</span><br><span class="line">    temp&#x3D;*a;</span><br><span class="line">    *a&#x3D;*b;</span><br><span class="line">    *b&#x3D;temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="swap-plus"><a href="#swap-plus" class="headerlink" title="swap_plus"></a><font color=orange>swap_plus</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">&#x2F;&#x2F; 引用传递</span><br><span class="line">void swap(int &amp;a,int &amp;b); &#x2F;&#x2F;告诉编译器，我要接受的是一个地址</span><br><span class="line">int main()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">        int x&#x3D;3;</span><br><span class="line">        int y&#x3D;4;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        std::cout&lt;&lt; &quot;please input two value: &quot;;</span><br><span class="line">        &#x2F;&#x2F; std::cin&gt;&gt;x&gt;&gt;y;</span><br><span class="line"></span><br><span class="line">        swap(x,y);</span><br><span class="line">        std::cout &lt;&lt;&quot;after change: &quot;&lt;&lt; x&lt;&lt;&#39; &#39;&lt;&lt;y&lt;&lt;&quot;\n\n&quot;;</span><br><span class="line">        return 0;  &#x2F;&#x2F;0代表成功</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void swap(int &amp;a,int &amp;b)</span><br><span class="line">&#123;</span><br><span class="line">    int temp;</span><br><span class="line">    temp&#x3D;a;</span><br><span class="line">    a&#x3D;b;</span><br><span class="line">    b&#x3D;temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>int main (int argc, char**argv) in C++</title>
      <link href="2020/05/29/int-main-int-argc-char-argv-in-C/"/>
      <url>2020/05/29/int-main-int-argc-char-argv-in-C/</url>
      
        <content type="html"><![CDATA[<h3 id="int-main-int-argc-char-argv-in-C"><a href="#int-main-int-argc-char-argv-in-C" class="headerlink" title="int main (int argc, char**argv) in C++"></a><font color=orange>int main (int argc, char**argv) in C++</font></h3><p>int main ( int argc , char** argv )  and int main ( int argc , char* argv[] ) have the same effect</p><p>this is a main_argv.cpp, you can input it in your vscode.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">G:\Project_Li\C++demo\C_plus_little\</span><br><span class="line"></span><br><span class="line">.vscode</span><br><span class="line">main_argv.cpp</span><br><span class="line">main_argv.exe</span><br><span class="line">there are 3 files in the document C_plus_little</span><br><span class="line"></span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main( int argc , char** argv )&#123;</span><br><span class="line">    cout &lt;&lt; &quot;The argv is : &quot;&lt;&lt; endl ;</span><br><span class="line">    for(int i&#x3D;0 ; i&lt;argc ; i++)</span><br><span class="line">         cout &lt;&lt; argv[i] &lt;&lt; endl ;</span><br><span class="line">    cout &lt;&lt; &quot;the argc is : &quot;&lt;&lt; argc &lt;&lt; endl ; </span><br><span class="line">    return 0;</span><br><span class="line">　&#125;</span><br></pre></td></tr></table></figure><p>when you input it, you can run your code. After that, there is a file “main_argv.exe”  have the same path with main_argv.cpp.</p><p>under document C_plus_little:<br>you can input this commond in the cmd:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;main_argv &#39;This&#39; &#39;is&#39; &#39;just&#39; &#39;a&#39; &#39;test&#39;</span><br></pre></td></tr></table></figure><p>and then, output:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">This</span><br><span class="line">is</span><br><span class="line">just</span><br><span class="line">a</span><br><span class="line">test</span><br><span class="line">the argc is : 6</span><br></pre></td></tr></table></figure><p>Note : the content in every quotation mark is a parameter, plus the code name “C_plus_little”, total 6, so  the argc is : 6. argv is the pointer array to point these parameters</p><p>每个引号的内容为一个参数，再加上程序名 C_plus_little ，一共６个参数。最终输出的参数个数 argc 是６.argv则是指向这些参数的指针数组。</p><p>or you can build a sh file, int.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">.&#x2F;main_argv \</span><br><span class="line">-This\</span><br><span class="line">-is\</span><br><span class="line">-just\</span><br><span class="line">-a\</span><br><span class="line">-test\</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>output</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">The argv is :</span><br><span class="line">G:\Project_Li\C++demo\C_plus_little\main_argv.exe</span><br><span class="line">-This</span><br><span class="line">-is</span><br><span class="line">-just</span><br><span class="line">-a</span><br><span class="line">-test</span><br><span class="line">the argc is : 6</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Starter in Vscode-C++</title>
      <link href="2020/05/28/Starter-in-Vscode-C/"/>
      <url>2020/05/28/Starter-in-Vscode-C/</url>
      
        <content type="html"><![CDATA[<p>Detail in <a href="https://github.com/ToneLi/Tutorial">Starter in vscodeC++.pdf</a><br>There are 3 steps to run a C++ code in Vscode:</p><ul><li>pip vscode</li><li>pip mingw-w64</li><li>config the enviroment in vscode</li></ul><h3 id="pip-vscode"><a href="#pip-vscode" class="headerlink" title="pip vscode"></a><font color=orange>pip vscode</font></h3><p>Just download it in <a href="https://code.visualstudio.com/">vscode website</a></p><h3 id="pip-mingw-w64"><a href="#pip-mingw-w64" class="headerlink" title="pip mingw-w64"></a><font color=orange>pip mingw-w64</font></h3><p>You can choose online installation package or offline installation package.<br>It’s difficult to download offline installation package <font color=red>mingw64</font>, so I will provide my link from <a href="https://pan.baidu.com/s/1Ns9ZHisdg5ogMkq-mn3icA">BaiduYun</a>, extraction code: mukr. </p><h3 id="config-the-enviroment-in-vscode"><a href="#config-the-enviroment-in-vscode" class="headerlink" title="config the enviroment in vscode"></a><font color=orange>config the enviroment in vscode</font></h3><p>When you create a .cpp script and run it, it may tell you “you can not find the task g++”. it’s the question about code enviroment. It needs two files:</p><p>file 1: launch.json</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;: &quot;0.2.0&quot;,</span><br><span class="line">    &quot;configurations&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;(gdb) Launch&quot;,</span><br><span class="line">            &quot;preLaunchTask&quot;: &quot;build&quot;,</span><br><span class="line">            &quot;type&quot;: &quot;cppdbg&quot;,</span><br><span class="line">            &quot;request&quot;: &quot;launch&quot;,</span><br><span class="line">            &quot;program&quot;: &quot;$&#123;fileDirname&#125;&#x2F;$&#123;fileBasenameNoExtension&#125;.exe&quot;,</span><br><span class="line">            &quot;args&quot;: [],</span><br><span class="line">            &quot;stopAtEntry&quot;: false,</span><br><span class="line">            &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;,</span><br><span class="line">            &quot;environment&quot;: [],</span><br><span class="line">            &quot;externalConsole&quot;: true,</span><br><span class="line">            &quot;MIMode&quot;: &quot;gdb&quot;,</span><br><span class="line">            &quot;miDebuggerPath&quot;: &quot;E:\\sofeware\\mainGW-W64\\mingw64\\bin\\gdb.exe&quot;, &#x2F;&#x2F; 这里修改GDB路径为安装的mingw64的bin下的gdb.exe路径</span><br><span class="line">            &quot;setupCommands&quot;: [</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;,</span><br><span class="line">                    &quot;text&quot;: &quot;-enable-pretty-printing&quot;,</span><br><span class="line">                    &quot;ignoreFailures&quot;: true</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">file 2: tasks.json</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;: &quot;2.0.0&quot;,</span><br><span class="line">    &quot;tasks&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;label&quot;: &quot;build&quot;,</span><br><span class="line">            &quot;type&quot;: &quot;shell&quot;,</span><br><span class="line">            &quot;group&quot;: &#123;</span><br><span class="line">                &quot;kind&quot;: &quot;build&quot;,</span><br><span class="line">                &quot;isDefault&quot;: true</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;presentation&quot;: &#123;</span><br><span class="line">                &quot;echo&quot;: true,</span><br><span class="line">                &quot;reveal&quot;: &quot;always&quot;,</span><br><span class="line">                &quot;focus&quot;: false,</span><br><span class="line">                &quot;panel&quot;: &quot;shared&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;windows&quot;: &#123;</span><br><span class="line">                &quot;command&quot;: &quot;g++&quot;,</span><br><span class="line">                &quot;args&quot;: [</span><br><span class="line">                    &quot;-ggdb&quot;,</span><br><span class="line">                    &quot;\&quot;$&#123;file&#125;\&quot;&quot;,</span><br><span class="line">                    &quot;--std&#x3D;c++11&quot;,</span><br><span class="line">                    &quot;-o&quot;,</span><br><span class="line">                    &quot;\&quot;$&#123;fileDirname&#125;\\$&#123;fileBasenameNoExtension&#125;.exe\&quot;&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Accepted papers in ACL2020</title>
      <link href="2020/05/23/Accepted-papers-in-ACL2020/"/>
      <url>2020/05/23/Accepted-papers-in-ACL2020/</url>
      
        <content type="html"><![CDATA[<p><a href="https://acl2020.org/program/accepted/">Accepted papers in ACL2020</a>.</p><p>I mainly focus on multi-hop KB QA, there are four papers: </p><ul><li><p>Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings</p></li><li><p>Low-Resource Generation of Multi-hop Reasoning Questions</p></li><li><p>Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering</p></li><li><p>Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Code model in 2020-5-14</title>
      <link href="2020/05/14/Code-model-in-2020-5-14/"/>
      <url>2020/05/14/Code-model-in-2020-5-14/</url>
      
        <content type="html"><![CDATA[<h3 id="python-csv-DictWriter"><a href="#python-csv-DictWriter" class="headerlink" title="python_ csv_DictWriter"></a><font color=orange>python_ csv_DictWriter</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import csv</span><br><span class="line"></span><br><span class="line">def write_csv( csv_name, info):</span><br><span class="line">    with open(csv_name, &#39;a&#39;, newline&#x3D;&#39;&#39;)as f:</span><br><span class="line">        mycsv &#x3D; csv.DictWriter(f, delimiter&#x3D;&#39;|&#39;,fieldnames&#x3D;[&#39;title&#39;, &#39;content&#39;, &#39;type&#39;])</span><br><span class="line"></span><br><span class="line">        mycsv.writerow(info)</span><br><span class="line"></span><br><span class="line">info&#x3D;&#123;&quot;title&quot;:&quot;I am a teacher&quot;,&quot;content&quot;:&quot;Yes, come one baby&quot;,&quot;type&quot;:&quot;str&quot;&#125;</span><br><span class="line"></span><br><span class="line">write_csv(&quot;yes.csv&quot;,info)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="python-csv-DictReader"><a href="#python-csv-DictReader" class="headerlink" title="python_csv_DictReader"></a><font color=orange>python_csv_DictReader</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import csv</span><br><span class="line"></span><br><span class="line">with open(&#39;name.csv&#39;) as csvfile:</span><br><span class="line">    reader &#x3D; csv.DictReader(csvfile)</span><br><span class="line">    for row in reader:</span><br><span class="line">        #循环打印数据的id和class值，此循环执行7次</span><br><span class="line">        print(row[&#39;id&#39;],row[&#39;class&#39;])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="python-add-argument"><a href="#python-add-argument" class="headerlink" title="python_add_argument"></a><font color=orange>python_add_argument</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">parser &#x3D; argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&#39;--sparse&#39;, action&#x3D;&#39;store_true&#39;, default&#x3D;False, help&#x3D;&#39;GAT with sparse version or not.&#39;)</span><br><span class="line">parser.add_argument(&#39;--seed&#39;, type&#x3D;str, default&#x3D;&quot;entities.txt&quot;, help&#x3D;&#39;Random seed.&#39;)</span><br><span class="line">parser.add_argument(&#39;--epochs&#39;, type&#x3D;int, default&#x3D;10000, help&#x3D;&#39;Number of epochs to train.&#39;)</span><br><span class="line"></span><br><span class="line">flags &#x3D; tf.app.flags</span><br><span class="line">flags.DEFINE_float(&quot;learning_rate&quot;, 0.01, &quot;Learning rate for Adam Optimizer.&quot;)</span><br><span class="line">FLAGS &#x3D; flags.FLAGS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">args &#x3D; parser.parse_args()</span><br><span class="line"></span><br><span class="line">print(args.sparse)</span><br><span class="line">print(args.seed)</span><br><span class="line">print(args.epochs)</span><br><span class="line"># print(FLAGS.learning_rate)</span><br></pre></td></tr></table></figure><h3 id="python-NetworkX"><a href="#python-NetworkX" class="headerlink" title="python_NetworkX"></a><font color=orange>python_NetworkX</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import networkx as nx</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">NetworkX 主要用于创造、操作复杂网络，以及学习复杂网络的结构、动力学及其功能。用于分析网络结构，建立网络模型，设计新的网络算法，绘制网络等等</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def que1():</span><br><span class="line">    G &#x3D; nx.DiGraph()</span><br><span class="line"></span><br><span class="line">    G.add_edge(1,2,relation&#x3D;1)</span><br><span class="line">    G.add_edge(2, 5, relation&#x3D;1)</span><br><span class="line">    G.add_edge(2, 3, relation&#x3D;1)</span><br><span class="line">    G.add_edge(2, 1, relation&#x3D;1)</span><br><span class="line">    G.add_edge(1, 8, relation&#x3D;1)</span><br><span class="line">    indeg&#x3D;G.in_degree()</span><br><span class="line">    all_nodes&#x3D;set(nx.nodes(G))</span><br><span class="line">    print(all_nodes)</span><br><span class="line">    # G.add_edge(3, 2)</span><br><span class="line">    print(&quot;输出全部节点：&#123;&#125;&quot;.format(G.nodes()))</span><br><span class="line">    print(&quot;输出全部边：&#123;&#125;&quot;.format(G.edges()))</span><br><span class="line">    print(&quot;输出全部边的数量：&#123;&#125;&quot;.format(G.number_of_edges()))</span><br><span class="line">    nx.draw(G)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def que2():</span><br><span class="line">    G &#x3D; nx.DiGraph()</span><br><span class="line">    G.add_node(1)</span><br><span class="line">    G.add_node(2)</span><br><span class="line">    G.add_nodes_from([3, 4, 5, 6])</span><br><span class="line">    # G.add_cycle([1, 2, 3, 4])</span><br><span class="line">    G.add_edge(1, 3)</span><br><span class="line">    G.add_edges_from([(3, 5), (3, 6), (6, 7)])</span><br><span class="line">    print(&quot;输出全部节点：&#123;&#125;&quot;.format(G.nodes()))</span><br><span class="line">    print(&quot;输出全部边：&#123;&#125;&quot;.format(G.edges()))</span><br><span class="line">    print(&quot;输出全部边的数量：&#123;&#125;&quot;.format(G.number_of_edges()))</span><br><span class="line">    nx.draw(G)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def que3():</span><br><span class="line">    G &#x3D; nx.cubical_graph()</span><br><span class="line">    plt.subplot(121)</span><br><span class="line">    nx.draw(G)</span><br><span class="line">    plt.subplot(122)</span><br><span class="line">    nx.draw(G, pos&#x3D;nx.circular_layout(G), nodecolor&#x3D;&#39;r&#39;, edge_color&#x3D;&#39;b&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line">def que4():</span><br><span class="line">    G &#x3D; nx.path_graph(8)</span><br><span class="line">    nx.draw(G)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def que5():</span><br><span class="line">    G &#x3D; nx.cycle_graph(24)</span><br><span class="line">    pos &#x3D; nx.spring_layout(G, iterations&#x3D;200)</span><br><span class="line">    nx.draw(G, pos, node_color&#x3D;range(24), node_size&#x3D;800, cmap&#x3D;plt.cm.Blues)</span><br><span class="line">    plt.show()</span><br><span class="line">def que6():</span><br><span class="line">    G &#x3D; nx.petersen_graph()</span><br><span class="line">    plt.subplot(121)</span><br><span class="line">    nx.draw(G, with_labels&#x3D;True, font_weight&#x3D;&#39;bold&#39;)</span><br><span class="line">    plt.subplot(122)</span><br><span class="line">    nx.draw_shell(G, nlist&#x3D;[range(5, 10), range(5)], with_labels&#x3D;True, font_weight&#x3D;&#39;bold&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def que7():</span><br><span class="line">    G &#x3D; nx.DiGraph()</span><br><span class="line">    nx.add_path(G, [0, 1, 2, 3])</span><br><span class="line">    print(G.in_degree(1))## node 0 with degree 0  # is the number of edges pointing to the node</span><br><span class="line">    print(list(G.in_degree([0, 1, 2])))</span><br><span class="line">    nx.draw(G)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def que8():</span><br><span class="line">    G &#x3D; nx.DiGraph()</span><br><span class="line">    G.add_edge(1, 2, weight&#x3D;4.7)</span><br><span class="line">    G.add_edges_from([(3, 4), (4, 5)], color&#x3D;&#39;red&#39;)</span><br><span class="line">    G.add_edges_from([(1, 2, &#123;&#39;color&#39;: &#39;blue&#39;&#125;), (2, 3, &#123;&#39;weight&#39;: 8&#125;)])</span><br><span class="line">    G[1][2][&#39;weight&#39;] &#x3D; 4.7</span><br><span class="line">    G.edges[3, 4][&#39;weight&#39;] &#x3D; 4.2</span><br><span class="line">    nx.draw(G)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def que9():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    networkx can produce many classical chat, this demo is complete_graph</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    plt.subplots(2, 2, figsize&#x3D;(15, 6))</span><br><span class="line"></span><br><span class="line">    K_5 &#x3D; nx.complete_graph(5)</span><br><span class="line">    plt.subplot(221)</span><br><span class="line">    plt.title(&#39;complete_graph&#39;)</span><br><span class="line">    nx.draw(K_5, with_labels&#x3D;True, font_weight&#x3D;&#39;bold&#39;)</span><br><span class="line">    plt.axis(&#39;on&#39;)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line">def que10():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for all_simple_paths</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    G &#x3D; nx.complete_graph(4)</span><br><span class="line">    for path in nx.all_simple_paths(G, source&#x3D;0, target&#x3D;3):</span><br><span class="line">        print(path)</span><br><span class="line">    paths &#x3D; nx.all_simple_paths(G, source&#x3D;0, target&#x3D;3, cutoff&#x3D;2)</span><br><span class="line">    print(list(paths))</span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    que10()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="python-NetworkX-1"><a href="#python-NetworkX-1" class="headerlink" title="python_NetworkX"></a><font color=orange>python_NetworkX</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">s&#x3D;[&#39;moonraker&#39;,&quot;ee&quot;,&quot;ttrer&quot;]</span><br><span class="line">pop_obj&#x3D;s.pop(0) # </span><br><span class="line">print (pop_obj)</span><br><span class="line">result: moonraker</span><br><span class="line">s&#x3D;[&quot;ee&quot;,&quot;ttrer&quot;]</span><br></pre></td></tr></table></figure><h3 id="python-str-byte"><a href="#python-str-byte" class="headerlink" title="python_str_byte"></a><font color=orange>python_str_byte</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">b &#x3D; b&quot;example&quot;</span><br><span class="line"></span><br><span class="line"># str object</span><br><span class="line">s &#x3D; &quot;example&quot;</span><br><span class="line"></span><br><span class="line"># str to bytes</span><br><span class="line">bytes(s, encoding&#x3D;&quot;utf8&quot;)</span><br><span class="line"></span><br><span class="line"># bytes to str</span><br><span class="line">str(b, encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line"></span><br><span class="line"># an alternative method</span><br><span class="line"># str to bytes</span><br><span class="line">str.encode(s)</span><br><span class="line"></span><br><span class="line"># bytes to str</span><br><span class="line">print(bytes.decode(b))</span><br></pre></td></tr></table></figure><h3 id="python-tqdm"><a href="#python-tqdm" class="headerlink" title="python_tqdm"></a><font color=orange>python_tqdm</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">for row in tqdm(line):  # tqdm 进度条</span><br><span class="line">    answer &#x3D; row[&#39;answer&#39;]</span><br><span class="line"></span><br><span class="line">---------------------------</span><br><span class="line"></span><br><span class="line">or:</span><br><span class="line">import time</span><br><span class="line">from tqdm import *</span><br><span class="line">for i in tqdm(range(1000)):</span><br><span class="line">    time.sleep(.01) </span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="python-unicodedata"><a href="#python-unicodedata" class="headerlink" title="python_unicodedata"></a><font color=orange>python_unicodedata</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import unicodedata</span><br><span class="line">print(unicodedata.normalize(&#39;NFKD&#39;, u&#39;aあä&#39;).encode(&#39;ascii&#39;, &#39;ignore&#39;))</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">unicodedata.normalize(form, unistr)</span><br><span class="line">把一串UNICODE字符串转换为普通格式的字符串，具体格式支持NFC、NFKC、NFD和NFKD格式。一些文本元素即可以使用静态的预先组合好的形式，也可使用动态组合的形式。Unicode字符的不同表示序列被认为是等价的。如果两个或多个序列被认为是等价的，Unicode标准不规定哪一种特定的序列是正确的，而认为每一个序列只不过与其它序列等价。</span><br><span class="line">如 果需要一种单一的单一的表示方式，可以使用一种规范化的Unicode文本形式来减少不想要区别。Unicode标准定义了四种规范化形式： Normalization Form D (NFD)，Normalization Form KD (NFKD)，Normalization Form C (NFC)，和Normalization Form KC (NFKC)。大约来说，NFD和NFKD将可能的字符进行分解，而NFC和NFKC将可能的字符进行组合。</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import unicodedata</span><br><span class="line">s &#x3D; u&quot;Marek Čech&quot;   #(u表示是unicode而非 ascii码，不加报错！)</span><br><span class="line">line &#x3D; unicodedata.normalize(&#39;NFKD&#39;,s).encode(&#39;ascii&#39;,&#39;ignore&#39;)</span><br><span class="line">print(line)</span><br><span class="line">print(bytes.decode(line))</span><br></pre></td></tr></table></figure><h3 id="python-union"><a href="#python-union" class="headerlink" title="python_union"></a><font color=orange>python_union</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; &#123;&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;&#125;</span><br><span class="line">y &#x3D; &#123;&quot;google&quot;, &quot;runoob&quot;, &quot;apple&quot;&#125;</span><br><span class="line"></span><br><span class="line">z &#x3D; x.union(y)  # &#123;&#39;google&#39;, &#39;apple&#39;, &#39;cherry&#39;, &#39;banana&#39;, &#39;runoob&#39;&#125;</span><br><span class="line"></span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure><h3 id="python-whoosh"><a href="#python-whoosh" class="headerlink" title="python_whoosh"></a><font color=orange>python_whoosh</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Whoosh是一个用来索引文本并能根据索引搜索的的包含类和方法的类库。它允许你开发一</span><br><span class="line">个针对自己内容的搜索引擎。例如，如果你想创建一个博客软件，你可以使用Whoosh添</span><br><span class="line">加一个允许用户搜索博客类目的搜索功能。</span><br><span class="line">https:&#x2F;&#x2F;blog.csdn.net&#x2F;bcmm2009&#x2F;article&#x2F;details&#x2F;88717649</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from whoosh.filedb.filestore import RamStorage</span><br><span class="line">from whoosh.index import create_in</span><br><span class="line">from whoosh.fields import *</span><br><span class="line">st &#x3D; RamStorage()</span><br><span class="line">st.create()</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">当开始使用Whoosh时，你需要一个索引的对象。第一次使用时，你需要创建一个索引，并同时定义一个索引的schema。</span><br><span class="line">schema中要列出索引需要的字段。</span><br><span class="line"></span><br><span class="line">一个字段是索引中的每个文档的一类信息，例如标题或文档内容。一个字段可以同时被储存并索引，也可以只有其中之一</span><br><span class="line">（被索引意味着可以被搜索；被存储意味着获得索引的值与结果一起返回，这个特性在字段是一个标题时比较有用)。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">schema &#x3D; Schema(title&#x3D;TEXT(stored&#x3D;True), path&#x3D;ID(stored&#x3D;True), content&#x3D;TEXT)</span><br><span class="line">ix &#x3D; st.create_index(schema) #当你有了一个Schema，你就可以创建一个索引了</span><br><span class="line"></span><br><span class="line"># #TODO:  现在我们有了一个索引对象，现在我们开始添加文档。索引对象的writer方法返回一个IndexWriter对象，它可以让你添加一个文档到索引中</span><br><span class="line">writer &#x3D; ix.writer()</span><br><span class="line">writer.add_document(title&#x3D;u&quot;First document&quot;, path&#x3D;u&quot;&#x2F;a&quot;, content&#x3D;u&quot;This is the first document we&#39;ve added!&quot;)</span><br><span class="line">writer.add_document(title&#x3D;u&quot;Second document&quot;, path&#x3D;u&quot;&#x2F;b&quot;,content&#x3D;u&quot;The second one is even more interesting!&quot;)</span><br><span class="line">writer.commit() #调用IndexWriter对象的commit方法将文档写入索引</span><br><span class="line"></span><br><span class="line">from whoosh.qparser import QueryParser</span><br><span class="line"># #TODO:现在你的文档已经提交到索引中，你可以搜索他们了,当开始搜索时，我们需要一个Searcher对象：</span><br><span class="line">with ix.searcher() as searcher:</span><br><span class="line">    query &#x3D; QueryParser(&quot;content&quot;, ix.schema).parse(&quot;first good&quot;)</span><br><span class="line">    results &#x3D; searcher.search(query)</span><br><span class="line">    print(results[0])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Data progress for WikiMovie</title>
      <link href="2020/05/13/Data-progress-for-WikiMovie/"/>
      <url>2020/05/13/Data-progress-for-WikiMovie/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1606.03126.pdf?source=post_page---------------------------">WikiMovie</a> was proposed by Facebook in 2016. <a href="https://github.com/dapurv5/neural_kbqa">neural_kbqa</a> used tensorflow to build the Key-Value Memory Network by using this data. By reading its code about progressing data, I found this code do not have enough explanation for reader and ignore some problems. So I decided to comb this code. Enviroment: Python3.5</p><h3 id="Clean-the-source-entities-txt-progress-the-source-entities"><a href="#Clean-the-source-entities-txt-progress-the-source-entities" class="headerlink" title="Clean the source entities.txt, progress the source entities"></a><font color=orange>Clean the source entities.txt, progress the source entities</font></h3><p>entities.txt from the source data in WikiMovie.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">clearn_entities.py</span><br><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Cleans up the entities file.</span><br><span class="line"> - Removes commas</span><br><span class="line"> - Deduplication of entities</span><br><span class="line"> - Converts everything to lowercase</span><br><span class="line"> - Sorts entities lexicographically</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">from sortedcontainers import SortedSet</span><br><span class="line">from text_util import clean_word</span><br><span class="line"></span><br><span class="line">def main(args):</span><br><span class="line">  NEWLINE &#x3D; &quot;\n&quot;</span><br><span class="line">  entities_set &#x3D; SortedSet([])</span><br><span class="line">  count_raw &#x3D; 0</span><br><span class="line">  count_processed &#x3D; 0</span><br><span class="line">  with open(args.input, &#39;r&#39;) as entities_file:</span><br><span class="line">    with open(args.output, &#39;w&#39;) as clean_entities_file:</span><br><span class="line">      for entity in entities_file:</span><br><span class="line">        count_raw +&#x3D; 1</span><br><span class="line">        # print(entity)</span><br><span class="line">        entity_clean &#x3D; bytes.decode(clean_word(entity))</span><br><span class="line">        if len(entity_clean) &gt; 0:</span><br><span class="line">          entities_set.add(entity_clean)</span><br><span class="line">      for entity in entities_set:</span><br><span class="line">        print(entity)</span><br><span class="line">        count_processed +&#x3D; 1</span><br><span class="line">        clean_entities_file.write(entity + NEWLINE)</span><br><span class="line">  print( &quot;COUNT_RAW: &quot;, count_raw)</span><br><span class="line">  print(&quot;COUNT_PROCESSED: &quot;, count_processed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">  parser &#x3D; argparse.ArgumentParser(description&#x3D;&#39;Specify arguments&#39;)</span><br><span class="line">  parser.add_argument(&#39;--input&#39;,type&#x3D;str, default&#x3D;&quot;entities.txt&quot;,help&#x3D;&#39;the raw entities.txt file in WikiMovie&#39;,required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--output&#39;,type&#x3D;str,default&#x3D;&quot;clean_entities.txt&quot;, help&#x3D;&#39;the processed clean_entities.txt file&#39;,required&#x3D;False)</span><br><span class="line">  args &#x3D; parser.parse_args()</span><br><span class="line">  main(args)</span><br><span class="line">  # print(args.input)</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------</span><br><span class="line">text_util.py---&gt; function: clean_word</span><br><span class="line">import unicodedata</span><br><span class="line"></span><br><span class="line">def clean_word(word):</span><br><span class="line">  word &#x3D; word.strip(&#39;\n&#39;)</span><br><span class="line">  word &#x3D; word.strip(&#39;\r&#39;)</span><br><span class="line">  word &#x3D; word.lower()</span><br><span class="line">  word &#x3D; word.replace(&#39;%&#39;, &#39;&#39;) #99 and 44&#x2F;100% dead</span><br><span class="line">  word &#x3D; word.strip()</span><br><span class="line">  word &#x3D; word.replace(&#39;,&#39;, &#39;&#39;)</span><br><span class="line">  word &#x3D; word.replace(&#39;.&#39;, &#39;&#39;)</span><br><span class="line">  word &#x3D; word.replace(&#39;&quot;&#39;, &#39;&#39;)</span><br><span class="line">  word &#x3D; word.replace(&#39;\&#39;&#39;, &#39;&#39;)</span><br><span class="line">  word &#x3D; word.replace(&#39;?&#39;, &#39;&#39;)</span><br><span class="line">  word &#x3D; word.replace(&#39;|&#39;, &#39;&#39;)</span><br><span class="line">  # word &#x3D; str(word, &quot;utf-8&quot;) #Convert str -&gt; unicode (Remember default encoding is ascii in python)</span><br><span class="line">  word &#x3D; unicodedata.normalize(&#39;NFKD&#39;, word).encode(&#39;ascii&#39;,&#39;ignore&#39;) #Convert normalized unicode to python str</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">   &quot;p茅ter bacs贸&quot; in the file,  After using unicodedata.normalize, it disappeared.</span><br><span class="line">   The result of word  unicodedata.normalize is &quot;byte&quot;, not string. so need decode, excample as follow:</span><br><span class="line">   </span><br><span class="line">   b &#x3D; b&quot;example&quot;</span><br><span class="line">   print(bytes.decode(b))</span><br><span class="line">   result: example</span><br><span class="line">   </span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  word &#x3D; word.lower() #Don&#39;t remove this line, lowercase after the unicode normalization</span><br><span class="line">  return word</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Progress-the-wiki-entities-kb-txt"><a href="#Progress-the-wiki-entities-kb-txt" class="headerlink" title="Progress the wiki_entities_kb.txt "></a><font color=orange>Progress the wiki_entities_kb.txt </font></h3><p>aim: kismet directed_by william dieterle —&gt; kismet|directed_by|william dieterle<br>clean_kb.py:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Cleans up the kb file. Produces graph and doc files.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import argparse</span><br><span class="line">import csv</span><br><span class="line"></span><br><span class="line">from text_util import clean_line</span><br><span class="line">from text_util import clean_word</span><br><span class="line">from clean_utils import read_file_as_set</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(args):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  imput:  wiki_entities_kb.txt  and   clean_entities.txt,</span><br><span class="line">  output: clean_wiki-entities_kb_doc.csv (the triple, relations except has_plot)</span><br><span class="line">  output: clean_wiki-entities_kb_doc.csv (the triple, its relation just has_plot)</span><br><span class="line">  :param args:</span><br><span class="line">  :return:</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  relations_set &#x3D; set([])</span><br><span class="line">  entities_set &#x3D; set([])</span><br><span class="line">  valid_entities_set &#x3D; read_file_as_set(args.input_entities)</span><br><span class="line"></span><br><span class="line">  with open(args.input_kb, &#39;r&#39;) as kb_file:</span><br><span class="line">    with open(args.output_graph, &#39;w&#39;,newline&#x3D;&#39;&#39;) as output_graph_file: #  newline&#x3D;&#39;&#39;: to avoid the NUll line&#x2F;empty line 空行</span><br><span class="line">      with open(args.output_doc, &#39;w&#39;,newline&#x3D;&#39;&#39;) as output_doc_file:</span><br><span class="line">        graph_writer &#x3D; csv.DictWriter(output_graph_file, delimiter&#x3D;&#39;|&#39;,</span><br><span class="line">                                      fieldnames&#x3D;[&#39;subject&#39;, &#39;relation&#39;, &#39;object&#39;])</span><br><span class="line">        doc_writer &#x3D; csv.DictWriter(output_doc_file, delimiter&#x3D;&#39;|&#39;,</span><br><span class="line">                                    fieldnames&#x3D;[&#39;entity&#39;, &#39;fieldname&#39;, &#39;content&#39;])</span><br><span class="line">        for line in kb_file:</span><br><span class="line">          line &#x3D; clean_line(line)</span><br><span class="line">          if len(line) &#x3D;&#x3D; 0:</span><br><span class="line">            continue</span><br><span class="line">          e1, e2s, r &#x3D; None, None, None</span><br><span class="line">          cur &#x3D; []</span><br><span class="line">          found_relation &#x3D; False</span><br><span class="line">          for word in line.split(&quot; &quot;)[1:]:</span><br><span class="line">            if &#39;_&#39; in word and not found_relation:  # &quot;_&quot; refer to relation</span><br><span class="line">              r &#x3D; word</span><br><span class="line">              relations_set.add(r)</span><br><span class="line">              e1 &#x3D; &quot; &quot;.join(cur)</span><br><span class="line">              # print(&quot;e1&quot;,e1)</span><br><span class="line">              cur &#x3D; []</span><br><span class="line">              found_relation &#x3D; True</span><br><span class="line">            else:</span><br><span class="line">              cur.append(word)</span><br><span class="line">          e2s &#x3D; &quot; &quot;.join(cur)</span><br><span class="line"></span><br><span class="line">          e1 &#x3D; bytes.decode(clean_word(e1))</span><br><span class="line"></span><br><span class="line">          if r&#x3D;&#x3D;&quot;has_plot&quot;:</span><br><span class="line">            # print(e1, &quot;rrrrr&quot;, e2s, &quot;rrrr&quot;, r)</span><br><span class="line">            write_doc(e1, e2s, r, valid_entities_set, doc_writer)</span><br><span class="line">          else:</span><br><span class="line"></span><br><span class="line">            write_tuples(e1, e2s, r, valid_entities_set, graph_writer, entities_set)</span><br><span class="line">  print(&quot;COUNT_ENTITIES&quot;, len(entities_set))</span><br><span class="line">  print( &quot;COUNT_RELATIONS&quot;, len(relations_set))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def write_doc(e1, e2s, relation, valid_entities_set, doc_writer):</span><br><span class="line">  dict &#x3D; &#123;&#39;entity&#39;: e1, &#39;content&#39;: bytes.decode(clean_word(e2s)), &#39;fieldname&#39;: relation&#125;</span><br><span class="line"></span><br><span class="line">  if e1 in valid_entities_set:</span><br><span class="line">    # print(dict)</span><br><span class="line">    doc_writer.writerow(dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def write_tuples(e1, e2s, relation, valid_entities_set, graph_writer, entities_set):</span><br><span class="line">  for e2 in e2s.split(&quot;,&quot;):</span><br><span class="line">    e2 &#x3D; bytes.decode(clean_word(e2))</span><br><span class="line">    entities_set.add(e1)</span><br><span class="line">    entities_set.add(e2)</span><br><span class="line">    if e1 in valid_entities_set and e2 in valid_entities_set:</span><br><span class="line">      write_to_graph_file(graph_writer, (e1, relation, e2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def write_to_graph_file(graph_writer, tuple):</span><br><span class="line">  dict &#x3D; &#123;&#39;subject&#39;: tuple[0], &#39;relation&#39;: tuple[1], &#39;object&#39;: tuple[2]&#125;</span><br><span class="line">  graph_writer.writerow(dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">  parser &#x3D; argparse.ArgumentParser(description&#x3D;&#39;Specify arguments&#39;)</span><br><span class="line">  parser.add_argument(&#39;--input_kb&#39;,default&#x3D;&quot;wiki_entities_kb.txt&quot;, help&#x3D;&#39;the raw kb file&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--input_entities&#39;, default&#x3D;&quot;clean_entities.txt&quot;,help&#x3D;&#39;the processed entities file&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--output_graph&#39;,default&#x3D;&quot;clean_wiki-entities_kb_graph.csv&quot;, help&#x3D;&#39;the processed graph file&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--output_doc&#39;,default&#x3D;&quot;clean_wiki-entities_kb_doc.csv&quot;, help&#x3D;&#39;the processed document file&#39;, required&#x3D;False)</span><br><span class="line">  args &#x3D; parser.parse_args()</span><br><span class="line">  main(args)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Progress-wiki-entities-qa-train-txt"><a href="#Progress-wiki-entities-qa-train-txt" class="headerlink" title="Progress wiki-entities_qa_train.txt"></a><font color=orange>Progress wiki-entities_qa_train.txt</font></h3><p>aim: what movies are about ginger rogers? top hat, kitty foyle, the barklays of broadway —–&gt; what movies are about ginger rogers top hat|kitty foyle|the barklays of broadway (Note: the answer must be in the entities.txt)<br>clean_qa.py:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Cleans up the qa training&#x2F;test examples.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line">import csv</span><br><span class="line"></span><br><span class="line">from text_util import clean_line</span><br><span class="line">from text_util import clean_word</span><br><span class="line">from clean_utils import read_file_as_set</span><br><span class="line"></span><br><span class="line">def main(args):</span><br><span class="line">  valid_entities_set &#x3D; read_file_as_set(args.input_entities)</span><br><span class="line">  with open(args.input_examples, &#39;r&#39;,newline&#x3D;&#39;&#39;) as examples_file:</span><br><span class="line">    with open(args.output_examples, &#39;w&#39;,newline&#x3D;&#39;&#39;) as output_file:</span><br><span class="line">      writer &#x3D; csv.DictWriter(output_file, delimiter&#x3D;&#39;\t&#39;, fieldnames&#x3D;[&#39;question&#39;, &#39;answer&#39;])</span><br><span class="line">      for line in examples_file:</span><br><span class="line">        line &#x3D; clean_line(line)</span><br><span class="line">        # print(line)</span><br><span class="line">        q, ans &#x3D; line.split(&quot;?\t&quot;)</span><br><span class="line">        # print(q)</span><br><span class="line">        q_words &#x3D; q.split(&quot; &quot;)</span><br><span class="line">        # q_wo</span><br><span class="line">        q_words &#x3D; q_words[1:]</span><br><span class="line">        q_words &#x3D; [bytes.decode(clean_word(w)) for w in q_words]  # For Eg like (True Romance, when was it released?)</span><br><span class="line">        ans_entities &#x3D; ans.split(&quot;,&quot;)</span><br><span class="line">        ans_entities &#x3D; [bytes.decode(clean_word(ans_entity)) for ans_entity in ans_entities]</span><br><span class="line">        #</span><br><span class="line">        valid_ans_entities &#x3D; []</span><br><span class="line">        has_invalid_word &#x3D; False</span><br><span class="line">        for word in ans_entities:</span><br><span class="line">          if word in valid_entities_set:</span><br><span class="line">            valid_ans_entities.append(word)</span><br><span class="line">          else:</span><br><span class="line">            has_invalid_word &#x3D; True #there&#39;s a comma in one of the ans_entities</span><br><span class="line"></span><br><span class="line">        if has_invalid_word:</span><br><span class="line">          is_a_valid_split, valid_ans_entities &#x3D; get_valid_entities(ans_entities, valid_entities_set, 0)</span><br><span class="line">          valid_ans_entities &#x3D; list(reversed(valid_ans_entities))</span><br><span class="line">          # print(valid_ans_entities)</span><br><span class="line">        # #lastly if the line was messed up and you couldn&#39;t find valid entities, pick as many you can find and leave the invalid ones</span><br><span class="line">        if len(valid_ans_entities) &#x3D;&#x3D; 0:</span><br><span class="line">          for word in ans_entities:</span><br><span class="line">            if word in valid_entities_set:</span><br><span class="line">              valid_ans_entities.append(word)</span><br><span class="line"></span><br><span class="line">        if len(valid_ans_entities) &gt; 0:</span><br><span class="line">          writer.writerow(&#123;&#39;question&#39;: &#39; &#39;.join(q_words), &#39;answer&#39;: &#39;|&#39;.join(valid_ans_entities)&#125;)</span><br><span class="line">        #</span><br><span class="line"></span><br><span class="line">def get_valid_entities(potential_entities, dictionary, pos):</span><br><span class="line">  if pos &gt;&#x3D; len(potential_entities):</span><br><span class="line">    return True, []</span><br><span class="line"></span><br><span class="line">  for i in range(pos, len(potential_entities)):</span><br><span class="line">    chunk &#x3D; &quot; &quot;.join(potential_entities[pos:i+1])</span><br><span class="line">    if chunk in dictionary:</span><br><span class="line">      is_a_valid_split, chunks &#x3D; get_valid_entities(potential_entities, dictionary, i+1)</span><br><span class="line">      if is_a_valid_split:</span><br><span class="line">        chunks.append(chunk)</span><br><span class="line">        return True, chunks</span><br><span class="line">  return False, []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># def test_get_valid_entities():</span><br><span class="line">#   print(get_valid_entities([&quot;monster in law&quot;, &quot;they shoot horses&quot;, &quot;don&#39;t they&quot;, &quot;agnes of god&quot;],</span><br><span class="line">#                            set([&quot;monster in law&quot;, &quot;they shoot horses don&#39;t they&quot;,</span><br><span class="line">#                                 &quot;agnes&quot;, &quot;agnes of god&quot;, &quot;a&quot;, &quot;agnes&quot;]), 0))</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">  parser &#x3D; argparse.ArgumentParser(description&#x3D;&#39;Specify arguments&#39;)</span><br><span class="line">  parser.add_argument(&#39;--input_examples&#39;, default&#x3D;&quot;wiki-entities_qa_train.txt&quot;,help&#x3D;&#39;the raw qa pairs&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--input_entities&#39;,default&#x3D;&quot;clean_entities.txt&quot;, help&#x3D;&#39;the entities file&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--output_examples&#39;, default&#x3D;&quot;clean_wiki-entities_qa_train.csv&quot;,help&#x3D;&#39;the processed output file&#39;, required&#x3D;False)</span><br><span class="line">  args &#x3D; parser.parse_args()</span><br><span class="line">  main(args)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="gen-dictionary-py-and-gen-stopwords-py"><a href="#gen-dictionary-py-and-gen-stopwords-py" class="headerlink" title="gen_dictionary.py  and gen_stopwords.py "></a><font color=orange>gen_dictionary.py  and gen_stopwords.py </font></h3><p>produce dictionary and stopwords<br>gen_dictionary.py:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import csv</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">from collections import defaultdict</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"></span><br><span class="line">from data_utils import union</span><br><span class="line"></span><br><span class="line">PIPE &#x3D; &quot;|&quot;</span><br><span class="line">COMMA &#x3D; &quot;,&quot;</span><br><span class="line">SPACE &#x3D; &quot; &quot;</span><br><span class="line">TAB &#x3D; &quot;\t&quot;</span><br><span class="line">NEWLINE &#x3D; &quot;\n&quot;</span><br><span class="line"></span><br><span class="line">words &#x3D; set([])</span><br><span class="line">entities &#x3D; set([])</span><br><span class="line">relations &#x3D; set([])</span><br><span class="line">all &#x3D; set([])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def add_entity(entity):</span><br><span class="line">  entities.add(entity)</span><br><span class="line">  for word in entity.split(SPACE):</span><br><span class="line">    words.add(word)</span><br><span class="line"></span><br><span class="line">def add_sentence(sentence):</span><br><span class="line">  for word in sentence.split(SPACE):</span><br><span class="line">    words.add(word)</span><br><span class="line"></span><br><span class="line">def read_graph_file(graph_path):</span><br><span class="line">  with open(graph_path, &#39;r&#39;) as graph_file:</span><br><span class="line">    print(&quot;reading graph file ...&quot;)</span><br><span class="line">    reader &#x3D; csv.DictReader(graph_file, delimiter&#x3D;PIPE, fieldnames&#x3D;[&#39;e1&#39;, &#39;relation&#39;, &#39;e2&#39;])</span><br><span class="line">    for row in tqdm(reader):</span><br><span class="line">      entity1, relation, entity2 &#x3D; row[&#39;e1&#39;], row[&#39;relation&#39;], row[&#39;e2&#39;]</span><br><span class="line">      add_entity(entity1)</span><br><span class="line">      add_entity(entity2)</span><br><span class="line">      relations.add(relation)</span><br><span class="line">      relations.add(&quot;INV_&quot;+relation)</span><br><span class="line"></span><br><span class="line">def read_doc_file(doc_path):</span><br><span class="line">  with open(doc_path, &#39;r&#39;) as doc_file:</span><br><span class="line">    print( &quot;reading doc file ...&quot;)</span><br><span class="line">    reader &#x3D; csv.DictReader(doc_file, delimiter&#x3D;PIPE, fieldnames&#x3D;[&#39;e&#39;, &#39;relation&#39;, &#39;description&#39;])</span><br><span class="line">    for row in tqdm(reader):</span><br><span class="line">      entity, relation, description &#x3D; row[&#39;e&#39;], row[&#39;relation&#39;], row[&#39;description&#39;]</span><br><span class="line">      add_entity(entity)</span><br><span class="line">      relations.add(relation)</span><br><span class="line">      relations.add(&quot;INV_&quot;+relation)</span><br><span class="line">      add_sentence(description)</span><br><span class="line"></span><br><span class="line">def read_qa_file(qa_path):</span><br><span class="line">  with open(qa_path, &#39;r&#39;) as qa_file:</span><br><span class="line">    print( &quot;reading qa file ...&quot;)</span><br><span class="line">    reader &#x3D; csv.DictReader(qa_file, delimiter&#x3D;TAB, fieldnames&#x3D;[&#39;question&#39;, &#39;answer&#39;])</span><br><span class="line">    for row in tqdm(reader):</span><br><span class="line">      q, a &#x3D; row[&#39;question&#39;], row[&#39;answer&#39;]</span><br><span class="line">      add_sentence(q)</span><br><span class="line">      for e in a.split(PIPE):</span><br><span class="line">        add_entity(e)</span><br><span class="line"></span><br><span class="line">def write_idx(idx_path, s):</span><br><span class="line">  print( &quot;writing &quot;, idx_path, &quot; ...&quot;)</span><br><span class="line">  ordered &#x3D; sorted(s)</span><br><span class="line">  id &#x3D; 1</span><br><span class="line">  with open(idx_path, &#39;w&#39;) as idx_file:</span><br><span class="line">    writer &#x3D; csv.DictWriter(idx_file, delimiter&#x3D;&quot;\t&quot;, fieldnames&#x3D;[&#39;x&#39;, &#39;count&#39;])</span><br><span class="line">    for x in ordered:</span><br><span class="line">      writer.writerow(&#123;&#39;x&#39;: x, &#39;count&#39;: id&#125;)</span><br><span class="line">      id &#x3D; id + 1</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">  path_prefix &#x3D; os.getcwd()+&quot;\data\movieqa\\&quot;</span><br><span class="line">  dataset_name &#x3D; &quot;wiki-entities&quot;</span><br><span class="line">  train_path &#x3D; path_prefix + &quot;clean_&#123;name&#125;_qa_train.txt&quot;.format(name&#x3D;dataset_name)</span><br><span class="line">  test_path &#x3D; path_prefix + &quot;clean_&#123;name&#125;_qa_test.txt&quot;.format(name&#x3D;dataset_name)</span><br><span class="line">  dev_path &#x3D; path_prefix + &quot;clean_&#123;name&#125;_qa_dev.txt&quot;.format(name&#x3D;dataset_name)</span><br><span class="line">  graph_path &#x3D; path_prefix + &quot;clean_&#123;name&#125;_kb_graph.txt&quot;.format(name&#x3D;dataset_name)</span><br><span class="line">  doc_path &#x3D; path_prefix + &quot;clean_&#123;name&#125;_kb_doc.txt&quot;.format(name&#x3D;dataset_name)</span><br><span class="line">  read_graph_file(graph_path)</span><br><span class="line">  read_doc_file(doc_path)</span><br><span class="line">  read_qa_file(train_path)</span><br><span class="line">  read_qa_file(test_path)</span><br><span class="line">  read_qa_file(dev_path)</span><br><span class="line">  write_idx(path_prefix + &quot;&#123;name&#125;_word_idx.txt&quot;.format(name&#x3D;dataset_name), words)</span><br><span class="line">  write_idx(path_prefix + &quot;&#123;name&#125;_relation_idx.txt&quot;.format(name&#x3D;dataset_name), relations)</span><br><span class="line">  write_idx(path_prefix + &quot;&#123;name&#125;_entity_idx.txt&quot;.format(name&#x3D;dataset_name), entities)</span><br><span class="line">  all &#x3D; union(words, entities, relations)</span><br><span class="line">  write_idx(path_prefix + &quot;&#123;name&#125;_idx.txt&quot;.format(name&#x3D;dataset_name), all)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>gen_stopwords.py:<br>The author used question in the raw QA file to produce the stopword.txt. In the first, he made dic for question, and then to select words with more than 500 occurrences as stopwords. Otherwise, he  also considered common bigram phrases as stopwords, eg. the story.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line">import csv</span><br><span class="line"></span><br><span class="line">from sortedcontainers import SortedSet</span><br><span class="line"></span><br><span class="line">FREQ_THRESHOLD &#x3D; 500</span><br><span class="line">NEWLINE &#x3D; &#39;\n&#39;</span><br><span class="line">SPACE &#x3D; &quot; &quot;</span><br><span class="line"></span><br><span class="line">def get_bigrams(sent):</span><br><span class="line">  bigrams &#x3D; []</span><br><span class="line">  for i in range(0, len(sent)-1):</span><br><span class="line">    bigrams.append(sent[i] + SPACE + sent[i+1])</span><br><span class="line">  return bigrams</span><br><span class="line"></span><br><span class="line">def main(args):</span><br><span class="line">  dict &#x3D; &#123;&#125;</span><br><span class="line">  stopwords &#x3D; SortedSet([])</span><br><span class="line">  with open(args.input_examples, &#39;r&#39;) as input_examples_file:</span><br><span class="line">    reader &#x3D; csv.DictReader(input_examples_file, delimiter&#x3D;&#39;\t&#39;,</span><br><span class="line">                            fieldnames&#x3D;[&#39;question&#39;, &#39;answer&#39;])</span><br><span class="line">    for row in reader:</span><br><span class="line">      question &#x3D; row[&#39;question&#39;]</span><br><span class="line">      q_words &#x3D; question.split(&quot; &quot;)</span><br><span class="line">      for word in q_words:</span><br><span class="line">        freq &#x3D; dict.get(word, 0)</span><br><span class="line">        dict[word] &#x3D; freq+1</span><br><span class="line">      #also consider common bigram phrases as stopwords, eg. the story</span><br><span class="line">      for word in get_bigrams(q_words):</span><br><span class="line">        freq &#x3D; dict.get(word, 0)</span><br><span class="line">        dict[word] &#x3D; freq+1</span><br><span class="line"></span><br><span class="line">  if args.kb_docs:</span><br><span class="line">    with open(args.kb_docs, &#39;r&#39;) as kb_docs_file:</span><br><span class="line">      reader &#x3D; csv.DictReader(kb_docs_file, delimiter&#x3D;&#39;|&#39;, fieldnames&#x3D;[&quot;entity_name&quot;, &quot;fieldname&quot;, &quot;content&quot;])</span><br><span class="line">      for row in reader:</span><br><span class="line">        content &#x3D; row[&quot;content&quot;]</span><br><span class="line">        content_words &#x3D; content.split(&quot; &quot;)</span><br><span class="line">        for word in content_words:</span><br><span class="line">          freq &#x3D; dict.get(word, 0)</span><br><span class="line">          dict[word] &#x3D; freq+1</span><br><span class="line"></span><br><span class="line">  with open(args.output, &#39;w&#39;) as output_file:</span><br><span class="line">    writer &#x3D; csv.DictWriter(output_file, delimiter&#x3D;&#39;\t&#39;, fieldnames&#x3D;[&#39;stopword&#39;, &#39;frequency&#39;])</span><br><span class="line">    for word in dict.keys():</span><br><span class="line">      if dict[word] &gt; FREQ_THRESHOLD:</span><br><span class="line">        stopwords.add(word)</span><br><span class="line">    for stopword in stopwords:</span><br><span class="line">      writer.writerow(&#123;&#39;stopword&#39;: stopword, &#39;frequency&#39;: dict[stopword]&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">  parser &#x3D; argparse.ArgumentParser(description&#x3D;&#39;Specify arguments&#39;)</span><br><span class="line">  parser.add_argument(&#39;--input_examples&#39;,default&#x3D;&quot;clean_wiki-entities_qa_train.csv&quot;, help&#x3D;&#39;the raw qa pairs&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--kb_docs&#39;, default&#x3D;&quot;clean_wiki-entities_kb_doc.txt&quot;,help&#x3D;&#39;the clean_wiki-entities_kb_doc.txt file generated by clean_kb.py&#39;,</span><br><span class="line">                      required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--output&#39;,default&#x3D;&quot;stopwords.txt&quot;, help&#x3D;&#39;the stopwords file&#39;, required&#x3D;False)</span><br><span class="line">  args &#x3D; parser.parse_args()</span><br><span class="line">  main(args)</span><br></pre></td></tr></table></figure><h3 id="the-operation-in-KG"><a href="#the-operation-in-KG" class="headerlink" title="the operation in KG"></a><font color=orange>the operation in KG</font></h3><p>for: knowledge_graph.py:<br>two function:  </p><p><font color=red>get_candidate_neighbors</font>： Get all the n hops neighbors from a node in the graph</p><p><font color=red>get_all_paths</font>： find the relation path and entity path in KG.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line">import csv</span><br><span class="line">import networkx as nx</span><br><span class="line"></span><br><span class="line">from text_util import clean_line</span><br><span class="line">import os</span><br><span class="line">PIPE &#x3D; &quot;|&quot;</span><br><span class="line">HIGH_DEGREE_THRESHOLD &#x3D; 50</span><br><span class="line"></span><br><span class="line">class KnowledgeGraph(object):</span><br><span class="line">  def __init__(self, graph_path, unidirectional&#x3D;True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    directed: a value of false indicates that for every fact (e1, R, e2) inserted in the KB,</span><br><span class="line">              (e2, invR, e1) will be inserted in the KB</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    self.G &#x3D; nx.DiGraph()  # derection chat</span><br><span class="line">    with open(graph_path, &#39;r&#39;) as graph_file:</span><br><span class="line">      for line in graph_file:</span><br><span class="line">        line &#x3D; clean_line(line)</span><br><span class="line">        e1, relation, e2 &#x3D; line.split(PIPE)</span><br><span class="line"></span><br><span class="line">        self.G.add_edge(e1, e2,relation&#x3D;relation)   #self.G.add_edge(e1, e2, &#123;&quot;relation&quot;: relation&#125;) old version</span><br><span class="line">        if not unidirectional:</span><br><span class="line">          self.G.add_edge(e2, e1, relation&#x3D;self.get_inverse_relation(relation))</span><br><span class="line">    #</span><br><span class="line">    self.high_degree_nodes &#x3D; set([])</span><br><span class="line">    indeg &#x3D; self.G.in_degree() #The node in_degree is the number of edges pointing to the node</span><br><span class="line">    for v in indeg:</span><br><span class="line">      # print(v[1])</span><br><span class="line">      if v[1] &gt; HIGH_DEGREE_THRESHOLD:  #    old version: if indeg[v] &gt; HIGH_DEGREE_THRESHOLD: old version is not suitable for pythonn3.5</span><br><span class="line">        self.high_degree_nodes.add(v)</span><br><span class="line">    # print(len(self.high_degree_nodes))</span><br><span class="line">    self.all_entities &#x3D; set(nx.nodes(self.G))  #40134</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def get_inverse_relation(self, relation):</span><br><span class="line">    return &quot;INV_&quot;+relation</span><br><span class="line"></span><br><span class="line">  def get_all_paths(self, source, target, cutoff):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Returns two lists, the first is a list of paths from src to target where each path is itself a list</span><br><span class="line">    The paths are represented by nodes (entities) on the path.</span><br><span class="line">    The second list is a list of paths from src to target where each path is itself a list</span><br><span class="line">    The paths are represented by the edge types (relations) on the path</span><br><span class="line">    [ [e1, e2], [e1, e3, e2]], [[r1], [r2, r3] ]</span><br><span class="line">    cutoff: represents the max length of the path allowed</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if source &#x3D;&#x3D; target:</span><br><span class="line">      return [], []</span><br><span class="line">    paths_of_entities &#x3D; []</span><br><span class="line">    paths_of_relations &#x3D; []</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    [&#39;moonraker&#39;, &#39;space&#39;, &#39;you only live twice&#39;, &#39;lewis gilbert&#39;]</span><br><span class="line">    [&#39;moonraker&#39;, &#39;james bond&#39;, &#39;the spy who loved me&#39;, &#39;lewis gilbert&#39;]</span><br><span class="line">    [&#39;moonraker&#39;, &#39;james bond&#39;, &#39;you only live twice&#39;, &#39;lewis gilbert&#39;]</span><br><span class="line">    [&#39;moonraker&#39;, &#39;bond&#39;, &#39;the spy who loved me&#39;, &#39;lewis gilbert&#39;]</span><br><span class="line">    [&#39;moonraker&#39;, &#39;bond&#39;, &#39;you only live twice&#39;, &#39;lewis gilbert&#39;]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for path in nx.all_simple_paths(self.G, source, target, cutoff):</span><br><span class="line">      # print(path)</span><br><span class="line">      paths_of_entities.append(path)</span><br><span class="line">      relations_path &#x3D; []</span><br><span class="line">      for i in range(0, len(path)-1):</span><br><span class="line">        relation &#x3D; self.G[path[i]][path[i+1]][&#39;relation&#39;]</span><br><span class="line">        relations_path.append(relation)</span><br><span class="line">      paths_of_relations.append(relations_path)</span><br><span class="line">    return paths_of_entities, paths_of_relations</span><br><span class="line"></span><br><span class="line">  def get_candidate_neighbors(self, node, num_hops&#x3D;2, avoid_high_degree_nodes&#x3D;True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Get all the n hops neighbors from a node in the graph</span><br><span class="line">    avoid_high_degree_nodes &#x3D; True, skips any path going through a high degree node</span><br><span class="line">    See constructor for definition of high degree nodes</span><br><span class="line"></span><br><span class="line">        A</span><br><span class="line">    |     |</span><br><span class="line">    B     C</span><br><span class="line">    |</span><br><span class="line">    D</span><br><span class="line"></span><br><span class="line">    q&#x3D;[A] pop A    visited&#x3D;[A]   dist&#x3D;&#123;A:0&#125;</span><br><span class="line">                   visited&#x3D;[A,B]   dist&#x3D;&#123;A:0,B:1&#125;</span><br><span class="line">                    visited&#x3D;[A,B,C]   dist&#x3D;&#123;A:0,B:1,C:1&#125;</span><br><span class="line">    q&#x3D;[B] pop B    visited&#x3D;[B]   dist&#x3D;&#123;A:0,B:1,C:1&#125;</span><br><span class="line">                  visited&#x3D;[A,B,C,D]   dist&#x3D;&#123;A:0,B:1,C:1,D:2&#125;</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    result &#x3D; set([])</span><br><span class="line">    q &#x3D; [node]</span><br><span class="line">    visited &#x3D; set([node]) # all nodes</span><br><span class="line">    # print(q)</span><br><span class="line">    dist &#x3D; &#123;node: 0&#125;</span><br><span class="line">    # print(q)</span><br><span class="line">    while len(q) &gt; 0:</span><br><span class="line">      u &#x3D; q.pop(0)</span><br><span class="line">      result.add(u)</span><br><span class="line">      # print(&quot;result&quot;,result)</span><br><span class="line">      for nbr in self.G.neighbors(u):</span><br><span class="line">        if nbr in self.high_degree_nodes and avoid_high_degree_nodes:</span><br><span class="line">          continue</span><br><span class="line">        if nbr not in visited:</span><br><span class="line">          visited.add(nbr)</span><br><span class="line">          dist[nbr] &#x3D; dist[u] + 1</span><br><span class="line">          # print(&quot;dist&quot;,dist[u])</span><br><span class="line">          if dist[nbr] &lt;&#x3D; num_hops:</span><br><span class="line">            q.append(nbr)</span><br><span class="line">    result.remove(node)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">  def get_adjacent_entities(self, node):</span><br><span class="line">    return set(self.G.neighbors(node))</span><br><span class="line"></span><br><span class="line">  def get_relation(self, source, target):</span><br><span class="line">    return self.G[source][target][&#39;relation&#39;]</span><br><span class="line"></span><br><span class="line">  def log_statistics(self):</span><br><span class="line">    print( &quot;NUM_NODES&quot;, len(self.get_entities()))</span><br><span class="line"></span><br><span class="line">  def get_entities(self):</span><br><span class="line">    return self.all_entities</span><br><span class="line"></span><br><span class="line">  def get_high_degree_entities(self):</span><br><span class="line">    return self.high_degree_nodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">  graph_path &#x3D; file_path&#x3D;os.getcwd()+&quot;\data\movieqa\clean_wiki-entities_kb_graph.txt&quot;</span><br><span class="line"></span><br><span class="line">  kb &#x3D; KnowledgeGraph(graph_path, unidirectional&#x3D;False)</span><br><span class="line">  entities_paths, relations_paths &#x3D; kb.get_all_paths(source&#x3D;&quot;moonraker&quot;, target&#x3D;&quot;lewis gilbert&quot;, cutoff&#x3D;3)</span><br><span class="line">  # print(relations_paths)</span><br><span class="line">  print (kb.get_candidate_neighbors(&quot;moonraker&quot;))</span><br><span class="line">  print (len(kb.get_candidate_neighbors(&quot;moonraker&quot;, num_hops&#x3D;2))) #511</span><br><span class="line">  kb.log_statistics()</span><br><span class="line">  print (kb.get_candidate_neighbors(&quot;what&quot;, num_hops&#x3D;1))</span><br><span class="line">  for path in kb.get_all_paths(&#39;bruce lee&#39;, &#39;bruce lee&#39;, cutoff&#x3D;2):</span><br><span class="line">    print (path)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Using-whoosh-to-search"><a href="#Using-whoosh-to-search" class="headerlink" title="Using whoosh to search "></a><font color=orange>Using whoosh to search </font></h3><p>For file search_index.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">import codecs</span><br><span class="line">import unicodedata</span><br><span class="line"></span><br><span class="line">from whoosh import qparser</span><br><span class="line">from whoosh import scoring</span><br><span class="line">from whoosh.index import create_in</span><br><span class="line">from whoosh.fields import *</span><br><span class="line">from whoosh.qparser import QueryParser</span><br><span class="line">from whoosh.filedb.filestore import RamStorage</span><br><span class="line"></span><br><span class="line">from text_util import clean_line</span><br><span class="line">from clean_utils import read_file_as_dict</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">PIPE &#x3D; &quot;|&quot;</span><br><span class="line">SPACE &#x3D; &quot; &quot;</span><br><span class="line"></span><br><span class="line">class SearchIndex(object):</span><br><span class="line">  def __init__(self, doc_path, stopwords&#x3D;None):</span><br><span class="line">    st &#x3D; RamStorage()</span><br><span class="line">    st.create()</span><br><span class="line">    schema &#x3D; Schema(entity_name&#x3D;TEXT(stored&#x3D;True), fieldname&#x3D;TEXT(stored&#x3D;True), content&#x3D;TEXT()) #Schema   模式 schema指明了在一个index中的document的field</span><br><span class="line">    self.ix &#x3D; st.create_index(schema)  #创建索引</span><br><span class="line">    writer &#x3D; self.ix.writer()   #ok，现在我们有了一个索引对象，现在我们开始添加文档</span><br><span class="line">    self.remove_stopwords_while_indexing &#x3D; False</span><br><span class="line">    if stopwords:</span><br><span class="line">      self.remove_stopwords_while_indexing &#x3D; True</span><br><span class="line">      self.stopwords_dict &#x3D; read_file_as_dict(stopwords)</span><br><span class="line"></span><br><span class="line">    with codecs.open(doc_path, &#39;r&#39;, &quot;utf-8&quot;) as doc_file:</span><br><span class="line">      for line in doc_file:</span><br><span class="line">        line &#x3D; clean_line(line)</span><br><span class="line">        entity_name, fieldname, content &#x3D; line.split(PIPE)</span><br><span class="line">        writer.add_document(entity_name&#x3D;entity_name, fieldname&#x3D;fieldname, content&#x3D;self.remove_stopwords_from_text(content))</span><br><span class="line">    writer.commit() #调用IndexWriter对象的commit方法将文档写入索引，  现在你的文档已经提交到索引中，你可以搜索他们了</span><br><span class="line"></span><br><span class="line">  def remove_stopwords_from_text(self, content):</span><br><span class="line">    words &#x3D; content.split(SPACE)</span><br><span class="line">    words_clean &#x3D; []</span><br><span class="line">    for word in words:</span><br><span class="line">      if self.remove_stopwords_while_indexing and word not in self.stopwords_dict:</span><br><span class="line">        words_clean.append(word)</span><br><span class="line">    return &quot; &quot;.join(words_clean) if len(words_clean) &gt; 0 else content</span><br><span class="line"></span><br><span class="line">  def get_candidate_docs(self, question, limit&#x3D;20):</span><br><span class="line">    docs &#x3D; set([])</span><br><span class="line">    question &#x3D; self.remove_stopwords_from_text(question)</span><br><span class="line">    print(question)</span><br><span class="line">    with self.ix.searcher() as searcher:  #当开始搜索时，我们需要一个Searcher对象：</span><br><span class="line">      query &#x3D; QueryParser(&quot;content&quot;, self.ix.schema, group&#x3D;qparser.OrGroup).parse(question) # 搜索 包含question 的 content</span><br><span class="line">      # print(query)</span><br><span class="line">      results &#x3D; searcher.search(query, limit&#x3D;limit) #当你有了一个Searcher和一个query的对象，你可以用Searcher的search()函数执行一个查询并得到一个搜索结果对象</span><br><span class="line">      # print(results)</span><br><span class="line">      for result in results:</span><br><span class="line">        # print(result)</span><br><span class="line">        docs.add(result[&#39;entity_name&#39;])</span><br><span class="line">    # # #TODO: remove th unicode normalization since this will be done earlier in the pipeline</span><br><span class="line">    docs &#x3D; [bytes.decode(unicodedata.normalize(&#39;NFKD&#39;, doc).encode(&#39;ascii&#39;,&#39;ignore&#39;)) for doc in docs]</span><br><span class="line">    return docs</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">  kb_doc &#x3D;os.getcwd() + &quot;\data\movieqa\clean_wiki-entities_kb_doc.txt&quot;</span><br><span class="line">  stopword&#x3D;os.getcwd() + &quot;\data\movieqa\stopwords.txt&quot;</span><br><span class="line">  searcher &#x3D; SearchIndex(kb_doc,stopword)</span><br><span class="line">  print (searcher.get_candidate_docs(&quot;ginger rogers and&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>there is also a excample for whoosh, the basic flow, Chinese version</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Whoosh是一个用来索引文本并能根据索引搜索的的包含类和方法的类库。它允许你开发一</span><br><span class="line">个针对自己内容的搜索引擎。例如，如果你想创建一个博客软件，你可以使用Whoosh添</span><br><span class="line">加一个允许用户搜索博客类目的搜索功能。[字符匹配，去搜索包含该字符的文档，返回title  或者其他信息]</span><br><span class="line">https:&#x2F;&#x2F;blog.csdn.net&#x2F;bcmm2009&#x2F;article&#x2F;details&#x2F;88717649</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from whoosh.filedb.filestore import RamStorage</span><br><span class="line">from whoosh.index import create_in</span><br><span class="line">from whoosh.fields import *</span><br><span class="line">st &#x3D; RamStorage()</span><br><span class="line">st.create()</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">当开始使用Whoosh时，你需要一个索引的对象。第一次使用时，你需要创建一个索引，并同时定义一个索引的schema。</span><br><span class="line">schema中要列出索引需要的字段。</span><br><span class="line"></span><br><span class="line">一个字段是索引中的每个文档的一类信息，例如标题或文档内容。一个字段可以同时被储存并索引，也可以只有其中之一</span><br><span class="line">（被索引意味着可以被搜索；被存储意味着获得索引的值与结果一起返回，这个特性在字段是一个标题时比较有用)。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">schema &#x3D; Schema(title&#x3D;TEXT(stored&#x3D;True), path&#x3D;ID(stored&#x3D;True), content&#x3D;TEXT)</span><br><span class="line">ix &#x3D; st.create_index(schema) #当你有了一个Schema，你就可以创建一个索引了</span><br><span class="line"></span><br><span class="line"># #TODO:  现在我们有了一个索引对象，现在我们开始添加文档。索引对象的writer方法返回一个IndexWriter对象，它可以让你添加一个文档到索引中</span><br><span class="line">writer &#x3D; ix.writer()</span><br><span class="line">writer.add_document(title&#x3D;u&quot;First document&quot;, path&#x3D;u&quot;&#x2F;a&quot;, content&#x3D;u&quot;This is the first document we&#39;ve added!&quot;)</span><br><span class="line">writer.add_document(title&#x3D;u&quot;Second document&quot;, path&#x3D;u&quot;&#x2F;b&quot;,content&#x3D;u&quot;The second one is even more interesting!&quot;)</span><br><span class="line">writer.commit() #调用IndexWriter对象的commit方法将文档写入索引</span><br><span class="line"></span><br><span class="line">from whoosh.qparser import QueryParser</span><br><span class="line"># #TODO:现在你的文档已经提交到索引中，你可以搜索他们了,当开始搜索时，我们需要一个Searcher对象：</span><br><span class="line">with ix.searcher() as searcher:</span><br><span class="line">    query &#x3D; QueryParser(&quot;content&quot;, ix.schema).parse(&quot;first&quot;)</span><br><span class="line">    results &#x3D; searcher.search(query)</span><br><span class="line">    print(results[0])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Get-the-question-ans-entities-sources-relations-targets"><a href="#Get-the-question-ans-entities-sources-relations-targets" class="headerlink" title="Get the question, ans_entities, [sources,relations,targets ]   "></a><font color=orange>Get the question, ans_entities, [sources,relations,targets ]   </font></h3><p>gen_kv_data.py:</p><p> question—&gt; qn_entities —&gt; 去除相邻边数大于50的<br> question—&gt; 通过whoosh 得到相关文档的实体， by  clean_wiki-entities_kb_doc.txt<br> candidate_entities 包含  qn_entities （question 通过知识库检索的实体）, relevant_entities （通过文档查找的相似实体<br> 每个实体都有一个解释， 看哪个解释里包含question, 那么解释对应的实体为relevant_entities）, nbr_qn_entities （qn_entities 的相邻实体）</p><p>This file to make the training data:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line">data_excample:</span><br><span class="line">question: what movies are about ginger rogers</span><br><span class="line">qn_entities: ginger rogers [see function--get_question_entities]</span><br><span class="line">ans_entities: top hat|kitty foyle|the barkleys of broadway (we should use the KG to filter the answer)</span><br><span class="line">sources(h): black widow(h1)|tight spot(h2)|roberta(h3)</span><br><span class="line">relations(r):starred_actors (r1)|starred_actors(r2)|has_tags(r3)</span><br><span class="line">targets(t):van heflin(t1)|edward g robinson(t2)|fred astaire (t3)</span><br><span class="line"></span><br><span class="line">triple:(h1,r1,t1),(h2,r2,t2),(h3,r3,t3)</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def get_question_entities(self, question):</span><br><span class="line">    qn_entities &#x3D; []</span><br><span class="line">    q_words &#x3D; question.split(&quot; &quot;)</span><br><span class="line">    max_gram &#x3D; &quot;&quot;</span><br><span class="line">    for n in range(1, len(q_words) + 1):</span><br><span class="line">      i &#x3D; 0</span><br><span class="line">      while i + n &lt;&#x3D; len(q_words):</span><br><span class="line">        gram &#x3D; q_words[i:i + n]</span><br><span class="line">        gram &#x3D; &quot; &quot;.join(gram)</span><br><span class="line">        if gram in self.valid_entities_set:</span><br><span class="line">          qn_entities.append(gram)</span><br><span class="line">        i &#x3D; i + 1</span><br><span class="line"></span><br><span class="line">    #remove stop entities, substrings, spurious entities</span><br><span class="line">    qn_entities &#x3D; self.remove_all_stopwords_except_one(qn_entities)</span><br><span class="line">    qn_entities &#x3D; self.remove_substrings(qn_entities)</span><br><span class="line">    qn_entities &#x3D; self.remove_spurious_entities(qn_entities, question)</span><br><span class="line">    return set(qn_entities)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line">gen_kv_data.py</span><br><span class="line"></span><br><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;python</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line">import csv</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">from text_util import clean_word</span><br><span class="line">from clean_utils import read_file_as_dict</span><br><span class="line">from knowledge_graph import KnowledgeGraph</span><br><span class="line">from search_index import SearchIndex</span><br><span class="line">from question_parser import QuestionParser</span><br><span class="line">from data_utils import *</span><br><span class="line"></span><br><span class="line">from tqdm import tqdm</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MAX_RELEVANT_ENTITIES &#x3D; 4</span><br><span class="line">HOPS_FROM_QN_ENTITY &#x3D; 1</span><br><span class="line">MAX_CANDIDATE_ENTITIES &#x3D; 1024</span><br><span class="line">MAX_CANDIDATE_TUPLES &#x3D; 2048</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def remove_high_degree_qn_entities(qn_entities):</span><br><span class="line">  &quot;&quot;&quot; Remove all high degree entities from the question except one&quot;&quot;&quot;</span><br><span class="line">  qn_entities_clean &#x3D; set([])</span><br><span class="line">  if len(qn_entities) &gt; 1:</span><br><span class="line">    for qn_entity in qn_entities:</span><br><span class="line">      if qn_entity not in knowledge_base.get_high_degree_entities():</span><br><span class="line">        qn_entities_clean.add(qn_entity)</span><br><span class="line">  return qn_entities_clean if len(qn_entities_clean) &gt; 0 else qn_entities</span><br><span class="line"></span><br><span class="line">def remove_invalid_ans_entities(ans_entities):</span><br><span class="line">  ans_entities_clean &#x3D; set([])</span><br><span class="line">  for ans_entity in ans_entities:</span><br><span class="line">    if ans_entity in knowledge_base.get_entities():</span><br><span class="line">      ans_entities_clean.add(ans_entity)</span><br><span class="line">  return ans_entities_clean if len(ans_entities_clean) &gt; 0 else ans_entities</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_neighboring_entities(entities, num_hops&#x3D;2):</span><br><span class="line">  nbr_entities &#x3D; set([])</span><br><span class="line">  for entity in entities:</span><br><span class="line">    for nbr in knowledge_base.get_candidate_neighbors(entity, num_hops&#x3D;num_hops,</span><br><span class="line">                                                      avoid_high_degree_nodes&#x3D;True):</span><br><span class="line">      nbr_entities.add(nbr)</span><br><span class="line">  return nbr_entities</span><br><span class="line"></span><br><span class="line">def get_tuples_involving_entities(candidate_entities):</span><br><span class="line">  tuples &#x3D; set([])</span><br><span class="line">  for s in candidate_entities:</span><br><span class="line">    if s in knowledge_base.get_high_degree_entities():</span><br><span class="line">      continue</span><br><span class="line">    for t in knowledge_base.get_adjacent_entities(s):</span><br><span class="line">      r &#x3D; knowledge_base.get_relation(s,t)</span><br><span class="line">      tuples.add((s, r, t))</span><br><span class="line">  return tuples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(args):</span><br><span class="line">  with open(args.input_examples, &#39;r&#39;) as input_examples_file:</span><br><span class="line">    with open(args.output_examples, &#39;w&#39;) as output_examples_file:</span><br><span class="line">      reader &#x3D; csv.DictReader(input_examples_file, delimiter&#x3D;&#39;\t&#39;, fieldnames&#x3D;[&#39;question&#39;, &#39;answer&#39;])</span><br><span class="line">      # print(reader)</span><br><span class="line"></span><br><span class="line">      writer &#x3D; csv.DictWriter(output_examples_file, delimiter&#x3D;&#39;\t&#39;,</span><br><span class="line">                              fieldnames&#x3D;[&#39;question&#39;, &#39;qn_entities&#39;, &#39;ans_entities&#39;,</span><br><span class="line">                                          &#39;sources&#39;, &#39;relations&#39;, &#39;targets&#39;])</span><br><span class="line">      for row in tqdm(reader):  # tqdm 进度条</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        question---&gt; qn_entities ---&gt; 去除相邻边数大于50的entity</span><br><span class="line">        question---&gt; 通过whoosh 得到相关文档的实体， by  clean_wiki-entities_kb_doc.txt</span><br><span class="line">        candidate_entities 包含  qn_entities （question 通过知识库检索的实体）, relevant_entities （通过文档查找的相似实体</span><br><span class="line">        每个实体都有一个解释， 看哪个解释里包含question, 那么解释对应的实体为relevant_entities）, nbr_qn_entities （qn_entities 的相邻实体）</span><br><span class="line">        </span><br><span class="line">        triple in candidate_entities [source, relation, targets]</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        answer &#x3D; row[&#39;answer&#39;]</span><br><span class="line">        ans_entities &#x3D; answer.split(PIPE)</span><br><span class="line">        ans_entities &#x3D; remove_invalid_ans_entities(ans_entities)</span><br><span class="line">        question &#x3D; row[&#39;question&#39;]</span><br><span class="line">        qn_entities &#x3D; question_parser.get_question_entities(question)</span><br><span class="line">        qn_entities &#x3D; remove_high_degree_qn_entities(qn_entities)</span><br><span class="line">        relevant_entities &#x3D; search_index.get_candidate_docs(question, limit&#x3D;MAX_RELEVANT_ENTITIES)</span><br><span class="line">        nbr_qn_entities &#x3D; get_neighboring_entities(qn_entities, num_hops&#x3D;HOPS_FROM_QN_ENTITY)</span><br><span class="line">        candidate_entities &#x3D; union(qn_entities, relevant_entities, nbr_qn_entities)</span><br><span class="line">        # Clip candidate entities by stochastically sampling a subset</span><br><span class="line">        if len(candidate_entities) &gt; MAX_CANDIDATE_ENTITIES:</span><br><span class="line">          candidate_entities &#x3D; set(random.sample(candidate_entities, MAX_CANDIDATE_ENTITIES))</span><br><span class="line">        tuples &#x3D; get_tuples_involving_entities(candidate_entities)</span><br><span class="line">        if len(tuples) &gt; MAX_CANDIDATE_TUPLES:</span><br><span class="line">          tuples &#x3D; set(random.sample(tuples, MAX_CANDIDATE_TUPLES))</span><br><span class="line">        sources &#x3D; extract_dimension_from_tuples_as_list(tuples, 0)</span><br><span class="line">        relations &#x3D; extract_dimension_from_tuples_as_list(tuples, 1)</span><br><span class="line">        targets &#x3D; extract_dimension_from_tuples_as_list(tuples, 2)</span><br><span class="line">        output_row &#x3D; &#123;</span><br><span class="line">          &#39;question&#39;: question,</span><br><span class="line">          &#39;qn_entities&#39;: get_str_of_seq(qn_entities),</span><br><span class="line">          &#39;ans_entities&#39;: get_str_of_seq(ans_entities),</span><br><span class="line">          &#39;sources&#39;: get_str_of_seq(sources),</span><br><span class="line">          &#39;relations&#39;: get_str_of_seq(relations),</span><br><span class="line">          &#39;targets&#39;: get_str_of_seq(targets)</span><br><span class="line">        &#125;</span><br><span class="line">        writer.writerow(output_row)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">  file_path&#x3D;os.getcwd()+&quot;\data\movieqa&quot;</span><br><span class="line"></span><br><span class="line">  parser &#x3D; argparse.ArgumentParser(description&#x3D;&#39;Specify arguments&#39;)</span><br><span class="line">  parser.add_argument(&#39;--input_examples&#39;, default&#x3D;file_path+&quot;\clean_wiki-entities_qa_train.txt&quot;,help&#x3D;&#39;the raw qa pairs&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--input_graph&#39;,default&#x3D;file_path+&quot;\clean_wiki-entities_kb_graph.txt&quot;, help&#x3D;&#39;the graph file&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--input_doc&#39;, default&#x3D;file_path+&quot;\clean_wiki-entities_kb_doc.txt&quot;,help&#x3D;&#39;the doc file&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--stopwords&#39;,default&#x3D;file_path+&quot;\stopwords.txt&quot; ,help&#x3D;&#39;stopwords file&#39;, required&#x3D;False)</span><br><span class="line">  parser.add_argument(&#39;--output_examples&#39;, default&#x3D;file_path+&quot;\Mingchen2.txt&quot;,help&#x3D;&#39;the processed output file&#39;, required&#x3D;False)</span><br><span class="line">  args &#x3D; parser.parse_args()</span><br><span class="line"></span><br><span class="line">  #global variables</span><br><span class="line">  knowledge_base &#x3D; KnowledgeGraph(args.input_graph, unidirectional&#x3D;False)</span><br><span class="line">  search_index &#x3D; SearchIndex(args.input_doc, args.stopwords)</span><br><span class="line">  stop_vocab &#x3D; read_file_as_dict(args.stopwords)</span><br><span class="line">  # print(stop_vocab)</span><br><span class="line">  question_parser &#x3D; QuestionParser(knowledge_base.get_entities(), stop_vocab)</span><br><span class="line">  main(args)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Code model in 2020-5-11</title>
      <link href="2020/05/11/Code-model-in-2020-5-11/"/>
      <url>2020/05/11/Code-model-in-2020-5-11/</url>
      
        <content type="html"><![CDATA[<h3 id="clip-by-norm-in-TF"><a href="#clip-by-norm-in-TF" class="headerlink" title="clip-by-norm in TF"></a><font color=orange>clip-by-norm in TF</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">def numpy_detail():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    梯度裁剪的最直接目的就是防止梯度爆炸，手段就是控制梯度的最大范式</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    import numpy as np</span><br><span class="line">    #生成0-9之间的数组成的列表 (梯度)</span><br><span class="line">    init_t_list &#x3D; np.asarray([0,1,2,3,4])</span><br><span class="line">    #方式1：使用np自带的函数</span><br><span class="line">    l2 &#x3D; np.linalg.norm(init_t_list) #16.881943016134134</span><br><span class="line">    print(l2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    #假设裁剪规约数等于5.0</span><br><span class="line">    clip_norm &#x3D; 5.0</span><br><span class="line">    #求裁剪后的值</span><br><span class="line">    t_list &#x3D; init_t_list * clip_norm &#x2F; max(l2, clip_norm)</span><br><span class="line">    print(t_list)</span><br><span class="line">    #裁剪后L2值</span><br><span class="line">    t_list_l2 &#x3D; np.linalg.norm(t_list)</span><br><span class="line">    print(t_list_l2)</span><br><span class="line"></span><br><span class="line">def tensorflow_version():</span><br><span class="line">    import tensorflow as tf</span><br><span class="line">    # 产生一个梯度，此处与上面的保持一致为0-9组成的序列</span><br><span class="line">    clip_norm&#x3D;6</span><br><span class="line">    gradients &#x3D; [float(i) for i in range(5)]  # [0,1,2,3,4]</span><br><span class="line">    # 创建回话</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init &#x3D; tf.global_variables_initializer()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        gradients, global_norm &#x3D; tf.clip_by_global_norm(gradients, clip_norm)</span><br><span class="line">        print(sess.run(gradients))</span><br><span class="line">        print(sess.run(global_norm))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Code model in 2020-5-10</title>
      <link href="2020/05/10/Code-model-in-2020-5-10/"/>
      <url>2020/05/10/Code-model-in-2020-5-10/</url>
      
        <content type="html"><![CDATA[<h3 id="tf-reduce-sum"><a href="#tf-reduce-sum" class="headerlink" title="tf.reduce_sum()"></a><font color=orange>tf.reduce_sum()</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[[[ 1   2   3   4]</span><br><span class="line">  [ 5   6   7   8]</span><br><span class="line">  [ 9   10 11 12]],</span><br><span class="line"> [[ 13  14 15 16]</span><br><span class="line">  [ 17  18 19 20]</span><br><span class="line">  [ 21  22 23 24]]]</span><br><span class="line"></span><br><span class="line">tf.reduce_sum(tensor, axis&#x3D;0) axis&#x3D;0:</span><br><span class="line">[[1+13   2+14   3+15 4+16]</span><br><span class="line"> [5+17   6+18   7+19 8+20]</span><br><span class="line"> [9+21 10+22 11+23 12+24]]</span><br><span class="line">tf.reduce_sum(tensor, axis&#x3D;1) axis&#x3D;1:</span><br><span class="line">[[ 1 + 5 + 9   2 + 6+10   3 + 7+11   4 + 8+12]</span><br><span class="line"> [13+17+21     14+18+22   15+19+23   16+20+24]]</span><br><span class="line">tf.reduce_sum(tensor, axis&#x3D;2) axis&#x3D;2:</span><br><span class="line">[[1+2+3+4          5+6+7+8          9+10+11+12]</span><br><span class="line"> [13+14+15+16      17+18+19+20      1+22+23+24]]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning Rate Annealing</title>
      <link href="2020/05/09/Learning-Rate-Annealing/"/>
      <url>2020/05/09/Learning-Rate-Annealing/</url>
      
        <content type="html"><![CDATA[<h3 id="In-the-first-I-reprint-a-blog-to-decribe-the-learning-rate-annealing-in-the-last-there-is-a-excample-for-a-novel-method-in-MemN2N"><a href="#In-the-first-I-reprint-a-blog-to-decribe-the-learning-rate-annealing-in-the-last-there-is-a-excample-for-a-novel-method-in-MemN2N" class="headerlink" title="In the first, I reprint a blog to decribe the learning rate annealing, in the last, there is a excample for a novel method in MemN2N"></a><font color=green>In the first, I reprint a blog to decribe the learning rate annealing, in the last, there is a excample for a novel method in MemN2N</font></h3><p><a href="https://blog.csdn.net/lanmengyiyu/article/details/79341487">reprint web</a></p><p>在训练神经网络时，一般情况下学习率都会随着训练而变化，这主要是由于，在神经网络训练的后期，如果学习率过高，会造成loss的振荡，但是如果学习率减小的过快，又会造成收敛变慢的情况。因此，如何调整学习率也是一个值得讨论的问题。目前，比较常见的学习率退火方式有如下三种：</p><h4 id="随步数衰减"><a href="#随步数衰减" class="headerlink" title="随步数衰减"></a><font color=orange>随步数衰减</font></h4><p>比较常见的随步数衰减方式是每经过5个epoch学习率减小为一半，或者每经过20个epoch学习率减小为原来的十分之一。或者还有一种常见的经验做法，当发现验证集的loss停止下降的趋势时，将学习率固定的缩小一定的比例，如一半。</p><p>在实际应用中，随步数衰减的方法应用最为广泛。</p><h4 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a><font color=orange>指数衰减</font></h4><p>指数衰减可以用如下的数学公式表示, α=α0<em>e</em>*(−kt),其中，t表示迭代次数，而α0,k是超参数</p><h4 id="1-t衰减"><a href="#1-t衰减" class="headerlink" title="1/t衰减"></a><font color=orange>1/t衰减</font></h4><p>1/t衰减可以用如下的数学公式表示，α=α0/(1+kt)<br>与指数衰减的公式定义一致，t表示迭代次数，而α0,k是超参数</p><p>Method from <a href="https://github.com/domluna/memn2n">A version for MemN2N</a>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for t in range(1, FLAGS.epochs+1):</span><br><span class="line">    # Stepped learning rate</span><br><span class="line">    if t - 1 &lt;&#x3D; FLAGS.anneal_stop_epoch: #100, &quot;Epoch number to end annealed lr schedule.</span><br><span class="line">        anneal &#x3D; 2.0 ** ((t - 1) &#x2F;&#x2F; FLAGS.anneal_rate)  #25, &quot;Number of epochs between halving the learnign rate.&quot;</span><br><span class="line">    else:</span><br><span class="line">        anneal &#x3D; 2.0 ** (FLAGS.anneal_stop_epoch &#x2F;&#x2F; FLAGS.anneal_rate)</span><br><span class="line">    lr &#x3D; FLAGS.learning_rate &#x2F; anneal</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Code model in 2020-5-9</title>
      <link href="2020/05/09/Code-model-in-2020-5-9/"/>
      <url>2020/05/09/Code-model-in-2020-5-9/</url>
      
        <content type="html"><![CDATA[<h3 id="What’s-the-meaning-of-“1”-in-enumerate-seq-1"><a href="#What’s-the-meaning-of-“1”-in-enumerate-seq-1" class="headerlink" title="What’s the meaning of “1” in enumerate(seq,1)"></a><font color=orange>What’s the meaning of “1” in enumerate(seq,1)</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1: start position</span><br><span class="line"></span><br><span class="line">seq &#x3D; [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span><br><span class="line">for i, element in enumerate(seq,1):</span><br><span class="line">    print(i, element)</span><br><span class="line"></span><br><span class="line">Result:</span><br><span class="line">1 one</span><br><span class="line">2 two</span><br><span class="line">3 three</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i, element in enumerate(seq):</span><br><span class="line">    print(i, element)</span><br><span class="line"></span><br><span class="line">Result:</span><br><span class="line">0 one</span><br><span class="line">1 two</span><br><span class="line">2 three</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Padding-0-in-a-sentence"><a href="#Padding-0-in-a-sentence" class="headerlink" title="Padding 0 in a sentence"></a><font color=orange>Padding 0 in a sentence</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ss &#x3D; []</span><br><span class="line">       story&#x3D;[[&#39;mary&#39;, &#39;moved&#39;, &#39;to&#39;, &#39;the&#39;, &#39;bathroom&#39;], [&#39;john&#39;, &#39;went&#39;, &#39;to&#39;, &#39;the&#39;, &#39;hallway&#39;]]</span><br><span class="line">       for i, sentence in enumerate(story, 1):</span><br><span class="line">           ls &#x3D; max(0, sentence_size - len(sentence))</span><br><span class="line"></span><br><span class="line">           ss.append([word_idx[w] for w in sentence] + [0] * ls)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="1-and-1-in-Python"><a href="#1-and-1-in-Python" class="headerlink" title="[:-1]  and [::-1] in Python"></a><font color=orange>[:-1]  and [::-1] in Python</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">b &#x3D; a[i:j]   # 表示复制a[i]到a[j-1]，以生成新的list对象</span><br><span class="line"></span><br><span class="line">a &#x3D; [0,1,2,3,4,5,6,7,8,9]</span><br><span class="line">b &#x3D; a[1:3]   # [1,2]</span><br><span class="line"></span><br><span class="line"># 当i缺省时，默认为0，即 a[:3]相当于 a[0:3]</span><br><span class="line"># 当j缺省时，默认为len(alist), 即a[1:]相当于a[1:10]</span><br><span class="line"># 当i,j都缺省时，a[:]就相当于完整复制一份a</span><br><span class="line"></span><br><span class="line">b &#x3D; a[i:j:s]    # 表示：i,j与上面的一样，但s表示步进，缺省为1.</span><br><span class="line"># 所以a[i:j:1]相当于a[i:j]</span><br><span class="line"></span><br><span class="line"># 当s&lt;0时，i缺省时，默认为-1. j缺省时，默认为-len(a)-1</span><br><span class="line"># 所以a[::-1]相当于 a[-1:-len(a)-1:-1]，也就是从最后一个元素到第一个元素复制一遍，即倒序。</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="sklearn-model-selection–train-test-split"><a href="#sklearn-model-selection–train-test-split" class="headerlink" title="sklearn.model_selection–train_test_split()"></a><font color=orange>sklearn.model_selection–train_test_split()</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X, y &#x3D; np.arange(30).reshape((10,3)), range(10)</span><br><span class="line">X_train, X_test ,y_train, y_test&#x3D; train_test_split(X, y,test_size&#x3D;0.3, random_state &#x3D; 10, shuffle&#x3D;True) # random_state???</span><br><span class="line">print(X_train)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Novel-method-to-make-batch"><a href="#Novel-method-to-make-batch" class="headerlink" title="Novel method to make batch"></a><font color=orange>Novel method to make batch</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">n_train&#x3D;10</span><br><span class="line">batch_size&#x3D;2</span><br><span class="line">A&#x3D;range(0, n_train-batch_size, batch_size) #range(0, 8, 2)</span><br><span class="line">print(A)</span><br><span class="line">print(list(A))</span><br><span class="line">B&#x3D;range(batch_size, n_train, batch_size) # range(2, 10, 2)</span><br><span class="line">print(B)</span><br><span class="line">print(list(B))</span><br><span class="line">dic&#x3D;dict(zip(A,B))</span><br><span class="line">s&#x3D;[(start, end) for start, end in zip(A,B)]</span><br><span class="line">print(dic)</span><br><span class="line">print(s)</span><br><span class="line"></span><br><span class="line">Result:</span><br><span class="line"></span><br><span class="line">range(0, 8, 2)</span><br><span class="line">[0, 2, 4, 6]</span><br><span class="line">range(2, 10, 2)</span><br><span class="line">[2, 4, 6, 8]</span><br><span class="line">&#123;0: 2, 2: 4, 4: 6, 6: 8&#125;</span><br><span class="line">[(0, 2), (2, 4), (4, 6), (6, 8)]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="np-argmax-–reprint"><a href="#np-argmax-–reprint" class="headerlink" title="np.argmax() –reprint"></a><font color=orange>np.argmax() –reprint</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">1维：</span><br><span class="line">   import numpy as np</span><br><span class="line">a &#x3D; np.array([3, 1, 2, 4, 6, 1])</span><br><span class="line">b&#x3D;np.argmax(a)#取出a中元素最大值所对应的索引，此时最大值位6，其对应的位置索引值为4，（索引值默认从0开始）</span><br><span class="line">print(b)#</span><br><span class="line"></span><br><span class="line">2维：</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">a &#x3D; np.array([[1, 5, 5, 2],</span><br><span class="line">              [9, 6, 2, 8],</span><br><span class="line">              [3, 7, 9, 1]])</span><br><span class="line">b&#x3D;np.argmax(a, axis&#x3D;0)#对二维矩阵来讲a[0][1]会有两个索引方向，第一个方向为a[0](列)，默认按列方向搜索最大值</span><br><span class="line">#a的第一列为1，9，3,最大值为9，所在位置为1，</span><br><span class="line">#a的第一列为5，6，7,最大值为7，所在位置为2，</span><br><span class="line">#此此类推，因为a有4列，所以得到的b为1行4列，</span><br><span class="line">print(b)#[1 2 2 1]</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">3维：</span><br><span class="line">import numpy as np</span><br><span class="line">a &#x3D; np.array([</span><br><span class="line">              [</span><br><span class="line">                  [1, 5, 5, 2],</span><br><span class="line">                  [9, -6, 2, 8],</span><br><span class="line">                  [-3, 7, -9, 1]</span><br><span class="line">              ],</span><br><span class="line"> </span><br><span class="line">              [</span><br><span class="line">                  [-1, 7, -5, 2],</span><br><span class="line">                  [9, 6, 2, 8],</span><br><span class="line">                  [3, 7, 9, 1]</span><br><span class="line">              ],</span><br><span class="line">            [</span><br><span class="line">                  [21, 6, -5, 2],</span><br><span class="line">                  [9, 36, 2, 8],</span><br><span class="line">                  [3, 7, 79, 1]</span><br><span class="line">              ]</span><br><span class="line">            ])</span><br><span class="line">b&#x3D;np.argmax(a, axis&#x3D;0)#对于三维度矩阵，a有三个方向a[0][1][2]</span><br><span class="line">#当axis&#x3D;0时，是在a[0]方向上找最大值，即两个矩阵做比较，具体</span><br><span class="line">#(1)比较3个矩阵的第一行，即拿[1, 5, 5, 2],</span><br><span class="line">#                         [-1, 7, -5, 2],</span><br><span class="line">#                         [21, 6, -5, 2],</span><br><span class="line">#再比较每一列的最大值在那个矩阵中，可以看出第一列1，-2，21最大值为21，在第三个矩阵中，索引值为2</span><br><span class="line">#第2列5，7，6最大值为7，在第2个矩阵中，索引值为1.....，最终得出比较结果[2 1 0 0]</span><br><span class="line">#再拿出三个矩阵的第二行，按照上述方法，得出比较结果 [0 2 0 0]</span><br><span class="line">#一共有三个，所以最终得到的结果b就为3行4列矩阵</span><br><span class="line">print(b)</span><br><span class="line">#[[0 0 0 0]</span><br><span class="line"> #[0 1 0 0]</span><br><span class="line"> #[1 0 1 0]]</span><br><span class="line"> </span><br><span class="line">c&#x3D;np.argmax(a, axis&#x3D;1)#对于三维度矩阵，a有三个方向a[0][1][2]</span><br><span class="line">#当axis&#x3D;1时，是在a[1]方向上找最大值，即在列方向比较，此时就是指在每个矩阵内部的列方向上进行比较</span><br><span class="line">#(1)看第一个矩阵</span><br><span class="line">                  # [1, 5, 5, 2],</span><br><span class="line">                  # [9, -6, 2, 8],</span><br><span class="line">                  # [-3, 7, -9, 1]</span><br><span class="line">#比较每一列的最大值，可以看出第一列1，9，-3最大值为9，，索引值为1</span><br><span class="line">#第2列5，-6，7最大值为7，，索引值为2</span><br><span class="line"># 因此对第一个矩阵，找出索引结果为[1，2，0,1]</span><br><span class="line">#再拿出2个，按照上述方法，得出比较结果 [1 0 2 1]</span><br><span class="line">#一共有三个，所以最终得到的结果b就为3行4列矩阵</span><br><span class="line">print(c)</span><br><span class="line">#[[1 2 0 1]</span><br><span class="line"> # [1 0 2 1]</span><br><span class="line"> # [0 1 2 1]]</span><br><span class="line"> </span><br><span class="line">d&#x3D;np.argmax(a, axis&#x3D;2)#对于三维度矩阵，a有三个方向a[0][1][2]</span><br><span class="line">#当axis&#x3D;2时，是在a[2]方向上找最大值，即在行方向比较，此时就是指在每个矩阵内部的行方向上进行比较</span><br><span class="line">#(1)看第一个矩阵</span><br><span class="line">                  # [1, 5, 5, 2],</span><br><span class="line">                  # [9, -6, 2, 8],</span><br><span class="line">                  # [-3, 7, -9, 1]</span><br><span class="line">#寻找第一行的最大值，可以看出第一行[1, 5, 5, 2]最大值为5，，索引值为1</span><br><span class="line">#第2行[9, -6, 2, 8],最大值为9，，索引值为0</span><br><span class="line"># 因此对第一个矩阵，找出行最大索引结果为[1，0，1]</span><br><span class="line">#再拿出2个矩阵，按照上述方法，得出比较结果 [1 0 2 1]</span><br><span class="line">#一共有三个，所以最终得到的结果d就为3行3列矩阵</span><br><span class="line">print(d)</span><br><span class="line"># [[1 0 1]</span><br><span class="line">#  [1 0 2]</span><br><span class="line">#  [0 1 2]]</span><br><span class="line">###################################################################</span><br><span class="line">#最后一种情况，指定矩阵a[0, -1, :]，第一个数字0代表取出第一个矩阵（从前面可以看出a有3个矩阵）为</span><br><span class="line"># [1, 5, 5, 2],</span><br><span class="line"># [9, -6, 2, 8],</span><br><span class="line"># [-3, 7, -9, 1]</span><br><span class="line">#第二个数字“-1”代表拿出倒数第一行，为</span><br><span class="line"># [-3, 7, -9, 1]</span><br><span class="line">#这一行的最大索引值为1</span><br><span class="line"> </span><br><span class="line"># ，-1，代表最后一行</span><br><span class="line">m&#x3D;np.argmax(a[0, -1, :])</span><br><span class="line">print(m)#1</span><br><span class="line"> </span><br><span class="line">#h,取a的第2个矩阵</span><br><span class="line"># [-1, 7, -5, 2],</span><br><span class="line"># [9, 6, 2, 8],</span><br><span class="line"># [3, 7, 9, 1]</span><br><span class="line">#的第3行</span><br><span class="line"># [3, 7, 9, 1]</span><br><span class="line">#的最大值为9，索引为2</span><br><span class="line">h&#x3D;np.argmax(a[1, 2, :])</span><br><span class="line">print(h)#2</span><br><span class="line"> </span><br><span class="line">g&#x3D;np.argmax(a[1,:, 2])#g,取出矩阵a，第2个矩阵的第3列为-5，2，9,最大值为9，索引为2</span><br><span class="line">print(g)#2</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Codes model in 2020/5/8</title>
      <link href="2020/05/08/Codes-model-in-2020-5-8/"/>
      <url>2020/05/08/Codes-model-in-2020-5-8/</url>
      
        <content type="html"><![CDATA[<h3 id="Split"><a href="#Split" class="headerlink" title="Split"></a><font color=orange>Split</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">S &#x3D; &quot;this is  a wonderful....world!!!&quot;</span><br><span class="line">print (S.split( ))</span><br><span class="line">print (S.split(&#39;i&#39;,1))  the first position i</span><br><span class="line">print (S.split(&#39;w&#39;))</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">[&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;wonderful....world!!!&#39;]</span><br><span class="line">[&#39;th&#39;, &#39;s is  a wonderful....world!!!&#39;]</span><br><span class="line">[&#39;this is  a &#39;, &#39;onderful....&#39;, &#39;orld!!!&#39;]</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="split-‘-W-and-W"><a href="#split-‘-W-and-W" class="headerlink" title="split(‘(\W*) and \W+"></a><font color=orange>split(‘(\W*) and \W+</font></h3><pre><code>print(re.split(&#39;(\W*)&#39;,&#39;Hello，world&#39;))  # [&#39;Hello&#39;, &#39;，&#39;, &#39;world&#39;]print(re.split(&#39;\W*&#39;,&#39;Hello，world&#39;)) #[&#39;Hello&#39;, &#39;world&#39;]print(re.split(&#39;\W+&#39;,&#39;Hello，world&#39;)) # [&#39;Hello&#39;, &#39;world&#39;]print(re.split(&#39;(\W+)?&#39;,&#39;Hello，world&#39;)) # [&#39;Hello&#39;, &#39;，&#39;, &#39;world&#39;]print(re.split(&#39;(\W+)&#39;,&#39;Hello，world&#39;)) # [&#39;Hello&#39;, &#39;，&#39;, &#39;world&#39;]print(re.split(&#39;(\W+)?&#39;,&#39;Bob dropped the apple. Where is the apple?&#39;)) #[&#39;Bob&#39;, &#39; &#39;, &#39;dropped&#39;, &#39; &#39;, &#39;the&#39;, &#39; &#39;, &#39;apple&#39;, &#39;. &#39;, &#39;Where&#39;, &#39; &#39;, &#39;is&#39;, &#39; &#39;, &#39;the&#39;, &#39; &#39;, &#39;apple&#39;, &#39;?&#39;, &#39;&#39;]</code></pre><h3 id="Open-a-file-dir"><a href="#Open-a-file-dir" class="headerlink" title="Open a file dir"></a><font color=orange>Open a file dir</font></h3><pre><code>def load_task(data_dir):    files = os.listdir(data_dir)    files = [os.path.join(data_dir, f) for f in files]    # print(files)    s = &#39;qa&#123;&#125;_&#39;.format(task_id)    train_file = [f for f in files if s in f and &#39;train&#39; in f][0]    test_file = [f for f in files if s in f and &#39;test&#39; in f][0]    return train_file, test_file</code></pre><h3 id="A-method-about-‘Hello，world’-—-gt-‘Hello’-‘，’-‘world’"><a href="#A-method-about-‘Hello，world’-—-gt-‘Hello’-‘，’-‘world’" class="headerlink" title="A method about {‘Hello，world’   —-&gt;   ‘Hello’, ‘，’, ‘world’}"></a><font color=orange>A method about {‘Hello，world’   —-&gt;   ‘Hello’, ‘，’, ‘world’}</font></h3><pre><code>def tokenize(sent):    &#39;&#39;&#39;Return the tokens of a sentence including punctuation.&#39;&#39;&#39;    return [x.strip() for x in re.split(&#39;(\W+)?&#39;, sent) if x.strip()]print(&quot;token&quot;,tokenize(&#39;Hello，world&#39; ))  # token [&#39;Hello&#39;, &#39;，&#39;, &#39;world&#39;]</code></pre><h3 id="Strick-for-“for-“"><a href="#Strick-for-“for-“" class="headerlink" title="Strick for “for “"></a><font color=orange>Strick for “for “</font></h3><pre><code>a=[[1,2],[3,4],[5,6]]for i in a:    for j in i:        print(j)b=[j for i in a for j in i]Note: They have the same result</code></pre><h3 id="Tuple-Traverse"><a href="#Tuple-Traverse" class="headerlink" title="Tuple Traverse"></a><font color=orange>Tuple Traverse</font></h3><pre><code>d=[(&quot;w&quot;,&quot;4&quot;,&quot;w&quot;),(&quot;w&quot;,&quot;2&quot;,&quot;w&quot;)]print([set(s+e+w)for s,e,w in d])result:[&#123;&#39;w&#39;, &#39;4&#39;&#125;, &#123;&#39;2&#39;, &#39;w&#39;&#125;]</code></pre><h3 id="A-method-for-remove-duplication"><a href="#A-method-for-remove-duplication" class="headerlink" title="A method for remove duplication "></a><font color=orange>A method for remove duplication </font></h3><pre><code>from six.moves import reducedata=[&#123;&quot;df&quot;&#125;,&#123;&quot;g&quot;,&quot;df&quot;&#125;,&#123;&quot;g&quot;,&quot;dd&quot;,&quot;df&quot;&#125;]s=reduce(lambda x, y: x | y, data) # 去重，不排序print(sorted(s))result: [&#39;dd&#39;, &#39;df&#39;, &#39;g&#39;]</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Bug and code model in 2020/5/5</title>
      <link href="2020/05/05/Bug-and-code-model-in-2020-5-5/"/>
      <url>2020/05/05/Bug-and-code-model-in-2020-5-5/</url>
      
        <content type="html"><![CDATA[<h3 id="Word-embedding–glove"><a href="#Word-embedding–glove" class="headerlink" title="Word embedding–glove"></a><font color=orange>Word embedding–glove</font></h3><p>You can download it from <a href="https://nlp.stanford.edu/projects/glove/">stanfordNLP</a></p><p>Read this file and make the vector dic.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def make_vob_dic(dic, path):</span><br><span class="line">    vocab &#x3D; np.random.uniform(-0.1, 0.1, (len(dic), 300))</span><br><span class="line">    seen &#x3D; 0</span><br><span class="line"></span><br><span class="line">    gloves &#x3D; zipfile.ZipFile(path)</span><br><span class="line">    # print(gloves)</span><br><span class="line">    for glove in gloves.infolist():</span><br><span class="line"></span><br><span class="line">        with gloves.open(glove) as f:</span><br><span class="line">            for line in f:</span><br><span class="line">                # print(line)</span><br><span class="line">                if line !&#x3D; &quot;&quot;:</span><br><span class="line">                    splitline &#x3D; line.split()</span><br><span class="line">                    word &#x3D; splitline[0].decode(&#39;utf-8&#39;)</span><br><span class="line">                    embedding &#x3D; splitline[1:]</span><br><span class="line">                    if word in dic and len(embedding) &#x3D;&#x3D; 300:</span><br><span class="line">                        temp &#x3D; np.array([float(val) for val in embedding])</span><br><span class="line">                        vocab[dic[word], :] &#x3D; temp&#x2F;np.sqrt(np.sum(temp**2))</span><br><span class="line">                        seen +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    vocab &#x3D; vocab.astype(np.float32)</span><br><span class="line">    vocab[0, :]  &#x3D; 0.</span><br><span class="line">    print(&quot;pretrained vocab %s among %s&quot; %(seen, len(dic)))</span><br><span class="line">    return vocab</span><br></pre></td></tr></table></figure><h3 id="Making-batch-data"><a href="#Making-batch-data" class="headerlink" title="Making batch data"></a><font color=orange>Making batch data</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def create_batches(N, batch_size, skip_idx &#x3D; None, is_shuffle &#x3D; True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    :param N:   每个句子会有一个向量矩阵， (k*m), k是一个句子有几个词， m是向量维度。 N其实是有几个句子矩阵， 打乱的是句子顺序</span><br><span class="line">    :param batch_size:</span><br><span class="line">    :param skip_idx:</span><br><span class="line">    :param is_shuffle:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    batches &#x3D; []</span><br><span class="line">    shuffle_batch &#x3D; np.arange(N)</span><br><span class="line"></span><br><span class="line">    if skip_idx:</span><br><span class="line">        shuffle_batch &#x3D; list(set(shuffle_batch) - set(skip_idx))</span><br><span class="line">    if is_shuffle:</span><br><span class="line">        np.random.shuffle(shuffle_batch)</span><br><span class="line"></span><br><span class="line">    M &#x3D; int((N-1)&#x2F;batch_size + 1)  #所有的数据一共有几个batch_size</span><br><span class="line">    for i in range(M):</span><br><span class="line">        batches +&#x3D; [shuffle_batch[i*batch_size: (i+1)*batch_size]]</span><br><span class="line">        print()</span><br><span class="line">    return batches</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Bugs and code in today</title>
      <link href="2020/05/04/Bugs-and-code-in-today/"/>
      <url>2020/05/04/Bugs-and-code-in-today/</url>
      
        <content type="html"><![CDATA[<h3 id="pip"><a href="#pip" class="headerlink" title="pip"></a><font color=orange>pip</font></h3><p>When you meet: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named &#39;pip&#39;</span><br></pre></td></tr></table></figure><p>You just:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;bootstrap.pypa.io&#x2F;get-pip.py -o get-pip.py   # 下载安装脚本</span><br><span class="line">sudo python get-pip.py    # 运行安装脚本</span><br></pre></td></tr></table></figure><p>other application in pip:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">pip --version</span><br><span class="line">pip --help</span><br><span class="line">pip install -U pip \  sudo easy_install --upgrade pip</span><br><span class="line"></span><br><span class="line">安装包</span><br><span class="line">pip install SomePackage              # 最新版本</span><br><span class="line">pip install SomePackage&#x3D;&#x3D;1.0.4       # 指定版本</span><br><span class="line">pip install &#39;SomePackage&gt;&#x3D;1.0.4&#39;     # 最小版</span><br><span class="line">pip install --upgrade SomePackage #升级包</span><br><span class="line">pip uninstall SomePackage  #卸载包</span><br><span class="line">pip search SomePackage  #搜索包</span><br><span class="line">pip show  #显示安装包信息</span><br><span class="line">pip show -f SomePackage #查看指定包的详细信息</span><br><span class="line">pip list -o  #pip list -o</span><br><span class="line"></span><br><span class="line">If your device have python2 and python 3, you just:</span><br><span class="line">python2 -m pip install XXX</span><br><span class="line">python3 -m pip install XXX</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="torch-and-torchvision"><a href="#torch-and-torchvision" class="headerlink" title="torch and torchvision"></a><font color=orange>torch and torchvision</font></h3><p>my torch version is 1.2.0, when I use “pip install tochvision”, error:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Collecting torchvision</span><br><span class="line">  Using cached torchvision-0.5.0-cp36-cp36m-win_amd64.whl (1.2 MB)</span><br><span class="line">ERROR: Could not find a version that satisfies the requirement torch&#x3D;&#x3D;1.4.0 (from torchvision) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)</span><br><span class="line">ERROR: No matching distribution found for torch&#x3D;&#x3D;1.4.0 (from torchvision)</span><br></pre></td></tr></table></figure><p>So you need low your version about torchvision, as follow:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchvision&#x3D;&#x3D;0.4.1</span><br></pre></td></tr></table></figure><h3 id="Get-your-local-time"><a href="#Get-your-local-time" class="headerlink" title="Get your local time"></a><font color=orange>Get your local time</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from time import gmtime, strftime</span><br><span class="line">s&#x3D;strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())</span><br><span class="line">print(s)</span><br></pre></td></tr></table></figure><h3 id="re-sub"><a href="#re-sub" class="headerlink" title="re.sub"></a><font color=orange>re.sub</font></h3><pre><code>import reA=&quot;Hi, ____, I &#39;m so                glad to introduce, apple&#39;s&quot;line1 = re.sub(&quot;&#39;s&quot;, &quot; &#39;s&quot;, A)print(line1)line2 =  re.sub(&quot;[ ]+&quot;, &quot; &quot;, A)print(line2)line3 =  re.sub(r&#39;[A-Z]+&#39;, &#39;*&#39;, A) # 这句话则表示匹配多个连续的数字，并将多个连续的数字替换为一个星号print(line3)line4 = re.sub(&quot;\_&quot;, &quot; &quot;, A)print(line4)&quot;&quot;&quot;Hi, ____, I &#39;m so                glad to introduce, apple &#39;sHi, ____, I &#39;m so glad to introduce, apple&#39;s*i, ____, * &#39;m so                glad to introduce, apple&#39;sHi,     , I &#39;m so                glad to introduce, apple&#39;s&quot;&quot;&quot;Question: _txt = re.sub(&#39;(?&lt;=\]),&#39;, &#39;&#39;, A), ???2:import reword=&quot;has_tags..fggg&quot;s=sum([&#39;_&#39;.join(r.split(&#39;.&#39;)[-1:]).split(&#39;_&#39;) for r in re.split(&#39;\.\.&#39;, word)], [])print(s) # [&#39;has&#39;, &#39;tags&#39;, &#39;fggg&#39;]print(re.split(&#39;\.\.&#39;, word))  #[&#39;has_tags&#39;, &#39;fggg&#39;]r=&quot;has_tags&quot;print(&#39;_&#39;.join(r.split(&#39;.&#39;)[-1:])) #has_tags</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Dataset for multi-hops KBQA</title>
      <link href="2020/05/04/Dataset-for-multi-hops-KBQA/"/>
      <url>2020/05/04/Dataset-for-multi-hops-KBQA/</url>
      
        <content type="html"><![CDATA[<p>This blog introduce some multi-hops KBQA datasets, as below,</p><h3 id="MetaQA"><a href="#MetaQA" class="headerlink" title="MetaQA"></a><font color=orange>MetaQA</font></h3><p>(MoviE Text Audio QA): This dataset was introduced by <a href="https://scholar.google.com.hk/scholar?hl=en&as_sdt=0,5&q=Variational+Reasoning+for+Question+Answering+with+Knowledge+Graph&btnG=">Zhang et al. (2017)</a>. It contains more than 400K questions for both single and multi-hop reasoning (1-hop, 2hop or 3-hop reasoning), and provides more realistic text and audio versions. METAQA serves as a comprehensive extension of WikiMovies<br>It has totally there sets: Vanilla, NTM and Audio.</p><p>You can learn more about it by <a href="https://github.com/yuyuz/MetaQA">yuyuz–MetaQA</a></p><table><thead><tr><th>method</th><th>%Hits@1</th><th>F1</th></tr></thead><tbody><tr><td>IRN</td><td>17.0</td><td>8.8</td></tr><tr><td>IRN-Cons</td><td>21.8</td><td>9.2</td></tr><tr><td>MemNN</td><td>12.0</td><td>6.4</td></tr><tr><td>KVMemNN</td><td>16.6</td><td>6.5</td></tr><tr><td>Iterative Sequence Matching</td><td>98.6</td><td>98.1</td></tr><tr><td>This report from <a href="https://ieeexplore.ieee.org/document/8970943">Muli-hop: Iterative Sequence Matching</a></td><td></td><td></td></tr></tbody></table><table><thead><tr><th>method</th><th>1-hop</th><th>2-hop</th><th>3-hop</th></tr></thead><tbody><tr><td>VRN</td><td>82.0</td><td>75.6</td><td>38.3</td></tr><tr><td>IRN</td><td>9.0</td><td>8.3</td><td>31.2</td></tr><tr><td>IRN-Cons</td><td>14.6</td><td>10.7</td><td>38.2</td></tr><tr><td>MemNN</td><td>7.0</td><td>11.3</td><td>16.0</td></tr><tr><td>KVMemNN</td><td>6.2</td><td>12.6</td><td>27.9</td></tr><tr><td>Iterative Sequence Matching</td><td>96.3</td><td>99.1</td><td>99.6</td></tr><tr><td>This report from <a href="https://ieeexplore.ieee.org/document/8970943">Muli-hop: Iterative Sequence Matching</a></td><td></td><td></td><td></td></tr><tr><td>Note:  %Hits@1 performance on different questions in MetaQA.</td><td></td><td></td><td></td></tr></tbody></table><h3 id="PathQuestion"><a href="#PathQuestion" class="headerlink" title="PathQuestion"></a><font color=orange>PathQuestion</font></h3><p>This dataset is set by work <a href="https://github.com/zmtkeke/IRN">IRN</a><br>They adopted two subsets of Freebase (Bollacker et al., 2008) as Knowledge Bases to construct the PathQuestion (PQ) and the PathQuestion-Large (PQL) datasets. They extracted paths between two entities which span two hops (es → r1 → e1 → r2 → a, denoted by -2H) or three hops (es→ r1 → e1 →r2 → e2→ r3 → a, denoted by -3H) and then generated natural language questions with templates. To make the generated questions analogical to real-world questions, we included paraphrasing templates and synonyms for relations by searching the Internet and two real-world datasets, WebQuestions (Berant et al., 2013) and WikiAnswers (Fader et al., 2013). In this way, the syntactic structure and surface wording of the generated questions have been greatly enriched.</p><p><font color=red>There is much room for complex question answering. For instance, answering “How old is Obama’s younger daughter?” needs to handle arithmetic operation. Furthermore, multi-constraint questions will also be considered in this framework.</font> </p><p>You can download it by <a href="https://github.com/zmtkeke/IRN">datasource</a>, this is also the project about “An Interpretable Reasoning Network for Multi-Relation Question Answering”</p><p>The result:<br>A: <a href="https://arxiv.org/abs/1904.01246">HR-BiLSTM with UHop + DQ–2019 </a><br>B: <a href="https://www.aclweb.org/anthology/C18-1171/">IRN–2018</a><br>C: <a href="https://arxiv.org/abs/1904.01246">ABWIM with UHop + DQ–2019</a><br>D: <a href="https://ieeexplore.ieee.org/document/8970943">Muli-hop: Iterative Sequence Matching </a></p><table><thead><tr><th>method</th><th>PQ-2H</th><th>PQ-3H</th><th>PQL-2H</th><th>PQL-3H</th></tr></thead><tbody><tr><td>A:UHop1</td><td>0.960</td><td>0.877</td><td>0.725</td><td>0.710</td></tr><tr><td>C:UHop2</td><td>100</td><td>99.62</td><td>95</td><td>89.37</td></tr><tr><td>B:IRN</td><td>100</td><td>99.62</td><td>97.5</td><td>89.37</td></tr><tr><td>This report from <a href="https://arxiv.org/abs/1904.01246">U-hop</a></td><td></td><td></td><td></td><td></td></tr></tbody></table><table><thead><tr><th>method</th><th>%Hits@1</th><th>F1</th></tr></thead><tbody><tr><td>IRN</td><td>86.9</td><td>80.2</td></tr><tr><td>IRN-Cons</td><td>89.8</td><td>82.9</td></tr><tr><td>MemNN</td><td>87.1</td><td>55.6</td></tr><tr><td>KVMemNN</td><td>88.0</td><td>56.3</td></tr><tr><td>Iterative Sequence Matching</td><td>96.7</td><td>96.0</td></tr><tr><td>Note: in Iterative Sequence Matching, they did not split the hop number.</td><td></td><td></td></tr><tr><td>This report from <a href="(https://ieeexplore.ieee.org/document/8970943">Muli-hop: Iterative Sequence Matching</a></td><td></td><td></td></tr></tbody></table><h3 id="WebQSP"><a href="#WebQSP" class="headerlink" title="WebQSP"></a><font color=orange>WebQSP</font></h3><p>This data is used in <a href="https://arxiv.org/abs/1904.01246">Uhop</a><br>WebQSP is the annotated version of WebQuestions (Berant et al., 2013), which contains questions that require a 1or 2-hop relation path to arrive at the answer entity. More speciﬁcally, about 40% of the questions require a 2-hop relation to reach the answer. This dataset is based on the Freebase. </p><table><thead><tr><th>method</th><th>Accuracy</th></tr></thead><tbody><tr><td>BiCNN</td><td>77.74</td></tr><tr><td>HR-BiLSTM</td><td>82.53</td></tr><tr><td>ABWIM</td><td>83.261</td></tr><tr><td>HR-BiLSTM with UHop</td><td>82.60</td></tr><tr><td>ABWIM with UHop</td><td>82.27</td></tr></tbody></table><h3 id="WC2014-WorldCup2014"><a href="#WC2014-WorldCup2014" class="headerlink" title="WC2014/WorldCup2014"></a><font color=orange>WC2014/WorldCup2014</font></h3><p>The dataset contains single-relation questions(denoted by WC-1H),two-hop path questions(WC2H), and conjunctive questions (WC-C). WC-M is the mixture of WC-1H and WC-2H</p><p>You can download it by <a href="https://github.com/zmtkeke/IRN">datasource</a></p><table><thead><tr><th>method</th><th>%Hits@1</th><th>F1</th></tr></thead><tbody><tr><td>IRN</td><td>90.7</td><td>68.4</td></tr><tr><td>IRN-Cons</td><td>92.6</td><td>70.5</td></tr><tr><td>MemNN</td><td>90.7</td><td>46.6</td></tr><tr><td>KVMemNN</td><td>90.5</td><td>47.1</td></tr><tr><td>Iterative Sequence Matching</td><td>99.9</td><td>99.9</td></tr><tr><td>This report from <a href="https://ieeexplore.ieee.org/document/8970943">Muli-hop: Iterative Sequence Matching</a></td><td></td><td></td></tr></tbody></table><h3 id="WebQuestion"><a href="#WebQuestion" class="headerlink" title="WebQuestion"></a><font color=orange>WebQuestion</font></h3><p>In 2019, <a href="https://www.aclweb.org/anthology/N19-1301/">Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering</a> used the WebQuestion as the training data. This dataset has 84% simple question, it lacks complex multi-hop and reasoning problems. You can download it in <a href="https://www.microsoft.com/en-us/download/details.aspx?id=52763">Webquestion</a></p><table><thead><tr><th>method</th><th>Accuracy</th></tr></thead><tbody><tr><td>STOP + KVQU + SQ</td><td>54.6</td></tr></tbody></table><p>This report from <a href="https://www.aclweb.org/anthology/N19-1301/">Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering </a></p><h3 id="QALD-4"><a href="#QALD-4" class="headerlink" title="QALD-4"></a><font color=orange>QALD-4</font></h3><p>QALD-4 dataset is built upon another KB, DBpedia. Like Freebase, DBpedia stores real world facts in the triple format, making it easier for our model to adapt to this new KB. The QALD-4 dataset consists of three QA datasets, namely, Multilingual QA, Biomedical QA and Hybrid QA. Here we evaluate our model on the multilingual QA dataset which contains 250 English question-SPARQL pairs, where 200 questions (80%) are used for training and 50 questions for testing. These questions are more complicated than those of WebQuestions, e.g., all QALD-4 questions require at least  two hops of inference over the KB to answer. I found the download sit in<br><a href="https://github.com/ag-sc/QALD">ag-sc\QALD</a> or <a href="https://github.com/americast/QALD">americast\QALD</a></p><table><thead><tr><th>method</th><th>Recall</th><th>Precision</th><th>F-measure</th></tr></thead><tbody><tr><td>Xser</td><td>0.71</td><td>0.72</td><td>0.72</td></tr><tr><td>gAnswer</td><td>0.37</td><td>0.37</td><td>0.37</td></tr><tr><td>CASIA</td><td>0.40</td><td>0.32</td><td>0.36</td></tr><tr><td>Enhancing K-V MNN</td><td>0.78</td><td>0.82</td><td>0.81</td></tr></tbody></table><p>This report from <a href="https://www.aclweb.org/anthology/N19-1301/">Enhancing Key-Value Memory Neural Networks for Knowledge Based Question Answering</a></p><h3 id="FB15K-237"><a href="#FB15K-237" class="headerlink" title="FB15K-237"></a><font color=orange>FB15K-237</font></h3><p>FB15K-237: The widely used FB15k  extracted from Freebase is not adopted in our experiments, since they suﬀer from test leakage through inverse relations, i.e., many test triples can be obtained by inverting them in the training data. You can contact me to download it.</p><table><thead><tr><th>Model</th><th>FB-MRR</th><th>FB-Hits@1</th><th>FB-Hits@10</th><th>NE-MRR</th><th>NE-Hits@1</th><th>NE-Hits@10</th></tr></thead><tbody><tr><td>NeuralLP</td><td>10.2</td><td>7.0</td><td>14.8</td><td>17.9</td><td>4.8</td><td>35.1</td></tr><tr><td>NTP-λ</td><td>21.0</td><td>17.4</td><td>30.8</td><td>15.5</td><td>10.2</td><td>33.4</td></tr><tr><td>MINERVA</td><td>30.5</td><td>28.4</td><td>34.1</td><td>20.1</td><td>16.2</td><td>28.3</td></tr><tr><td>MultiHop(DistMult)</td><td>38.1</td><td>38.1</td><td>50.3</td><td>20.0</td><td>14.5</td><td>30.6</td></tr><tr><td>MultiHop(ConvE)</td><td>42.7</td><td>36.7</td><td>53.3</td><td>23.1</td><td>17.8</td><td>32.9</td></tr><tr><td>Meta-KGR(DistMult)</td><td>45.8</td><td>40.3</td><td>58.0</td><td>24.8</td><td>19.7</td><td>34.5</td></tr><tr><td>Meta-KGR(ConvE)</td><td>46.9</td><td>41.2</td><td>58.8</td><td>25.3</td><td>19.7</td><td>34</td></tr><tr><td>Note: FB-FB15K-237, NE-NELL-995</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>MultiHop from <a href="https://arxiv.org/abs/1808.10568">Multi-Hop Knowledge Graph Reasoning with Reward Shaping-2018</a>, this paper used reinforcement learning (RL) to solve multi-hop problem.</p><h3 id="FB15K-237-Path"><a href="#FB15K-237-Path" class="headerlink" title="FB15K-237-Path"></a><font color=orange>FB15K-237-Path</font></h3><p><a href="https://ieeexplore.ieee.org/document/8970943">TravNM</a> proposed  FB15K-237-Path and YAGO3-10-Path based on FB15K-237 and YAGO3-10.</p><table><thead><tr><th>method</th><th>MRR-F</th><th>Hit@10-F</th><th>MRR-Y</th><th>Hit@10-Y</th></tr></thead><tbody><tr><td>Comp.Bilinear</td><td>0.369</td><td>44.1</td><td>0.467</td><td>35.3</td></tr><tr><td>Comp.Bilinear Diag</td><td>0.329</td><td>42.8</td><td>0.463</td><td>32.0</td></tr><tr><td>Comp. Tans-E</td><td>0.331</td><td>45.7</td><td>0.604</td><td>38.8</td></tr><tr><td>Rajarshi’s model</td><td>0.394</td><td>48.2</td><td>0.618</td><td>37.1</td></tr><tr><td>TravNM-Single</td><td>0.297</td><td>38.1</td><td>0.446</td><td>35.7</td></tr><tr><td>TravNM</td><td>0.437</td><td>52.1</td><td>0.641</td><td>42.9</td></tr><tr><td>Note: This report from <a href="https://ieeexplore.ieee.org/document/8970943">TravNM</a>.</td><td></td><td></td><td></td><td></td></tr><tr><td>MRR-F refers to MRR-FB15K-237-Path, MRR-Y refers to MRR- YAGO3-10-Path.</td><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="YAGO3-10"><a href="#YAGO3-10" class="headerlink" title="YAGO3-10"></a><font color=orange>YAGO3-10</font></h3><p>It is a subset of YAGO3, a large knowledge graph extracted from several sources. Each entity in YAGO310 co-occurs with at least 10 relations, and  most triples deal with descriptive attributes of people like citizenship, gender and profession. You can contact me to download it.</p><h3 id="YAGO3-10-path"><a href="#YAGO3-10-path" class="headerlink" title="YAGO3-10-path"></a><font color=orange>YAGO3-10-path</font></h3><p>This dataset was proposed by <a href="https://ieeexplore.ieee.org/document/8970943">TravNM</a>, it based on YAGO3-10</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Tutorial for Pytorch and Tensorflow</title>
      <link href="2020/05/03/Tutorial-for-Pytorch-and-Tensorflow/"/>
      <url>2020/05/03/Tutorial-for-Pytorch-and-Tensorflow/</url>
      
        <content type="html"><![CDATA[<h3 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a><font color=orange>Pytorch</font></h3><p>There is a Chinese version for Pytorch tutorial, you can visit it in <a href="http://pytorch123.com/">web</a>, or download the pdf version from <a href="https://pan.baidu.com/s/1yK6Kjlw_yulXP8pcD6AKlQ">Baiduyun</a>, code: 9x26.</p><p>install:<br>Before using below methods, you should make sure you have installed Anaconda and know the python version.</p><p>method 1: download the install file in <a href="https://anaconda.org/pytorch/pytorch/files?page=1">pytorch / packages / pytorch</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：Download the file: pytorch-1.2.0-py3.6_cpu_1.tar.bz2.tar</span><br><span class="line">2: Decompression:  pytorch-1.2.0-py3.6_cpu_1.tar.bz2</span><br><span class="line">3: Just use comand: conda install pytorch-1.2.0-py3.6_cpu_1.tar.bz2</span><br><span class="line">4: torchvision: pip install torchvision</span><br></pre></td></tr></table></figure><p>method 2:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip3 install http:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;cpu&#x2F;torch-0.4.0-cp36-cp36m-win_amd64.whl</span><br><span class="line"></span><br><span class="line">pip3 install torchvision (torchvision is a toolkit for image progress) </span><br></pre></td></tr></table></figure><p>method 3:</p><p>In its official website <a href="https://pytorch.org/get-started/previous-versions/#linux-and-windows-3">pytorch</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">v1.1.0</span><br><span class="line">Conda</span><br><span class="line">OSX</span><br><span class="line"># conda</span><br><span class="line">conda install pytorch&#x3D;&#x3D;1.1.0 torchvision&#x3D;&#x3D;0.3.0 -c pytorch</span><br><span class="line">Linux and Windows</span><br><span class="line"># CUDA 9.0</span><br><span class="line">conda install pytorch&#x3D;&#x3D;1.1.0 torchvision&#x3D;&#x3D;0.3.0 cudatoolkit&#x3D;9.0 -c pytorch</span><br><span class="line"></span><br><span class="line"># CUDA 10.0</span><br><span class="line">conda install pytorch&#x3D;&#x3D;1.1.0 torchvision&#x3D;&#x3D;0.3.0 cudatoolkit&#x3D;10.0 -c pytorch</span><br><span class="line"></span><br><span class="line"># CPU Only</span><br><span class="line">conda install pytorch-cpu&#x3D;&#x3D;1.1.0 torchvision-cpu&#x3D;&#x3D;0.3.0 cpuonly -c pytorch</span><br><span class="line">Wheel</span><br><span class="line">OSX</span><br><span class="line">pip install torch&#x3D;&#x3D;1.1.0 torchvision&#x3D;&#x3D;0.3.0</span><br><span class="line">Linux and Windows</span><br><span class="line"># CUDA 10.0</span><br><span class="line">Download and install wheel from https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;cu100&#x2F;torch_stable.html</span><br><span class="line"></span><br><span class="line"># CUDA 9.0</span><br><span class="line">Download and install wheel from https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;cu90&#x2F;torch_stable.html</span><br><span class="line"></span><br><span class="line"># CPU only</span><br><span class="line">Download and install wheel from https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;cpu&#x2F;torch_stable.html</span><br></pre></td></tr></table></figure><p>method 4:<br>By <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/">mirrors Tinghua</a></p><h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a><font color=orange>Tensorflow</font></h3><p>&lt;TensorFlow 2.0 深度学习算法实战&gt;, Chinese version. </p><p><a href="https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book">path one</a>, this path provides all code, you can learn it more effectively.</p><p><a href="https://pan.baidu.com/s/1fUBkuNbJrIP4ivvIHMOm1A">path two–Baiduyun</a>, code: 3337</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The summary about 2019 QA</title>
      <link href="2020/04/29/The-summary-about-2019-QA/"/>
      <url>2020/04/29/The-summary-about-2019-QA/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/349499033/answer/900173774?utm_source=qq&utm_medium=social&utm_oi=982033022975160320">What’s the research direction about 2019 QA</a></p><p><a href="https://medium.com/@mgalkin/knowledge-graphs-in-natural-language-processing-acl-2019-7a14eb20fce8">Knowledge Graphs in Natural Language Processing in ACL 2019</a></p><p><a href="https://pan.baidu.com/s/1KS52CuhkPGKn9ObkS4MLaw">the development report for 2019 artificial intelligence (AI)</a>. Code: qv4f</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&mid=2247499106&idx=1&sn=b96f9af41deeb4067dec9c02dae660e8&chksm=9094f4f1a7e37de732e0f8e77d9cb9e8e4c2d57c4f536ffd021113f5e336f95dfc2292ef3ee6&mpshare=1&scene=23&srcid=&sharer_sharetime=1588073010191&sharer_shareid=3e54f7e89ddf02d1c037040e0759883b#rd">Knowledge graph in ICLR2020</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How to read data from Mysql by python</title>
      <link href="2020/04/23/How-to-read-data-from-Mysql-by-python/"/>
      <url>2020/04/23/How-to-read-data-from-Mysql-by-python/</url>
      
        <content type="html"><![CDATA[<p>Tool: Mysql: mysql-installer-community-5.7.18.1.msi, visual tool: Navicat for Mysql. There is a excample about how to use python to connect our mysql.<br>Other reference database: MonggoDB.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">def connDB():</span><br><span class="line">conn &#x3D; pymysql.connect(host&#x3D;&#39;localhost&#39;, port&#x3D;3306, user&#x3D;&#39;root&#39;, passwd&#x3D;&#39;*******&#39;, db&#x3D;&quot;****&quot;, charset&#x3D;&#39;utf8&#39;)</span><br><span class="line">cur &#x3D; conn.cursor() #db: your database name</span><br><span class="line">return conn,cur</span><br><span class="line"></span><br><span class="line">cur &#x3D; connDB()[1]</span><br><span class="line">sql &#x3D;  &quot;select id,name from  check&quot;</span><br><span class="line">s&#x3D;cur.execute(sql)</span><br><span class="line"></span><br><span class="line">table &#x3D; cur.fetchall()</span><br><span class="line">print(table)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Extract triple from wikidata</title>
      <link href="2020/04/21/Extract-triple-from-wikidata/"/>
      <url>2020/04/21/Extract-triple-from-wikidata/</url>
      
        <content type="html"><![CDATA[<p>This is a code about how to extract triple from wikidata json.<br>I also summarized some Chinese Knowledge graph:</p><p><a href="https://github.com/thunlp/OpenHowNet">OpenHownet</a></p><p><a href="https://www.youbbs.org/t/3239">DaCilin</a></p><p><a href="http://www.openkg.cn/dataset/cndbpedia">CN-DBpedia(Fudan)</a> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import json</span><br><span class="line">import datetime</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">在wikidata中提取triple, light_wiki.json 为输出的3元组文件</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def json_read():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fw&#x3D;open(&quot;light_wiki.json&quot;,&quot;w&quot;)</span><br><span class="line">    with open(&quot;latest-all.json&quot;,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as load_f:</span><br><span class="line">        i&#x3D;0</span><br><span class="line">        for line in load_f:</span><br><span class="line">            i&#x3D;i+1</span><br><span class="line">            if i %5000 &#x3D;&#x3D;0:</span><br><span class="line">                print(i)</span><br><span class="line">            if line.strip()&#x3D;&#x3D;&quot;[&quot; or line.strip()&#x3D;&#x3D;&quot;]&quot;:</span><br><span class="line">                continue</span><br><span class="line">            # line&#x3D;json.loads(line.strip().strip(&quot;,&quot;))</span><br><span class="line">            line&#x3D;(line.strip().strip(&quot;,&quot;))</span><br><span class="line">            line &#x3D; json.loads(line)</span><br><span class="line">            d&#x3D;line</span><br><span class="line">            entitiy_id&#x3D;(d[&quot;id&quot;])          #id</span><br><span class="line">            if &quot;en&quot; in d[&quot;labels&quot;]:</span><br><span class="line">                tongyi&#x3D;(d[&quot;labels&quot;][&quot;en&quot;][&quot;value&quot;])+&quot;|&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                if &quot;en&quot; in d[&quot;aliases&quot;]:</span><br><span class="line">                    # print(d[&quot;aliases&quot;][&quot;en&quot;])</span><br><span class="line">                    for word in d[&quot;aliases&quot;][&quot;en&quot;]:</span><br><span class="line">                        w&#x3D;word[&quot;value&quot;]</span><br><span class="line">                        tongyi&#x3D;tongyi+w+&quot;|&quot;</span><br><span class="line"></span><br><span class="line">                else:</span><br><span class="line">                    tongyi&#x3D;tongyi</span><br><span class="line">            else:</span><br><span class="line">                tongyi&#x3D;&quot;null&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            synonym&#x3D;tongyi.strip(&quot;|&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            description&#x3D;&quot;&quot;</span><br><span class="line">            if &quot;en&quot; in d[&quot;descriptions&quot;]:</span><br><span class="line">                description&#x3D;(d[&quot;descriptions&quot;][&quot;en&quot;][&quot;value&quot;])  #描述</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            claims&#x3D;d[&quot;claims&quot;]</span><br><span class="line"></span><br><span class="line">            rel_tail&#x3D;&quot;&quot;</span><br><span class="line">            for claim in claims:</span><br><span class="line">                relation&#x3D;claim</span><br><span class="line">                tail&#x3D;&quot;&quot;</span><br><span class="line">                if &quot;datavalue&quot; in claims[claim][0][&quot;mainsnak&quot;]:</span><br><span class="line">                    id_value&#x3D;(claims[claim][0][&quot;mainsnak&quot;][&quot;datavalue&quot;][&quot;value&quot;])  #&#123;&#39;id&#39;: &#39;Q6936113&#39;, &#39;numeric-id&#39;: 6936113, &#39;entity-type&#39;: &#39;item&#39;&#125;</span><br><span class="line"></span><br><span class="line">                    if &quot;id&quot; in id_value and type(id_value)&#x3D;&#x3D;dict:</span><br><span class="line">                        tail&#x3D;(id_value[&quot;id&quot;])</span><br><span class="line">                    else:</span><br><span class="line">                        tail&#x3D;&quot;null&quot;</span><br><span class="line"></span><br><span class="line">                relation_tail&#x3D;relation+&quot;|&quot;+tail</span><br><span class="line">                rel_tail&#x3D;rel_tail+relation_tail+&quot;,&quot;</span><br><span class="line">            # print(rel_tail)</span><br><span class="line"></span><br><span class="line">            wirite_label&#x3D;[&quot;entitiy_id&quot;,&quot;synonym&quot;,&quot;rel_tail&quot;,&quot;description&quot;]</span><br><span class="line">            all&#x3D;[]</span><br><span class="line">            all.append(entitiy_id)</span><br><span class="line">            all.append(synonym)</span><br><span class="line">            all.append(rel_tail.strip(&quot;,&quot;))</span><br><span class="line">            all.append(description)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            dic_&#x3D;dict(zip(wirite_label,all))</span><br><span class="line">            json.dump(dic_,fw)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    starttime &#x3D; datetime.datetime.now()</span><br><span class="line">    # json_read()</span><br><span class="line">    endtime &#x3D; datetime.datetime.now()</span><br><span class="line"></span><br><span class="line">    print(((endtime - starttime).seconds)&#x2F;60)  #分钟</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The latest news about EMNLP2020 and COLING2020</title>
      <link href="2020/04/15/THe-latest-news-about-EMNLP2020-and-COLING2020/"/>
      <url>2020/04/15/THe-latest-news-about-EMNLP2020-and-COLING2020/</url>
      
        <content type="html"><![CDATA[<p><a href="https://2020.emnlp.org/">EMNLP 2020</a>: Conference on Empirical Methods in Natural Language Processing</p><ul><li>Submission Date: 2020-06-01</li><li>Notification of acceptance (long &amp; short papers): 2020-09-14</li></ul><p><a href="https://coling2020.org/">COLING 2020</a>: International Conference on Computational Linguistics</p><ul><li>Final submissions due: 2020-07-01</li><li>Notifications: 2020-10-01</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Some code models in LCN data progress</title>
      <link href="2020/04/15/Some-code-models-in-LCN-data-progress/"/>
      <url>2020/04/15/Some-code-models-in-LCN-data-progress/</url>
      
        <content type="html"><![CDATA[<p>Just for my learn</p><h3 id="word-char-position"><a href="#word-char-position" class="headerlink" title="word-[char+position]"></a><font color=blue>word-[char+position]</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line"></span><br><span class="line">deal_seq &#x3D; lambda x :[[t, i, i+1] for i, t in enumerate(x)] if not len(x) &#x3D;&#x3D; 0 else [[&quot;&lt;NW_oi&gt;&quot;, 0, 4]]</span><br><span class="line">deal_char &#x3D; lambda x : deal_seq(&#39;&#39;.join(x))</span><br><span class="line"></span><br><span class="line">que&#x3D;[&#39;国家&#39;, &#39;教育委员会&#39;, &#39;全国&#39;, &#39;中小学&#39;, &#39;勤工俭学&#39;, &#39;财务&#39;]</span><br><span class="line"></span><br><span class="line">d&#x3D;(&#39;&#39;.join(que))</span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line">print(deal_seq(d))</span><br><span class="line"></span><br><span class="line">if len(d)!&#x3D;0:</span><br><span class="line">    for i, t in enumerate(d):</span><br><span class="line">        print(t,i,i+1)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">input:que&#x3D;[&#39;国家&#39;, &#39;教育委员会&#39;, &#39;全国&#39;, &#39;中小学&#39;, &#39;勤工俭学&#39;, &#39;财务&#39;]</span><br><span class="line">result: [[&#39;国&#39;, 0, 1], [&#39;家&#39;, 1, 2], [&#39;教&#39;, 2, 3], [&#39;育&#39;, 3, 4], [&#39;委&#39;, 4, 5]...........</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="python-all"><a href="#python-all" class="headerlink" title="python -all"></a><font color=blue>python -all</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def all(iterable):</span><br><span class="line">    for element in iterable:</span><br><span class="line">        if not element:</span><br><span class="line">            return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line">s&#x3D;[1,2,3,4]</span><br><span class="line"></span><br><span class="line">d&#x3D;all([s[0] &#x3D;&#x3D; 1, len(s) &#x3D;&#x3D; 4])</span><br><span class="line">print(d)</span><br><span class="line">result: False</span><br></pre></td></tr></table></figure><h3 id="python–assert"><a href="#python–assert" class="headerlink" title="python–assert"></a><font color=blue>python–assert</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;-1</span><br><span class="line">#报错</span><br><span class="line">assert not a&gt;0,&quot;a超出范围&quot;</span><br><span class="line">#正常运行</span><br><span class="line">assert a&lt;0</span><br></pre></td></tr></table></figure><h3 id="Maximum-Matching"><a href="#Maximum-Matching" class="headerlink" title="Maximum Matching"></a><font color=blue>Maximum Matching</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import jieba</span><br><span class="line">sentence&#x3D;&quot;佛罗伦萨什么时候降水比较多&quot;</span><br><span class="line">words &#x3D; &#39; &#39;.join(jieba.cut(sentence)).split(&#39; &#39;)#[&#39;佛罗伦萨&#39;, &#39;什么&#39;, &#39;时候&#39;, &#39;降水&#39;, &#39;比较&#39;, &#39;多&#39;]-</span><br><span class="line">wordBank&#x3D;[&quot;佛&quot;, &quot;佛罗伦萨&quot;]</span><br><span class="line">print(words)</span><br><span class="line">def MM(sentence, wordBank,max_n&#x3D;20):</span><br><span class="line"></span><br><span class="line">    start_loc &#x3D; 0</span><br><span class="line">    wordList &#x3D; []</span><br><span class="line"></span><br><span class="line">    while start_loc &lt; len(sentence):</span><br><span class="line">        mark_have &#x3D; False</span><br><span class="line">        for end_loc in range(min(start_loc + max_n, len(sentence)), start_loc + 1, -1):</span><br><span class="line">            word_t &#x3D; sentence[start_loc:end_loc]</span><br><span class="line">            if word_t in wordBank:</span><br><span class="line">                wordList.append(word_t)</span><br><span class="line">                start_loc &#x3D; end_loc</span><br><span class="line">                mark_have &#x3D; True</span><br><span class="line">                break</span><br><span class="line">        if not mark_have:</span><br><span class="line">            wordList.append(sentence[start_loc])</span><br><span class="line">            start_loc +&#x3D; 1</span><br><span class="line">    wordList &#x3D; [word for word in wordList if not word &#x3D;&#x3D; &#39; &#39;]</span><br><span class="line">    return wordList</span><br><span class="line"></span><br><span class="line">result&#x3D;[]</span><br><span class="line">for word in words:</span><br><span class="line">    # print(word)</span><br><span class="line">    result+&#x3D;MM(word,wordBank)</span><br><span class="line">print(result)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">result: [&#39;佛罗伦萨&#39;, &#39;什&#39;, &#39;么&#39;, &#39;时&#39;, &#39;候&#39;, &#39;降&#39;, &#39;水&#39;, &#39;比&#39;, &#39;较&#39;, &#39;多&#39;], 保留在词典里的词，剩余的为单个char</span><br><span class="line">先是结巴分词，再是对分完的每一个词进行最大匹配处理，再通过百度词库进行筛选，不明白为何这么做，直接遍历匹配不行吗</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><h3 id="MWS"><a href="#MWS" class="headerlink" title="MWS"></a><font color=blue>MWS</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line"># 在这个工程中不再需要结巴分词</span><br><span class="line">S&#x3D;&quot;佛罗伦萨什么时候降水比较多&quot;</span><br><span class="line">wordBank&#x3D;[&quot;佛&quot;,&quot;罗伦萨&quot;,&quot;什么&quot;]</span><br><span class="line"></span><br><span class="line">def MWS(sentence, wordBank, index&#x3D;True, max_len&#x3D;20):</span><br><span class="line">    assert not wordBank is None</span><br><span class="line">    wordList &#x3D; []</span><br><span class="line">    for i in range(len(sentence)):</span><br><span class="line">        for j in range(i + 1, len(sentence) + 1):</span><br><span class="line">            if j - i &gt; max_len:</span><br><span class="line">                break</span><br><span class="line">            word_t &#x3D; sentence[i:j]</span><br><span class="line">            if word_t in wordBank:</span><br><span class="line">                if index:</span><br><span class="line">                    wordList.append([word_t, i, j])</span><br><span class="line">                else:</span><br><span class="line">                    wordList.append(word_t)</span><br><span class="line">    return wordList</span><br><span class="line"></span><br><span class="line">D&#x3D;MWS(S,wordBank)</span><br><span class="line">print(D)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">result: [[&#39;佛&#39;, 0, 1], [&#39;罗伦萨&#39;, 1, 4], [&#39;什么&#39;, 4, 6]]</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><h3 id="python-range"><a href="#python-range" class="headerlink" title="python-range"></a><font color=blue>python-range</font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">range(start,stop, scan)</span><br><span class="line">scan：每次跳跃的间距(步长参数)，默认为1。例如：range（0， 5）</span><br><span class="line">等价于 range(0, 5, 1)，步长值为正时表示从左向右取，如果为负，则表示从右向左取。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># print(list(range(0,-30,-1)))</span><br><span class="line">#</span><br><span class="line"># print(list(range(5)))</span><br><span class="line"></span><br><span class="line">for i in range(10,1,-1):</span><br><span class="line">    print(i) #[10,9,8,7,6,5,4,3,2,1]</span><br><span class="line">for i in range(1,10,-1):</span><br><span class="line">    print(i)#i   []</span><br><span class="line"></span><br><span class="line">for i in range(1, 10, 1):</span><br><span class="line">    print(i)  # i</span><br></pre></td></tr></table></figure><h3 id="python–-tqdm"><a href="#python–-tqdm" class="headerlink" title="python– tqdm "></a><font color=blue>python– tqdm </font></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">from tqdm._tqdm import trange</span><br><span class="line"></span><br><span class="line">for i in tqdm(range(100)):</span><br><span class="line">    time.sleep(0.01)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">result:</span><br><span class="line">100%|██████████| 100&#x2F;100 [00:01&lt;00:00, 92.75it&#x2F;s]</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>my own framework for NLP</title>
      <link href="2020/04/05/my-own-framework-for-NLP/"/>
      <url>2020/04/05/my-own-framework-for-NLP/</url>
      
        <content type="html"><![CDATA[<p>I think it’s time to summary some tasks about NLP, some essential models, some tips according my experience. My previous research mainly focused on text classification, even though I need to learn more about it. In this section, I will introduce the four main tasks about NLP in the first, these tasks are also the framework about this filed, </p><p><font color=green> Four tasks, so please choose a subtask you like</font></p><p>There are four tasks we need to learn as a starter about NLP <a href="https://github.com/ToneLi/Some-charts-about-my-research/blob/master/NLP-task.png">NLP-framwork</a>. Of course there are other interesting tasks in here, such as word embedding (the input layer about model), entity linking, entity embedding (about knowledge graph). </p><p><font color=green>Yes, I already have a research direction (coarse field), what should I do in the next?</font></p><ul><li>I think the text classification task is the best choise for NLP beginner, because it’s easier than other tasks. Before you want to learn the latest model, you should learn the grounding model about this task, because if you run these basics models, you will understand how to build a deep learning model, how to progress data, model is model, in the other word, the improvement of the model is based on the work of the predecessor. We should learn how to work before we want to run. There is a tutorial about “Language identiﬁcation”, if you are interested  in it, you can learn it in detail <a href="https://github.com/ToneLi/Some-methods-for-LangID">LanID_tutorial</a></li></ul><ul><li>If you like the machine translation, I think you should learn the basics model–Seq2Seq. </li></ul><ul><li>If you want to learn the name entity recognition, you should learn how to use RNN to solve this task, or apply RNN into word sense disambiguation.</li></ul><p><font color=green> Reproducing paper is better than reading paper all the time, so just do it!</font> </p><p>Before you want to reproduce a work (paper), you should learn some basic skills about some open DL frameworks, such as Tensorflow, Pytorch, etc. You can refer other researchers’ work in github, and learn this code until you can handle it, you can progress other data. Such as <a href="https://github.com/galsang/ABCNN">ABCNN:Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</a></p><p><font color=green>In the last</font><br>I sincerely hope these advises can help you, even though it’s just represent my own opinion. I know research is a tough progress, but if you want to get the further development, get into the famous company or university, get a perfect research platform, we need change little by little. In the futher, I may focus on text matching, keep learning about it. If you are interested in this task, we can do it together. <a href="https://github.com/ToneLi/Typical-models-for-Text-Matching">text matching</a>. </p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The summary about special semantic unit in NLP</title>
      <link href="2020/03/31/The-summary-about-special-semantic-unit-in-NLP/"/>
      <url>2020/03/31/The-summary-about-special-semantic-unit-in-NLP/</url>
      
        <content type="html"><![CDATA[<p>We all know that the word and sentence of Chinese have its special structure. If there is a English sentence, we can segment the word by the space, and then regard the word as a semantic unit. But if there is a Chinese sentence, we will may find it’s difficult to segment the word, even though there are many word segmentation tools, such as jieba, THULAC, etc. For instance, <font color=Blue>“我在石膏村钢铁厂工作”</font>, after segmenting by these tools, it may be <font color=Blue>“我在,石膏村,钢铁厂,工作”</font>. As a Chinese, we all know <font color=Blue>石膏村钢铁厂</font> is a word. To avoid this issues, some researchers decided to do this <font color=Blue>“我,在,石,膏,村,钢,铁,厂,工,作”</font>, every character as a semantic unit. Of course, there must be other progress methods, I want to summary it as below, it’s so interesting! </p><h2 id="Lattice"><a href="#Lattice" class="headerlink" title="Lattice"></a><font color=green>Lattice</font></h2><h3 id="Lattice-CNNs-for-Matching-Based-Chinese-Question-Answering–2019"><a href="#Lattice-CNNs-for-Matching-Based-Chinese-Question-Answering–2019" class="headerlink" title="Lattice CNNs for Matching Based Chinese Question Answering–2019"></a><a href="https://arxiv.org/abs/1902.09087">Lattice CNNs for Matching Based Chinese Question Answering–2019</a></h3><p>For a sentence in Chinese, which is a sequence of Chinese characters, all of its possible substrings that can be considered as words. Then, through an existing lookup vocabulary, which contains frequent words in BaiduBaike to filter the word.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sentence &quot;中国人民生活品质高&quot;</span><br><span class="line">------&gt;: 中--国--人--民--生--活--品--质--高 </span><br><span class="line">------&gt;: 中国人--民--生--活--品--质--高</span><br><span class="line">------&gt;: 中--国人--民生--活--品--质--高  </span><br><span class="line">------&gt;: 中国人--民生--活--品--质--高 </span><br></pre></td></tr></table></figure><h2 id="Ideograph-and-stroke"><a href="#Ideograph-and-stroke" class="headerlink" title="Ideograph and stroke "></a><font color=green>Ideograph and stroke </font></h2><h3 id="Chinese–Japanese-Unsupervised-Neural-Machine-Translation-Using-Sub-character-Level-Information–2019"><a href="#Chinese–Japanese-Unsupervised-Neural-Machine-Translation-Using-Sub-character-Level-Information–2019" class="headerlink" title="Chinese–Japanese Unsupervised Neural Machine Translation Using Sub-character Level Information–2019"></a><a href="https://arxiv.org/abs/1903.00149">Chinese–Japanese Unsupervised Neural Machine Translation Using Sub-character Level Information–2019</a></h3><p>The English language is called “alphabetic languages” (字母语言)</p><p>The Chinese language is called “logographic language” (象形文字语言)</p><p>Ideograph (象形文字) and stroke (笔画)</p><p>The reason about why use it?</p><p>(1) This alleviates the risk of producing<unk>symbols when the model encounters infrequent “unknown” words, also known as the out-of-vocabulary (OOV) problem.</p><p>(2) it is possible to build a significantly smaller vocabulary to cover a large amount of training data. This has been researched quite extensively within tasks such as word embeddings, as mentioned previously.</p><p>(3) Note that the ideograph and stroke sequences have higher proportion of shared parts than words, which are very useful for byte pair encoding (BPE) algorithm and shared vocabulary in machine translation system</p><h4 id="Sub-character-Units-in-NLP"><a href="#Sub-character-Units-in-NLP" class="headerlink" title="Sub-character Units in NLP"></a>Sub-character Units in NLP</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Character  Semantic ideograph  Phonetic ideograph  Pinyin</span><br><span class="line">驰 run          马 horse            也                chí </span><br><span class="line">池 pool         水(氵) water        也                chí </span><br><span class="line">施 impose        方 direction       也                sh </span><br></pre></td></tr></table></figure><h4 id="Stroke-Information-in-NLP"><a href="#Stroke-Information-in-NLP" class="headerlink" title="Stroke Information in NLP"></a>Stroke Information in NLP</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CN-character      风 景 </span><br><span class="line">CN-ideograph      几㐅 日亠口小_1</span><br><span class="line">CN-stroke          丿㇈㇒㇔ ㇑㇕㇐㇐㇔㇐㇑㇆㇐㇚㇒㇔_1</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Pinyin"><a href="#Pinyin" class="headerlink" title="Pinyin"></a><font color=green>Pinyin</font></h2><h3 id="Lattice-Based-Transformer-Encoder-for-Neural-Machine-Translation–2019"><a href="#Lattice-Based-Transformer-Encoder-for-Neural-Machine-Translation–2019" class="headerlink" title="Lattice-Based Transformer Encoder for Neural Machine Translation–2019"></a><a href="https://arxiv.org/abs/1906.01282v1">Lattice-Based Transformer Encoder for Neural Machine Translation–2019</a></h3><p>just Pinyin, I think its idea is same as n-gram, just changed Chinese word to Pinyin representation. The authors introduce the position embedding to their model (like transformer). Please see the detail in this paper.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">贸易发展局 副总裁</span><br><span class="line">mao----yi----fa----zhan----ju----fu----zong----cai</span><br><span class="line">c1-----c2----c3----c4------c5-----c6----c7-----c8</span><br><span class="line">e(0:2)--&gt;maoyi</span><br><span class="line">e(2:4)--&gt;fazhan</span><br></pre></td></tr></table></figure><h3 id="Pinyin-as-Subword-Unit-for-Chinese-Sourced-Neural-Machine-Translation–2017"><a href="#Pinyin-as-Subword-Unit-for-Chinese-Sourced-Neural-Machine-Translation–2017" class="headerlink" title="Pinyin as Subword Unit for Chinese-Sourced Neural Machine Translation–2017"></a><a href="http://doras.dcu.ie/23197/1/Pinyin%20as%20Subword%20Unit%20for%20Chinese-Sourced%20Neural%20Machine%20Translation.pdf">Pinyin as Subword Unit for Chinese-Sourced Neural Machine Translation–2017</a></h3><p>This paper from Andy (Adapt Center-Irish), his team used Pinyin and the tone about Chinese character to parse the Chinese word. The excample as below:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Chinese: 在本届世足赛大放异</span><br><span class="line">English: dazzles at the world cup </span><br><span class="line">factored NMT: 在|zai 本|ben 届|jie 世足赛|shizusai 大放异彩|dafangyicai </span><br><span class="line">WdPy(BPE) zai ben jie shizusai dafangyicai </span><br><span class="line">factored WdPy: zai|4 ben|3 jie|4 shizusai|4-2-4 dafangyicai|4-4-4-3 </span><br></pre></td></tr></table></figure><p>They set up four different conﬁgurations: ChPy, ChPyT, WdPy and WdPyT.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Chinese KBQA</title>
      <link href="2020/03/30/Chinese-KBQA/"/>
      <url>2020/03/30/Chinese-KBQA/</url>
      
        <content type="html"><![CDATA[<p><font color=blue size=3>Data:</font></p><p>From NLPCC 2016 and NLPCC 2017, the data of NLPCC2017 includes the data of NLPCC2016. You can download the KBQA data from  <a href="http://tcci.ccf.org.cn/conference/2017/taskdata.php">NLPCC 2017</a> Shared Task Sample Data—&gt;task 5 [Open Domain Question Answering]</p><p><font color=blue size=3>Paper reference:</font></p><p><a href="https://arxiv.org/abs/1902.09087">Yuxuan Lai, etc. Lattice CNNs for Matching Based Chinese Question Answering, AAAI2019</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Release the LiCHD-1(Chinese medical QA dataset)</title>
      <link href="2020/03/29/Realse-the-LiCHD-1-Chinese-medical-QA-dataset/"/>
      <url>2020/03/29/Realse-the-LiCHD-1-Chinese-medical-QA-dataset/</url>
      
        <content type="html"><![CDATA[<p>As there are few clean and public Chinese Health question answer datasets, we developed a dataset <a href="https://github.com/ToneLi/A-novel-Chinese-medical-QA-dataset-LiCHD-1">LiCHD-1</a> which is used to verify the new QA model.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CCKS2020</title>
      <link href="2020/03/28/CCKS2020/"/>
      <url>2020/03/28/CCKS2020/</url>
      
        <content type="html"><![CDATA[<p>CCKS (China Conference on Knowledge Graph and Semantic Computing) will be held in Nanchang (8/15/2020-8/18/2020). Its topic is “Knowledge Graph and  Cognitive Intelligence”<a href="http://sigkg.cn/ccks2020">Window</a>.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>My new project about LangID have uploaded in my github</title>
      <link href="2020/02/06/My-new-project-about-LangID-have-uploaded-in-my-github/"/>
      <url>2020/02/06/My-new-project-about-LangID-have-uploaded-in-my-github/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/ToneLi/Some-methods-for-LangID">In there</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Language Identification database</title>
      <link href="2020/01/31/Language-Identification-database/"/>
      <url>2020/01/31/Language-Identification-database/</url>
      
        <content type="html"><![CDATA[<pre><code>Task Description:We would also like to ask you to implement a method to identify the language a document is written in. Please do not use an existing library to solve the task, but implement an approach on your own. If you re-implement an approach taken from the literature, please provide a reference. The suggested programming language is Python or Java.</code></pre><p>This is a sub-task about NLP: Language Identification. It is the same as the text classification. the data sample is as below:</p><pre><code>115500 tenten en 77000 dekzeilen uitgedeeld in pakistan 153800 mensen zijn daarmee geholpen,    nl11e foro de biarritz consacré aux relations entre l europe et lamérique latine,                    fr12e forum francoallemand sur les formations conjointes dexcellence strasbourg,                    fr1313 zur halbzeit hier geht es weiter mit der 2halbzeit handball livestream zdf esp ger dhb,    de</code></pre><p>In the next, I will summarize some dataset and source link, </p><p>1: <a href="https://github.com/saffsd/langid.py">langid.py</a></p><p>2: <a href="https://zenodo.org/record/841984#collapseCitations">WiLI-2018 - Wikipedia Language Identification database</a></p><p>3: <a href="https://blog.csdn.net/tcjy1000/article/details/48242409">ISO 639-1: symbols about languages</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>padded_batch in tensorflow</title>
      <link href="2020/01/06/padded-batch-in-tensorflow/"/>
      <url>2020/01/06/padded-batch-in-tensorflow/</url>
      
        <content type="html"><![CDATA[<p>In tensorflow, padded_batch is a good function to fill the<br>sentence with “0”, such as:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[1, 2, 3, 4],                   </span><br><span class="line">[1, 2, 3],</span><br><span class="line">[1, 2, 3],           </span><br><span class="line">[1, 2, 3, 4]</span><br><span class="line"></span><br><span class="line">to---&gt;</span><br><span class="line"></span><br><span class="line">[1, 2, 3, 4, 0],</span><br><span class="line">[1, 2, 3, 0, 0],</span><br><span class="line">[1, 2, 3, 0, 0],</span><br><span class="line">[1, 2, 3, 4, 0]</span><br></pre></td></tr></table></figure><p>I will give an excample:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def gen():</span><br><span class="line">    yield (np.array([5,6,7]), np.array(1))</span><br><span class="line">    yield (np.array([8,4,3,2,6,7]), np.array(0))</span><br><span class="line"></span><br><span class="line">data &#x3D; tf.data.Dataset.from_generator(gen, output_types&#x3D;(tf.int32, tf.int32))</span><br><span class="line">data &#x3D; data.apply(tf.contrib.data.shuffle_and_repeat(buffer_size&#x3D;2))</span><br><span class="line">data &#x3D; data.padded_batch(10, padded_shapes&#x3D;([5], []))#data.padded_batch(batch_size, padded_shapes&#x3D;max_shape),其中max_shape是您想要的填充张量的大小。</span><br><span class="line"></span><br><span class="line">iterator &#x3D; tf.data.Iterator.from_structure(data.output_types, data.output_shapes)</span><br><span class="line">batch &#x3D; iterator.get_next()</span><br><span class="line">init_op &#x3D; iterator.make_initializer(data)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    batch_out &#x3D; sess.run(batch)</span><br><span class="line">    print(batch_out)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">使用dataset中的padded_batch方法来进行，参数padded_shapes #指明每条记录中各成员要pad成的形状，成员若是scalar，则用[]</span><br><span class="line">，若是list，则用[mx_length]，若是array，则用[d1,...,dn]，若是string 则用 (),</span><br><span class="line">假如各成员的顺序是scalar数据、list数据、array数据，string 数据， 则padded_shapes&#x3D;([], [mx_length], [d1,...,dn]，())；该方法的函数说明如下：</span><br><span class="line"></span><br><span class="line">padded_batch(</span><br><span class="line">    batch_size,</span><br><span class="line">    padded_shapes,</span><br><span class="line">    padding_values&#x3D;None    #默认使用各类型数据的默认值，一般使用时可忽略该项</span><br><span class="line">再如[None]  为默认填充为到所有句子的最大长度</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow with Transformer in MT</title>
      <link href="2020/01/06/Tensorflow-with-Transformer-in-MT/"/>
      <url>2020/01/06/Tensorflow-with-Transformer-in-MT/</url>
      
        <content type="html"><![CDATA[<p>1:<a href="https://spaces.ac.cn/archives/4765/comment-page-1">The simplistic reading of Attention is all you need</a></p><p>2:<a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p><p>3: <a href="https://github.com/Kyubyong/transformer">Kyubyong/transformer</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The Illustrated Transformer for Machine translation (reprint)</title>
      <link href="2020/01/05/The-Illustrated-Transformer-for-Machine-translation-reprint/"/>
      <url>2020/01/05/The-Illustrated-Transformer-for-Machine-translation-reprint/</url>
      
        <content type="html"><![CDATA[<p><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p><p><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271">Chinese version</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How to change a sentence into lattice sentence</title>
      <link href="2020/01/03/How-to-change-a-sentence-into-lattice-sentence/"/>
      <url>2020/01/03/How-to-change-a-sentence-into-lattice-sentence/</url>
      
        <content type="html"><![CDATA[<pre><code>This is a question, we don&#39;t want to use jieba or other word segmentation tools to clear the boundary of word. We have a dict, such as &#123;小明，中国人，中国， 人&#125;，sentence &#123;小明是中国人&#125;. I want to get the lattice sentence: &#123;小明，中国，中国人，人&#125;. What should I do? I use the same method in Chinese medical QA. I will give my code in the next.</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def baidu_dic():</span><br><span class="line">    words&#x3D;[]</span><br><span class="line">    with open(&quot;baike_dict.txt&quot;, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line &#x3D; line.strip().split(&quot;\t&quot;)</span><br><span class="line">            words.append(line[0])</span><br><span class="line">    ids&#x3D;list(range(len(words)))</span><br><span class="line"></span><br><span class="line">    dic_&#x3D;dict(zip(words,ids))</span><br><span class="line">    return dic_</span><br><span class="line"></span><br><span class="line">def _word_ngrams(tokens, baidubaike_dic,stop_words&#x3D;None, ngram_range&#x3D;(1, 1)):</span><br><span class="line">    tokens &#x3D; [char for char in tokens]</span><br><span class="line">    &quot;&quot;&quot;Turn tokens into a sequence of n-grams after stop words filtering&quot;&quot;&quot;</span><br><span class="line">    # handle stop words</span><br><span class="line">    if stop_words is not None:</span><br><span class="line">        tokens &#x3D; [w for w in tokens if w not in stop_words]</span><br><span class="line">    # handle token n-grams</span><br><span class="line">    min_n, max_n &#x3D; ngram_range</span><br><span class="line">    if max_n !&#x3D; 1:</span><br><span class="line">        original_tokens &#x3D; tokens</span><br><span class="line">        sen &#x3D; []</span><br><span class="line">        n_original_tokens &#x3D; len(original_tokens)</span><br><span class="line">        for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):</span><br><span class="line">            for i in range(n_original_tokens - n + 1):</span><br><span class="line">                word&#x3D;&quot;&quot;.join(original_tokens[i: i + n])</span><br><span class="line">                if word in baidubaike_dic:</span><br><span class="line">                    sen.append(word)</span><br><span class="line">        return sen</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def make_lattice(sentence,baidubaike_dic):</span><br><span class="line">    sentence_lattice &#x3D; []</span><br><span class="line">    sentence_length &#x3D; len(sentence)</span><br><span class="line">    if sentence_length &gt;&#x3D; 10:</span><br><span class="line">        for j in range(1, 11):</span><br><span class="line">            lattice_sen &#x3D; _word_ngrams(sentence, baidubaike_dic, ngram_range&#x3D;(j, j))</span><br><span class="line"></span><br><span class="line">            if lattice_sen !&#x3D; None:</span><br><span class="line">                if len(lattice_sen) &gt; 0:</span><br><span class="line">                    sentence_lattice &#x3D; sentence_lattice + list(set(lattice_sen))</span><br><span class="line">    else:</span><br><span class="line">        for j in range(1, sentence_length):</span><br><span class="line">            lattice_sen &#x3D; _word_ngrams(sentence, baidubaike_dic, ngram_range&#x3D;(j, j))</span><br><span class="line"></span><br><span class="line">            if lattice_sen !&#x3D; None:</span><br><span class="line">                if len(lattice_sen) &gt; 0:</span><br><span class="line">                    sentence_lattice &#x3D; sentence_lattice + list(set(lattice_sen))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sentence_la &#x3D; &quot;&quot;</span><br><span class="line">    for word in sentence_lattice:</span><br><span class="line">        sentence_la &#x3D; sentence_la + word + &quot; &quot;</span><br><span class="line"></span><br><span class="line">    return sentence_la</span><br><span class="line"></span><br><span class="line">def sentence_to_lattice(input,output):</span><br><span class="line"></span><br><span class="line">    fw&#x3D;open(output,&quot;w&quot;,encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">    baidubaike_dic&#x3D;baidu_dic()</span><br><span class="line">    i&#x3D;0</span><br><span class="line">    with open(input,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            i&#x3D;i+1</span><br><span class="line">            print(i)</span><br><span class="line">            lines&#x3D;line.strip().split(&quot;\t&quot;)</span><br><span class="line"></span><br><span class="line">            question&#x3D;lines[0]</span><br><span class="line">            answer&#x3D;lines[1]</span><br><span class="line">            answer_sentence&#x3D;answer.strip().split(&quot;。&quot;)[0:-1]</span><br><span class="line">            question_lattice&#x3D; make_lattice(question, baidubaike_dic)</span><br><span class="line">            if len(question_lattice)&gt;0:</span><br><span class="line">                fw.write(question_lattice+&quot;\t&quot;)</span><br><span class="line">            else:</span><br><span class="line">                fw.write(&quot;None&quot; + &quot;\t&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            answer_lattice&#x3D;&quot;&quot;</span><br><span class="line">            for sentence in answer_sentence:</span><br><span class="line">                str_&#x3D;make_lattice(sentence,baidubaike_dic)</span><br><span class="line">                answer_lattice&#x3D;answer_lattice+str_+&quot;。&quot;</span><br><span class="line">            answer_lattice&#x3D;answer_lattice.strip(&quot;。&quot;)</span><br><span class="line">            if len(answer_lattice)&gt;0:</span><br><span class="line">                fw.write(answer_lattice+&quot;\t&quot;)</span><br><span class="line">            else:</span><br><span class="line">                fw.write(&quot;None&quot; + &quot;\t&quot;)</span><br><span class="line">            fw.write(str(lines[2])+&quot;\n&quot;)</span><br><span class="line">            fw.flush()</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    input&#x3D;&quot;LiCHD_train.txt&quot;</span><br><span class="line">    output&#x3D;&quot;LiCHD_train_lattice.txt&quot;</span><br><span class="line">    sentence_to_lattice(input,output)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>add error in tensorflow</title>
      <link href="2020/01/02/add-error-in-tensorflow/"/>
      <url>2020/01/02/add-error-in-tensorflow/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">If we don&#39;t input the data to model, just run the model.</span><br><span class="line"></span><br><span class="line">a:Tensor(&quot;truediv_29:0&quot;, shape&#x3D;(?,), dtype&#x3D;float32)</span><br><span class="line">b:Tensor(&quot;interaction_literal_all_pooling10-all_pool10&#x2F;Reshape:0&quot;, shape&#x3D;(?, 50), dtype&#x3D;float32)</span><br><span class="line"></span><br><span class="line">a+b: Tensor(&quot;add_29:0&quot;, shape&#x3D;(?, 50), dtype&#x3D;float32)</span><br><span class="line"></span><br><span class="line">This operation is wrong, if we input the data to model.</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Update my work for short text classification</title>
      <link href="2020/01/02/Update-my-work-for-short-text-classification/"/>
      <url>2020/01/02/Update-my-work-for-short-text-classification/</url>
      
        <content type="html"><![CDATA[<pre><code>I added two contract experiments (BERT-FiT and LUMFiT for classification) in my work according to the review in ECAI2020. Although I dislike the first reviewer, I think his point is right. So, I did it. Otherwise, I used the pre-training word embedding was computed by BERT instead of word2vec in my model to see whether the BERT embedding can improve the accuracy. If you are interest in my work, please contact with me.</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Meet the bug in Chinese medical QA</title>
      <link href="2020/01/01/Meet-the-bug-in-Chinese-medical-QA/"/>
      <url>2020/01/01/Meet-the-bug-in-Chinese-medical-QA/</url>
      
        <content type="html"><![CDATA[<p>Yes, I spent two days to debug this error for my model, I always think “I can debug all errors (that is a bit of an exaggeration–naughty)”. </p><p><font color=red size=3>Error:</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;D:\Li\bao\anaconda\setup\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1278, in _do_call</span><br><span class="line">    return fn(*args)</span><br><span class="line">  File &quot;D:\Li\bao\anaconda\setup\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1263, in _run_fn</span><br><span class="line">    options, feed_dict, fetch_list, target_list, run_metadata)</span><br><span class="line">  File &quot;D:\Li\bao\anaconda\setup\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1350, in _call_tf_sessionrun</span><br><span class="line">    run_metadata)</span><br><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] &#x3D; [10,2] vs. shape[1] &#x3D; [2,10]</span><br><span class="line"> [[Node: sim_features &#x3D; ConcatV2[N&#x3D;2, T&#x3D;DT_FLOAT, Tidx&#x3D;DT_INT32, _device&#x3D;&quot;&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;device:CPU:0&quot;](sim_features&#x2F;values_0, stack, gradients&#x2F;left1-w_pool1&#x2F;Sum_grad&#x2F;Fill)]]</span><br><span class="line"></span><br><span class="line">During handling of the above exception, another exception occurred:</span><br></pre></td></tr></table></figure><p><font color=red size=3>Reason and Solution:</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">I used this </span><br><span class="line">(self.output_features &#x3D; tf.concat([self.features, tf.stack(sims, axis&#x3D;1)], axis&#x3D;1, name&#x3D;&quot;output_features&quot;))</span><br><span class="line">to compute the features of output, its shape&#x3D;(10, ?).</span><br><span class="line">This code segement leads to the error. </span><br><span class="line">So I rewrited this function: self.sim_s &#x3D; (tf.stack(sims, axis&#x3D;1)). Its shape&#x3D;(?,10). pleasantly surprised！！ It&#39;s not OK.</span><br></pre></td></tr></table></figure><p><font color=red size=3>Another error:</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   raise type(e)(node_def, op, message)</span><br><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: Matrix size-incompatible: In[0]: [2,10], In[1]: [2,10]</span><br><span class="line"> [[node self-attention&#x2F;MatMul_2 (defined at G:\工程\问答\医疗\2020_SIGIR_experiment\1_My_five_model\2_HWLBS_4\HWLBS.py:212) ]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><font color=red size=3>Reason and Solution:</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wrong: out_feature&#x3D;(tf.matmul(self.sim_features,weights))  weights:shape&#x3D;(?, 100), self.sim_features: shape&#x3D;(?, 100).</span><br><span class="line">right:   out_feature&#x3D; tf.multiply(self.sim_features,weights)</span><br><span class="line"></span><br><span class="line">tf.matmul is the matrix multiplication, but the &quot;tf.multiply&quot; is the multiplication between corresponding elements in matrix.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><font color=red size=3>Another error and solution:</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">At first, my model runs one step needs spend more than three hours in my computer or </span><br><span class="line">workstation: HP-Z620. I thought it was insufficient computing resources. But it&#39;s wrong, the reason why it&#39;s so slow is that I repeatedly read the word vector matrix.</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>I published the code for the classification model (KASM)</title>
      <link href="2019/12/29/I-published-the-code-for-the-classification-model-KASM/"/>
      <url>2019/12/29/I-published-the-code-for-the-classification-model-KASM/</url>
      
        <content type="html"><![CDATA[<p>I created this script which about the KASM, for paper “Short Text Classification via Knowledge powered Attention with Similarity Matrix based CNN”. I just presented the model code, there are many source in our model, such as: father-child entity in Wikidata, etc. How to extract these information from Wikidata? How to vectorize? If you are interested in this, please contact me.<a href="https://github.com/ToneLi/KASM-Short-text-classification">Whole model code-KASM</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from Parameters import Parameters as pm</span><br><span class="line">from data_processing import *</span><br><span class="line">from tensorflow.contrib import layers</span><br><span class="line">class KASM(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        &quot;&quot;&quot;---输入-----------&quot;&quot;&quot;</span><br><span class="line">        # word level</span><br><span class="line">        self.input_word_x1 &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None, pm.word_seq_length], name&#x3D;&#39;input_word_x1&#39;)</span><br><span class="line">        self.input_y1 &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, pm.num_classes], name&#x3D;&#39;input_y1&#39;)</span><br><span class="line">        # 2 self entity level</span><br><span class="line">        self.input_selfentity_x1 &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None, pm.self_entity_seq_length], name&#x3D;&#39;input_selfentity_x1&#39;)</span><br><span class="line">        self.input_selfentity_y1 &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, pm.num_classes], name&#x3D;&#39;input_selfentity_y1&#39;)</span><br><span class="line">        # 3 father entity level</span><br><span class="line">        self.input_fatherentity_x1 &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None, pm.father_entity_seq_length],name&#x3D;&#39;input_fatherentity_x1&#39;)</span><br><span class="line">        self.input_fatherentity_y1 &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, pm.num_classes], name&#x3D;&#39;input_fatherentity_y1&#39;)</span><br><span class="line"></span><br><span class="line">        # 4 father entity level</span><br><span class="line">        self.input_cnn_x1 &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None, pm.cnn_inter_seq_length],name&#x3D;&#39;input_cnn_x1&#39;)</span><br><span class="line">        self.input_cnn_y1 &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, pm.num_classes],name&#x3D;&#39;input_cnn_y1&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;------句子长度-----------&quot;&quot;&quot;</span><br><span class="line">        self.word_seq_length &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None], name&#x3D;&#39;word_seq_length&#39;)</span><br><span class="line">        self.self_entity_seq_length &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None], name&#x3D;&#39;self_entity_seq_length&#39;)</span><br><span class="line">        self.father_entity_seq_length &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None], name&#x3D;&#39;father_entity_seq_length&#39;)</span><br><span class="line">        self.cnn_inter_seq_length &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None], name&#x3D;&#39;cnn_inter_seq_length&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        # 2 type</span><br><span class="line">        self.keep_pro &#x3D; tf.placeholder(tf.float32, name&#x3D;&#39;drop_out&#39;)</span><br><span class="line">        self.global_step &#x3D; tf.Variable(0, trainable&#x3D;False, name&#x3D;&#39;global_step&#39;)</span><br><span class="line">        self.KASM()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def KASM(self):</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;----------定义不同水平的细胞-----------&quot;&quot;&quot;</span><br><span class="line">        #word 水平的细胞</span><br><span class="line">        with tf.variable_scope(&#39;word_Cell&#39;): #word 细胞</span><br><span class="line">            word_cell_fw &#x3D; tf.contrib.rnn.GRUCell(pm.word_hidden_dim)</span><br><span class="line">            word_Cell_fw &#x3D; tf.contrib.rnn.DropoutWrapper(word_cell_fw, self.keep_pro)</span><br><span class="line">            word_cell_bw &#x3D; tf.contrib.rnn.GRUCell(pm.word_hidden_dim)</span><br><span class="line">            word_Cell_bw &#x3D; tf.contrib.rnn.DropoutWrapper(word_cell_bw, self.keep_pro)</span><br><span class="line"></span><br><span class="line">        # self-entity 水平的细胞</span><br><span class="line">        with tf.variable_scope(&#39;self_entity_Cell&#39;):  #(self-entity细胞)声明不同的cell 用 tf.variable_scope</span><br><span class="line">            selfentity_cell_fw &#x3D; tf.contrib.rnn.GRUCell(pm.selfentity_hidden_dim)</span><br><span class="line">            selfentity_Cell_fw &#x3D; tf.contrib.rnn.DropoutWrapper(selfentity_cell_fw, self.keep_pro)</span><br><span class="line">            selfentity_cell_bw &#x3D; tf.contrib.rnn.GRUCell(pm.selfentity_hidden_dim)</span><br><span class="line">            selfentity_Cell_bw &#x3D; tf.contrib.rnn.DropoutWrapper(selfentity_cell_bw, self.keep_pro)</span><br><span class="line">        # father-entity 水平的细胞</span><br><span class="line">            # self-entity 水平的细胞</span><br><span class="line">        with tf.variable_scope(&#39;father_entity_Cell&#39;):  # (self-entity细胞)声明不同的cell 用 tf.variable_scope</span><br><span class="line">            fatherentity_cell_fw &#x3D; tf.contrib.rnn.GRUCell(pm.fatherentity_hidden_dim)</span><br><span class="line">            fatherentity_Cell_fw &#x3D; tf.contrib.rnn.DropoutWrapper(fatherentity_cell_fw, self.keep_pro)</span><br><span class="line">            fatherentity_cell_bw &#x3D; tf.contrib.rnn.GRUCell(pm.fatherentity_hidden_dim)</span><br><span class="line">            fatherentity_Cell_bw &#x3D; tf.contrib.rnn.DropoutWrapper(fatherentity_cell_bw, self.keep_pro)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;----------输入嵌入矩阵-----------&quot;&quot;&quot;</span><br><span class="line">        #word 水平</span><br><span class="line">        with tf.variable_scope(&#39;word_embedding&#39;):</span><br><span class="line">            self.embedding &#x3D; tf.get_variable(&#39;word_embedding&#39;, shape&#x3D;[pm.word_vocab_size, pm.word_embedding_dim],</span><br><span class="line">                                             initializer&#x3D;tf.constant_initializer(pm.word_pre_training))</span><br><span class="line"></span><br><span class="line">            self.embedding_input &#x3D; tf.nn.embedding_lookup(self.embedding, self.input_word_x1)</span><br><span class="line">            # print(self.embedding)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #self-entity 水平</span><br><span class="line">        with tf.variable_scope(&#39;self_entity_embedding&#39;):</span><br><span class="line">            self.selfentity_embedding&#x3D; tf.get_variable(&#39;selfentity_embedding&#39;, shape&#x3D;[pm.selfentity_vocab_size, pm.self_entity_embedding_dim],</span><br><span class="line">                                             initializer&#x3D;tf.constant_initializer(pm.selfentity_pre_training))</span><br><span class="line">            self.selfentity_embedding_input &#x3D; tf.nn.embedding_lookup(self.selfentity_embedding, self.input_selfentity_x1)</span><br><span class="line"></span><br><span class="line">        # father-entity 水平</span><br><span class="line">        with tf.variable_scope(&#39;father_entity_embedding&#39;):</span><br><span class="line">            self.fatherentity_embedding &#x3D; tf.get_variable(&#39;fatherentity_embedding&#39;, shape&#x3D;[pm.fatherentity_vocab_size,</span><br><span class="line">                                                                                    pm.father_entity_embedding_dim],</span><br><span class="line">                                                                                    initializer&#x3D;tf.constant_initializer(</span><br><span class="line">                                                                                    pm.fatherentity_pre_training))</span><br><span class="line"></span><br><span class="line">            self.fatherentity_embedding_input &#x3D; tf.nn.embedding_lookup(self.fatherentity_embedding,</span><br><span class="line">                                                                     self.input_fatherentity_x1)</span><br><span class="line"></span><br><span class="line">        #cnn-interaction  水平</span><br><span class="line"></span><br><span class="line">        with tf.variable_scope(&#39;cnninteration_embedding&#39;):</span><br><span class="line">            self.cnninter_embedding &#x3D; tf.get_variable(&#39;cnninter_embedding&#39;, shape&#x3D;[pm.cnn_vocab_size,</span><br><span class="line">                                                                                    pm.cnn_inter_embedding_dim],</span><br><span class="line">                                                                                    initializer&#x3D;tf.constant_initializer(</span><br><span class="line">                                                                                    pm.cnninter_pre_training))</span><br><span class="line"></span><br><span class="line">            self.fatherentity_embedding_input &#x3D; tf.nn.embedding_lookup(self.cnninter_embedding,</span><br><span class="line">                                                                     self.input_cnn_x1)</span><br><span class="line"></span><br><span class="line">            self.embedded_chars_expanded &#x3D; tf.expand_dims(self.fatherentity_embedding_input, -1)</span><br><span class="line">            # print(&quot;1&quot;,self.fatherentity_embedding_input )</span><br><span class="line">            # print(&quot;2&quot;,self.embedded_chars_expanded )</span><br><span class="line">        &quot;&quot;&quot;-------CNN 架构---------&quot;&quot;&quot;</span><br><span class="line">        pooled_outputs &#x3D; []</span><br><span class="line">        for i, filter_size in enumerate(pm.filter_sizes):</span><br><span class="line">            filter_size &#x3D; int(filter_size)</span><br><span class="line">            #    Convolution Layer</span><br><span class="line">            filter_shape &#x3D; [filter_size, pm.cnn_inter_embedding_dim, 1, pm.num_filters]</span><br><span class="line">            W &#x3D; tf.Variable(tf.truncated_normal(filter_shape, stddev&#x3D;0.1), name&#x3D;&quot;cnn_W&quot;)</span><br><span class="line"></span><br><span class="line">            b &#x3D; tf.Variable(tf.constant(0.1, shape&#x3D;[pm.num_filters]), name&#x3D;&quot;b&quot;)</span><br><span class="line">            conv &#x3D; tf.nn.conv2d(</span><br><span class="line">                self.embedded_chars_expanded,</span><br><span class="line">                W,</span><br><span class="line">                strides&#x3D;[1, 1, 1, 1],</span><br><span class="line">                padding&#x3D;&quot;VALID&quot;,</span><br><span class="line">                name&#x3D;&quot;conv&quot;)</span><br><span class="line">            #</span><br><span class="line">            # Apply nonlinearity</span><br><span class="line">            h &#x3D; tf.nn.relu(tf.nn.bias_add(conv, b), name&#x3D;&quot;relu&quot;)</span><br><span class="line">            # Maxpooling over the outputs</span><br><span class="line">            pooled &#x3D; tf.nn.max_pool(</span><br><span class="line">                h,</span><br><span class="line">                ksize&#x3D;[1, pm.cnn_inter_seq_length - filter_size + 1, 1, 1],</span><br><span class="line">                strides&#x3D;[1, 1, 1, 1],</span><br><span class="line">                padding&#x3D;&#39;VALID&#39;,</span><br><span class="line">                name&#x3D;&quot;pool&quot;)</span><br><span class="line">            pooled_outputs.append(pooled)</span><br><span class="line"></span><br><span class="line">        # Combine all the pooled features</span><br><span class="line">        num_filters_total &#x3D; pm.num_filters * len(pm.filter_sizes)</span><br><span class="line">        self.h_pool &#x3D; tf.concat(pooled_outputs, 3)</span><br><span class="line">        self.h_pool_flat &#x3D; tf.reshape(self.h_pool, [-1, num_filters_total])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        #</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;----------细胞的连接，双向连接-----------&quot;&quot;&quot;</span><br><span class="line">        with tf.variable_scope(&#39;biRNN&#39;):</span><br><span class="line">            wordoutput, _ &#x3D; tf.nn.bidirectional_dynamic_rnn(cell_fw&#x3D;word_Cell_fw,</span><br><span class="line">                                                            cell_bw&#x3D;word_Cell_bw,</span><br><span class="line">                                                            inputs&#x3D;self.embedding_input,</span><br><span class="line">                                                            sequence_length&#x3D;self.word_seq_length,</span><br><span class="line">                                                            dtype&#x3D;tf.float32)</span><br><span class="line">            wordoutput &#x3D; tf.concat(wordoutput, 2) #[batch_size, seq_length, 2*hidden_dim] shape&#x3D;(?, 34, 100),</span><br><span class="line"></span><br><span class="line">        with tf.variable_scope(&#39;selfentitybiRNN&#39;):</span><br><span class="line">            selfentity_output, _ &#x3D; tf.nn.bidirectional_dynamic_rnn(cell_fw&#x3D;selfentity_Cell_fw,</span><br><span class="line">                                                                   cell_bw&#x3D;selfentity_Cell_bw,</span><br><span class="line">                                                                   inputs&#x3D;self.selfentity_embedding_input,</span><br><span class="line">                                                                   sequence_length&#x3D;self.self_entity_seq_length,</span><br><span class="line">                                                                   dtype&#x3D;tf.float32)</span><br><span class="line">            selfentity_output &#x3D; tf.concat(selfentity_output, 2) #[batch_size, seq_length, 2*hidden_dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        with tf.variable_scope(&#39;fatherentitybiRNN&#39;):</span><br><span class="line">            fatherentity_output, _ &#x3D; tf.nn.bidirectional_dynamic_rnn(cell_fw&#x3D;fatherentity_Cell_fw,</span><br><span class="line">                                                                   cell_bw&#x3D;fatherentity_Cell_bw,</span><br><span class="line">                                                                   inputs&#x3D;self.fatherentity_embedding_input,</span><br><span class="line">                                                                   sequence_length&#x3D;self.father_entity_seq_length,</span><br><span class="line">                                                                   dtype&#x3D;tf.float32)</span><br><span class="line">            fatherentity_output &#x3D; tf.concat(fatherentity_output, 2)  # [batch_size, seq_length, 2*hidden_dim]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            # print((tf.reshape(weights, [-1, seq_size, 1]), 1))#shape&#x3D;(?, 34, 1)</span><br><span class="line">            word_out_final &#x3D; tf.reduce_sum(wordoutput, 1)</span><br><span class="line">            selfentity_out_final &#x3D; tf.reduce_sum(selfentity_output, 1)</span><br><span class="line">            fatherentity_out_final &#x3D; tf.reduce_sum(fatherentity_output, 1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            # print(fatherentity_out_final)</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;-------张量拼接---------&quot;&quot;&quot;</span><br><span class="line">        end_final&#x3D;tf.concat([word_out_final,selfentity_out_final,fatherentity_out_final,self.h_pool_flat],1)</span><br><span class="line">        # print(&quot;end_final&quot;,end_final)</span><br><span class="line"></span><br><span class="line">        with tf.name_scope(&#39;dropout&#39;):</span><br><span class="line">            self.out_drop &#x3D; tf.nn.dropout(end_final, keep_prob&#x3D;self.keep_pro)</span><br><span class="line"></span><br><span class="line">        with tf.name_scope(&#39;output&#39;):</span><br><span class="line">            w &#x3D; tf.Variable(tf.truncated_normal([2*(pm.word_hidden_dim+pm.selfentity_hidden_dim+pm.fatherentity_hidden_dim)+pm.num_filters * len(pm.filter_sizes), pm.num_classes], stddev&#x3D;0.1), name&#x3D;&#39;w&#39;)</span><br><span class="line">            b &#x3D; tf.Variable(tf.zeros([pm.num_classes]), name&#x3D;&#39;b&#39;)</span><br><span class="line">            self.logits &#x3D; tf.matmul(self.out_drop, w) + b</span><br><span class="line">            self.predict &#x3D; tf.argmax(tf.nn.softmax(self.logits), 1, name&#x3D;&#39;predict&#39;) #shape&#x3D;(?,),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        with tf.name_scope(&#39;loss&#39;):</span><br><span class="line">            cross_entropy &#x3D; tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;self.logits, labels&#x3D;self.input_y1)</span><br><span class="line">            self.loss &#x3D; tf.reduce_mean(cross_entropy)+layers.l1_regularizer(0.3)(w)+layers.l2_regularizer(0.3)(w)</span><br><span class="line">    #</span><br><span class="line">        with tf.name_scope(&#39;optimizer&#39;):</span><br><span class="line">            optimizer &#x3D; tf.train.AdamOptimizer(pm.learning_rate)</span><br><span class="line">            gradients, variables &#x3D; zip(*optimizer.compute_gradients(self.loss))  # 计算变量梯度，得到梯度倿变量</span><br><span class="line">            gradients, _ &#x3D; tf.clip_by_global_norm(gradients, pm.clip)</span><br><span class="line">            # 对g进行l2正则化计算，比较其与clip的值，如果l2后的值更大，让梯庿(clip&#x2F;l2_g),得到新梯庿</span><br><span class="line">            self.optimizer &#x3D; optimizer.apply_gradients(zip(gradients, variables), global_step&#x3D;self.global_step)</span><br><span class="line">            # global_step 自动+1</span><br><span class="line">    #</span><br><span class="line">        with tf.name_scope(&#39;accuracy&#39;):</span><br><span class="line">            correct &#x3D; tf.equal(self.predict, tf.argmax(self.input_y1, 1))</span><br><span class="line">            self.accuracy &#x3D; tf.reduce_mean(tf.cast(correct, tf.float32), name&#x3D;&#39;accuracy&#39;)</span><br><span class="line">            # print(self.accuracy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def feed_data(self, word_x_batch, word_y_batch, word_seq_length,</span><br><span class="line">                  selfentity_x_batch,selfentity_y_batch,selfentity_length,</span><br><span class="line">                  fatherentity_x_batch, fatherentity_y_batch, fatherentity_length,</span><br><span class="line">                  cnn_x_batch, cnn_y_batch, cnn_length,</span><br><span class="line"></span><br><span class="line">                  keep_pro):</span><br><span class="line">        feed_dict &#x3D; &#123;self.input_word_x1: word_x_batch,</span><br><span class="line">                    self.input_y1: word_y_batch,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    self.input_selfentity_x1: selfentity_x_batch,</span><br><span class="line">                    self.input_selfentity_y1:selfentity_y_batch,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    self.input_fatherentity_x1: fatherentity_x_batch,</span><br><span class="line">                    self.input_fatherentity_y1: fatherentity_y_batch,</span><br><span class="line"></span><br><span class="line">                    self.input_cnn_x1: cnn_x_batch,</span><br><span class="line">                    self.input_cnn_y1: cnn_y_batch,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    self.word_seq_length: word_seq_length,</span><br><span class="line">                    self.self_entity_seq_length:selfentity_length,</span><br><span class="line">                    self.father_entity_seq_length:fatherentity_length,</span><br><span class="line">                    self.cnn_inter_seq_length:cnn_length,</span><br><span class="line">                    self.keep_pro: keep_pro&#125;</span><br><span class="line"></span><br><span class="line">        return feed_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def evaluate(self, sess, x1, y1,x2,y2,z1,z2,s1,s2,):</span><br><span class="line">        batch_test &#x3D; batch_iter(x1, y1,x2,y2,z1,z2,s1,s2,batch_size&#x3D;64)</span><br><span class="line">        for x_batch, y_batch,x_selfentity_train,y_selfentity_train,x_fatherentity_train,y_fatherentity_train,x_cnn,y_cnn in batch_test:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            seq_len &#x3D; sequence(x_batch)</span><br><span class="line">            selfentity_len &#x3D; sequence(x_selfentity_train)</span><br><span class="line">            fatherentity_len &#x3D; sequence(x_fatherentity_train)</span><br><span class="line">            cnninter_len &#x3D; sequence(x_cnn)</span><br><span class="line"></span><br><span class="line">            feed_dict &#x3D; self.feed_data(x_batch, y_batch, seq_len,</span><br><span class="line">                                       x_selfentity_train,y_selfentity_train,selfentity_len,</span><br><span class="line">                                       x_fatherentity_train,y_fatherentity_train, fatherentity_len,</span><br><span class="line">                                       x_cnn, y_cnn,cnninter_len,1.0)</span><br><span class="line"></span><br><span class="line">            test_loss, test_accuracy &#x3D; sess.run([self.loss, self.accuracy], feed_dict&#x3D;feed_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return test_loss, test_accuracy</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT vector in my five classification datasets</title>
      <link href="2019/12/29/BERT-vector-in-my-five-classification-datasets/"/>
      <url>2019/12/29/BERT-vector-in-my-five-classification-datasets/</url>
      
        <content type="html"><![CDATA[<p>Due to research needs, I need to use the word embedding which is computed by BERT in my model-KASM . So, I used BERT to generate the vector matrixs for five text classification datasets: MR, TREC, SST1, SST2, AG. If you need them, please contact me. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from bert_serving.client import BertClient</span><br><span class="line"></span><br><span class="line">def BERT_vec(word):</span><br><span class="line"></span><br><span class="line">    bc &#x3D; BertClient()</span><br><span class="line">    doc_vecs &#x3D; bc.encode([word])</span><br><span class="line">    vecs&#x3D;list(doc_vecs[0])</span><br><span class="line">    return(vecs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def obtain_vec_from_bert(input,output):</span><br><span class="line">    i&#x3D;0</span><br><span class="line">    fw&#x3D;open(output,&quot;w&quot;,encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">    with open(input,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            i &#x3D; i + 1</span><br><span class="line">            print(i)</span><br><span class="line">            line&#x3D;line.strip().split(&quot;\t&quot;)</span><br><span class="line">            word&#x3D;line[0]</span><br><span class="line">            fw.write(word+&quot;\t&quot;)</span><br><span class="line">            vecs&#x3D;BERT_vec(word)</span><br><span class="line">            for vec in  vecs:</span><br><span class="line">                fw.write(str(vec)+&quot; &quot;)</span><br><span class="line">            fw.write(&quot;\n&quot;)</span><br><span class="line">        fw.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    inputpath&#x3D;&quot;SST1_word_dic.txt&quot;</span><br><span class="line">    outputpath&#x3D;&quot;SST1_word_BERT_vec.txt&quot;</span><br><span class="line">    obtain_vec_from_bert(inputpath,outputpath)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT for text classification</title>
      <link href="2019/12/25/BERT-for-text-classification/"/>
      <url>2019/12/25/BERT-for-text-classification/</url>
      
        <content type="html"><![CDATA[<p>My reference is <a href="https://github.com/Socialbird-AILab/BERT-Classification-Tutorial">Socialbird-AILab</a> and <a href="https://mp.weixin.qq.com/s/XmeDjHSFI0UsQmKeOgwnyA">its detail</a></p><p>I use the same parameters as Socialbird-AILab. On the other hand, I have applied this model into five open dataset, TERC, SST1, SST2, AG, MR.<br>It’s a comparison experiment for our work: “Short Text Classiﬁcation via Knowledge powered Attention with Similarity Matrix based CNN”. In the next, I will present the run_classifier.py in the below, and tell you how to modify this file to fit your dataset. On the other hand, you can <a href="https://arxiv.org/abs/1905.05583?context=cs.CLs">refer this paper</a> for FineTuning BERT and how to tune BERT for classification.</p><pre><code>I use the same code and the data, but not the same device, Socialbird-AILab used WIndows Tensorflow-win-1.10.0-gpu, anaconda-1.9.2,NVIDIA GeoForce GTX 1080 Ti. My divice: my laptop-Lenovo, Intel(R) Core(TM)i7-6500 CPU @ 2.50GHZ 2.59GHZ, RAM: 16.0GB.They results: Eval_accurac=0.805              eval_loss=1.4425056              global_step=3125              loss=1.4425056But my results:                Eval_accurac=0.1              eval_loss=2.3363712              global_step=312              loss=2.3363It stopped running automatically, so the same parameters may have different results in different devices.</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">class MrpcProcessor(DataProcessor):</span><br><span class="line">    &quot;&quot;&quot;Processor for the MRPC data set (GLUE version).&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def get_train_examples(self, data_dir):</span><br><span class="line">        &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">        return self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, &quot;train.tsv&quot;)), &quot;train&quot;)</span><br><span class="line"></span><br><span class="line">    def get_dev_examples(self, data_dir):</span><br><span class="line">        &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">        return self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;dev&quot;)</span><br><span class="line"></span><br><span class="line">    def get_test_examples(self, data_dir):</span><br><span class="line">        &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        return self._create_examples(</span><br><span class="line">            self._read_tsv(os.path.join(data_dir, &quot;dev.tsv&quot;)), &quot;test&quot;)</span><br><span class="line">    #变了</span><br><span class="line">    def get_labels(self, labels):</span><br><span class="line">        &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">        return set(labels)</span><br><span class="line"></span><br><span class="line">    # new function get the result tsv</span><br><span class="line">    def get_results(self, data_dir):</span><br><span class="line">        &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">        return self._read_tsv(os.path.join(data_dir, &quot;test_results.tsv&quot;))</span><br><span class="line"></span><br><span class="line">    def _create_examples(self, lines, set_type):</span><br><span class="line">        &quot;&quot;&quot;Creates examples for the training and dev sets.&quot;&quot;&quot;</span><br><span class="line">        examples &#x3D; []</span><br><span class="line">        labels &#x3D; []</span><br><span class="line">        labels_test &#x3D; []</span><br><span class="line">        for (i, line) in enumerate(lines):</span><br><span class="line">            if i &#x3D;&#x3D; 0:</span><br><span class="line">                continue</span><br><span class="line">            guid &#x3D; &quot;%s-%s&quot; % (set_type, i)</span><br><span class="line"></span><br><span class="line">            # tokenization is based on vocab file</span><br><span class="line">            text_a &#x3D; tokenization.convert_to_unicode(line[1])</span><br><span class="line">            label &#x3D; tokenization.convert_to_unicode(line[0])</span><br><span class="line">            labels.append(label)</span><br><span class="line"></span><br><span class="line">            if set_type &#x3D;&#x3D; &quot;test&quot;:</span><br><span class="line">                label &#x3D; &quot;0&quot;</span><br><span class="line">            labels_test.append(label)</span><br><span class="line">            examples.append(</span><br><span class="line">                InputExample(guid&#x3D;guid, text_a&#x3D;text_a, text_b&#x3D;None, label&#x3D;label))</span><br><span class="line"></span><br><span class="line">        return examples, labels, labels_test</span><br><span class="line"></span><br><span class="line">To change the class MrpcProcessor to what you need.</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>something you keep in mind will blossom someday</title>
      <link href="2019/12/22/something-you-keep-in-mind-will-blossom-someday/"/>
      <url>2019/12/22/something-you-keep-in-mind-will-blossom-someday/</url>
      
        <content type="html"><![CDATA[<p>Today is the Winter Solstice in China, and the last day of<br>post-graduate entrance examination. I just want to say “Something you keep in mind will blossom someday…”. At this time, I am writing the paper about Chinese medical QA. Begin to my model.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>soul diary 2019/12/20</title>
      <link href="2019/12/20/soul-diary-2019-12-20/"/>
      <url>2019/12/20/soul-diary-2019-12-20/</url>
      
        <content type="html"><![CDATA[<pre><code>The response about my rebuttal of ECAI2020 is over. To be honest, I am disgruntled with the reviewer1, but I am very proud of what I did as an undergraduate. Our team don&#39;t care about the result about accept/reject. Do yourself!! In the next, I will continue our team&#39;s work about  short text classification, Chinese medical question answering, knowledge graph and pass the IELTS. </code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>How to write a  good rebuttal</title>
      <link href="2019/12/19/How-to-write-a-good-rebuttal/"/>
      <url>2019/12/19/How-to-write-a-good-rebuttal/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.st.cs.uni-saarland.de/zeller/onresearch/rebuttal-patterns.php3">Patterns for writing good rebuttals, by Andreas Zeller</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Rebuttal of ECAI2020 is ongoing (add my recent rejection)</title>
      <link href="2019/12/19/Rebuttal-of-ECAI2020-is-ongoing/"/>
      <url>2019/12/19/Rebuttal-of-ECAI2020-is-ongoing/</url>
      
        <content type="html"><![CDATA[<p>This time, 3 reviews (reject, accept, weak reject). I will remember this time! little fuck (ECAI2020). <font color=red size=3> I have received three reviews, in the overall evalation, -2(reject), 2 (accept), -1 (weak reject).In the technical quality level, the scores are 2(poor), 5 (excellent), 4 (good). I really don’t know if the first reviewer have read my paper, this is disrespect to me. why you give me 2, others give me 5 and 4? You are kidding me!</font></p><p>My rebuttal.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">I really appreciate reviewers&#39; hard work. I am a student, this paper will determine if</span><br><span class="line">I can apply for a PHD degree. I have been rejected by EMNLP2019, WSDM2020, and modified</span><br><span class="line">our work based on experts&#39; reviews. </span><br><span class="line"></span><br><span class="line">Firstly, I think the first reviewer disgusts our work, he&#x2F;she does not use the enough</span><br><span class="line">reasons to reject our work (or he&#x2F;she does not read our paper). In the technical quality</span><br><span class="line">level, others give us 5 points(excellent) and 4 (good), he only gives us 2 points. I</span><br><span class="line">think it&#39;s unnormal. Other reasons are as follows:</span><br><span class="line"></span><br><span class="line">R1: reviewer says: the authors propose a similarity matrix based CNN model...(line 4)</span><br><span class="line">Q1: Our work just use similarity matrix based CNN to obtain the interaction information </span><br><span class="line">between sentence and label&#39;s representation. No one uses this information in text </span><br><span class="line">classification. I think this reviewer does not know our model&#39;s framework. He believes that </span><br><span class="line">the subject is actually part of our model.</span><br><span class="line"></span><br><span class="line">R2: reviewer says:  the effetiveness of leveraging KGs with the help of an attention mechaism..(line6)</span><br><span class="line">Q2: Actually, our model has two attention mechanisms: self-entity attention and parent-entity attention. Otherwise, he ignores an important part to improve</span><br><span class="line">our model&#39;s accuarcy --interaction information. (In our paper, page 6, right column, line 4), we have described this point.</span><br><span class="line"></span><br><span class="line">R3: line 9, ontologies is not something new..</span><br><span class="line">Q3: Ok, ontologies is not the new, but there are different ways to use ontology, such as, I introduce father entity in our model.</span><br><span class="line">or Many people uses transe to train the knowledge graph, can you say a novel knowledge embedding method which is used in NLP task is not a research?</span><br><span class="line"></span><br><span class="line">R4: you say: our paper lacks enough evidence to confirm this method.</span><br><span class="line">Q4: You want to let me use BERT as our standard comparison experiment (personal opinion), but you have said BERT is not good for this task. In paper</span><br><span class="line">&quot;Deep Short Text Classification with Knowledge Powered Attention (2019)&quot; and other latest works don&#39;t use BERT as their comparison experiment. </span><br><span class="line">If you confirm all works about text classification must have BERT as comparison experiment, I will accept your opinion.</span><br><span class="line">On the other hand, does all tasks use BERT? Why the reviewers in wsdm2020 and emnlp2019 do not let me have to use BERT? A BERT constitutes your reason for rejecting me.</span><br><span class="line"></span><br><span class="line">R5: grammar mistake.</span><br><span class="line">Q5: I accept it, I will polish my paper, and I think it does not influence the acception. But you did not point out any errors in my paper, </span><br><span class="line">so you said that there are errors in my paper that does not constitute evidence. I admit there are some syntax errors.</span><br><span class="line"></span><br><span class="line">At last, for reviewer 1: What you said is very unspecific.</span><br><span class="line"></span><br><span class="line">----------------------------------------</span><br><span class="line"></span><br><span class="line">Secondly, for reviewer 2, thank you for pointing out my errors in our work, thanks again! I will do the respone for you:</span><br><span class="line">R1: For instane, short text C:.. is?</span><br><span class="line">Q1: it&#39;s an excample.</span><br><span class="line"></span><br><span class="line">R2: &quot;stochasitic matrix&quot; is ?</span><br><span class="line">Q2: I am sure it&#39;s a correct term.</span><br><span class="line"></span><br><span class="line">R3: A wikidata entity may have multiple P313 triples.</span><br><span class="line">Q3: It&#39;s right, maybe I have a wrong description. An entity only has one meaning, it only has one father entity by relation P31. Not concept.</span><br><span class="line"></span><br><span class="line">R4: we can not regard..</span><br><span class="line">Q4: I have corrected it, it is &quot;we can not say that&quot;</span><br><span class="line"></span><br><span class="line">R5: Is \hat&#123;p^k&#125; a single nubmer or...</span><br><span class="line">Q5: I have defined it in Equation 11.</span><br><span class="line"></span><br><span class="line">R6: Is $\phi$ still ReLU here?</span><br><span class="line">Q6:  yes</span><br><span class="line"></span><br><span class="line">R7: optimized form a set \delta&quot;</span><br><span class="line">Q7:a set \delta&quot; ---  It is a set of parameters that need to be optimized.</span><br><span class="line"></span><br><span class="line">R8:  &quot;binary labels&quot; -&gt; Unsure ..</span><br><span class="line">Q8: It refers to just have two types label.</span><br><span class="line"></span><br><span class="line">R9: I wonder how the positive&#x2F;negative were ....</span><br><span class="line">Q9: For excample, [positive], we can clear this word&#39;s meaning by ourself, then we can get this concept&#39;s description in Wikidata by its meaning. Just like the sample &quot;HUM&quot; in our paper.</span><br><span class="line"></span><br><span class="line">-----------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">Thirdly, for reviewer 3:</span><br><span class="line">Thank you for your recognition of our work, I will correct these errors. I don&#39;t use BERT as our compared experiment, I think this cannot </span><br><span class="line">influence my paper&#39;s acception.  </span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">----------------------- REVIEW 1 ---------------------</span><br><span class="line">SUBMISSION: 571</span><br><span class="line">TITLE: Short Text Classiﬁcation via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line">AUTHORS: Mingchen Li, Gabtone Clinton, Yijia Mia and Feng Gao</span><br><span class="line"></span><br><span class="line">----------- Relevance -----------</span><br><span class="line">SCORE: 5 (excellent)</span><br><span class="line">----------- Significance -----------</span><br><span class="line">SCORE: 2 (poor)</span><br><span class="line">----------- Novelty -----------</span><br><span class="line">SCORE: 4 (good)</span><br><span class="line">----------- Technical quality -----------</span><br><span class="line">SCORE: 2 (poor)</span><br><span class="line">----------- Quality of the presentation -----------</span><br><span class="line">SCORE: 3 (fair)</span><br><span class="line">----------- Overall evaluation -----------</span><br><span class="line">SCORE: -2 (reject)</span><br><span class="line">----- TEXT:</span><br><span class="line">Short text classification is a critical task in my many text analytics related applications. In fact, this is a relatively old problem in NLP domain and researchers in the last two decades proposed alternative techniques to tackle this problem. In this paper, the authors propose a similarity matrix based CNN model where the model is enriched with external knowledge. The experiments conducted on five datasets demonstrates the effectiveness of leveraging KGs with the help of an attention mechanism for the purposes of text classification task. </span><br><span class="line"></span><br><span class="line">Despite being a well-studied problem, text classification is a popular area for further exploration as new deep learning techniques are developed. Exploitation of external knowledge and ontologies is not something new for this task. However, the authors of this paper did a good job of integrating this knowledge into complex deep learning models and exploiting it with an attention mechanism. From the point of technical depth and novelty of this idea the paper has adequate content. However the paper lacks enough evidence to confirm that this approach is more effective than some of the well-known techniques proposed in the literature. Bag-of-words approach is a very simple yet effective approach for text classification purposes. This simple approach can be augmented by simply applying text enrichment with the help of external ontologies that can tackle the explicit representation problem. Another version of this would be to leverage pre-trained word embeddings for the purposes !</span><br><span class="line">of dealing with implicit representation problem. A third dimension, (perhaps the most important one) would be to leverage BERT embeddings to apply fine tuning for the task of text classification. Recently, many research studies proved that BERT provides tremendous improvement for various NLP tasks including text classification. Based on our own observations, it is very hard to make further improvement over a fine-tuned BERT model by leveraging external knowledge simply because BERT has been trained from extremely large corpuses and captures both explicit and implicit representations. The authors of this paper should compare their approach with these three techniques (in particular the third one) to conclude that their approach is superior than state of the art. </span><br><span class="line"></span><br><span class="line">Another problem with the paper is that the write up needs some polishing to get ready for publication. There are many typos and grammar mistakes in the text which creates an impression that the submission has been rushed. A detailed proof reading is necessary to fix these errors.</span><br><span class="line"></span><br><span class="line">Once these points are addressed I think the paper will be ready for publication.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------------------- REVIEW 2 ---------------------</span><br><span class="line">SUBMISSION: 571</span><br><span class="line">TITLE: Short Text Classiﬁcation via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line">AUTHORS: Mingchen Li, Gabtone Clinton, Yijia Mia and Feng Gao</span><br><span class="line"></span><br><span class="line">----------- Relevance -----------</span><br><span class="line">SCORE: 5 (excellent)</span><br><span class="line">----------- Significance -----------</span><br><span class="line">SCORE: 5 (excellent)</span><br><span class="line">----------- Novelty -----------</span><br><span class="line">SCORE: 4 (good)</span><br><span class="line">----------- Technical quality -----------</span><br><span class="line">SCORE: 5 (excellent)</span><br><span class="line">----------- Quality of the presentation -----------</span><br><span class="line">SCORE: 3 (fair)</span><br><span class="line">----------- Overall evaluation -----------</span><br><span class="line">SCORE: 2 (accept)</span><br><span class="line">----- TEXT:</span><br><span class="line">The paper presents a complex pipeline for classifying short texts, combining word embeddings, knowledge graph embeddings, recurrent and convolutional neural networks. The approach is evaluated on 5 datasets (4 independent + 1 derrived) and in all the cases it achieves performance better than the considered baselines. Unfortunately, due to the complexity of the proposed solution and the page limit, the paper is very dense and thus quite hard to read.</span><br><span class="line"></span><br><span class="line">Minor remarks:</span><br><span class="line">* General: Consider using labels together with or instead the Wikidata Q... identifiers. They are not very suitable for using in human-readable examples.</span><br><span class="line">* Page 1, left column, bottom: &quot;will be failed&quot; -&gt; &quot;will fail&quot;</span><br><span class="line">* Page 1, right column, middle: &quot;For instance, short text C:...&quot; -&gt; why does it have such a label? Is it an example, or does it intrinsically follow from the presented short text?</span><br><span class="line">* Page 1, right column, bottom: &quot;position in variant&quot; -&gt; &quot;position invariant&quot;</span><br><span class="line">* Caption of Figure 1: P31 should be rather denoted as is_instance_of</span><br><span class="line">* Section 2 Problem Formulation: This reads more like Preliminaries than like a problem formulation. </span><br><span class="line">* Page 4, left column, top: &quot;stochastic matrix&quot; - I am not sure why this is correct or relevant term.</span><br><span class="line">* Page 4, left column, middle: &quot;self entity has only one parent entity&quot; - Why is that? A wikidata entity may have multiple P31 triples.</span><br><span class="line">* Page 4, left column, middle: &quot;we can not regard&quot; - I do not understand.</span><br><span class="line">* Page 4, Equation 12: Is \hat&#123;P^k&#125; a single number or a matrix? Max pooling may result in either, and it does not follow from the description.</span><br><span class="line">* Page 4, Equation 14: Is $\phi$ still ReLU here?</span><br><span class="line">* Page 4, right column, bottom: &quot;optimized form a set \delta&quot; - I do not understand.</span><br><span class="line">* Page 4, Equation 16: Logarithm is, in my opinion, a function of a single argument, but here are two (y_d and \delta). What does it mean?</span><br><span class="line">* Page 5, Section 4.1, SST-2: &quot;binary labels&quot; -&gt; Unsure what do you mean.</span><br><span class="line">* Page 5, Section 4.1: I wonder how the positive&#x2F;negative were transformed according to Definition 6.</span><br><span class="line">* Page 5, right column, bottom: &quot;convolutinoal layer&quot; -&gt; &quot;convolutional layer&quot;</span><br><span class="line">* Page 6, Table 3: I find it interesting that KASM-NA is better on AG-new than KASM. Any hints why that may be so? Also, what is the purpose of typesetting the last row of the table in bold? I thought it is to emphasize the best results, but the case of KASM-NA on AG-new indicates it is not so.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------------------- REVIEW 3 ---------------------</span><br><span class="line">SUBMISSION: 571</span><br><span class="line">TITLE: Short Text Classiﬁcation via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line">AUTHORS: Mingchen Li, Gabtone Clinton, Yijia Mia and Feng Gao</span><br><span class="line"></span><br><span class="line">----------- Relevance -----------</span><br><span class="line">SCORE: 4 (good)</span><br><span class="line">----------- Significance -----------</span><br><span class="line">SCORE: 3 (fair)</span><br><span class="line">----------- Novelty -----------</span><br><span class="line">SCORE: 4 (good)</span><br><span class="line">----------- Technical quality -----------</span><br><span class="line">SCORE: 4 (good)</span><br><span class="line">----------- Quality of the presentation -----------</span><br><span class="line">SCORE: 2 (poor)</span><br><span class="line">----------- Overall evaluation -----------</span><br><span class="line">SCORE: -1 (weak reject)</span><br><span class="line">----- TEXT:</span><br><span class="line">The paper addresses the relevant topic of short text classification which has been extensively researched in the past years. Typical problems that occur are related to the sparseness of the text given and, often related to or even derived from that, ambiguitiy of the meaning.</span><br><span class="line"></span><br><span class="line">The approach introduces a novel similarity matrix based convolutional neural network model which features the enrichment of sparse textual data with additional external knowledge from a knowledge graph. The paper includes the description of an experiment conducted on five different data sets to show the model effectiveness for the task of text classification.</span><br><span class="line"></span><br><span class="line">The approach nicely integrates different methods from machine learning (CNN) with knowledge representation (knowledge graph) for solving an interesting task. The paper is well-structured and has a sufficient technical depth to explain the model well. </span><br><span class="line"></span><br><span class="line">The main critique for the paper comes from the fact that the evaluation does not address recent advances in using well-trained BERT models for text classification which have shown rather convincing results. I.e., a comparion is still needed to show that the approach is state-of-the-art. Also, it seems advisable to investigate into a simple bag-of-works model as a baseline (which would not be much effort, but at the same time would effectively and nicely show the lower end of sophistication in the approach).</span><br><span class="line"></span><br><span class="line">Also, the language quality should be improved significantly as there occur quite a number of grammar and spelling errors.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------</span><br></pre></td></tr></table></figure><p>WSDM2020  (2019-10-11)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">Dear Mingchen Li,</span><br><span class="line"></span><br><span class="line">We regret to inform you that your submission was not selected for the WSDM 2020 conference:</span><br><span class="line"></span><br><span class="line">Submission 285: Short Text Classification via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line"></span><br><span class="line">This year we received 615 submissions. We were able to accept only 91 of these, representing an acceptance rate of 15%. Due to the limited number of available slots in the conference schedule, we unfortunately had to make very difficult decisions and decline several good submissions.</span><br><span class="line"></span><br><span class="line">The program committee worked very hard to thoroughly review all the submitted papers and to provide action points to improve your paper. All papers were reviewed by at least three program committee members, and by a senior PC member to oversee discussion amongst the reviewers and provide an overall recommendation for the paper. Additionally, a second SPC was assigned to further discuss many of the papers so to ensure consistency across the entire decision process. The meta-review by the senior PC member and the reviews of the PC members are below.  We hope these will be useful to revise your work for future submissions.</span><br><span class="line"></span><br><span class="line">Please consider submitting your work to one of the workshops associated with WSDM 2020:</span><br><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;www.wsdm-conference.org&#x2F;2020&#x2F;workshops.php</span><br><span class="line"></span><br><span class="line">While we understand that this outcome may be disappointing, we hope that you will still consider attending the conference. Early bird registration will be open soon at the conference website: http:&#x2F;&#x2F;www.wsdm-conference.org&#x2F;2020&#x2F;. </span><br><span class="line"></span><br><span class="line">We very much hope you will be able to attend WSDM 2020 this coming February in Houston.</span><br><span class="line"></span><br><span class="line">Best regards,</span><br><span class="line">Mounia Lalmas &amp; Wei Wang</span><br><span class="line">WSDM 2020 Program Co-Chairs</span><br><span class="line"></span><br><span class="line">SUBMISSION: 285</span><br><span class="line">TITLE: Short Text Classification via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line"></span><br><span class="line">-------------------------  METAREVIEW  ------------------------</span><br><span class="line">This is the meta-review. </span><br><span class="line"></span><br><span class="line">Summary of strengths and weaknesses of the paper:</span><br><span class="line">+: The problem studied in this work is important and trending.</span><br><span class="line">+: And the idea is reasonable.</span><br><span class="line">But unfortunately, there are some non-trivial problems:</span><br><span class="line">-: The presentation of the paper needs to be improved, including typos and descriptions.</span><br><span class="line">-: Experimental analysis is another major weakness of the paper. More strong baselines are expected. Ablation study is necessary.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Discussion:</span><br><span class="line">There are no conflicting points among individual reviews. All the reviewers agree that the paper is not ready for publication at WSDM in current version.</span><br><span class="line"></span><br><span class="line">Main reasons for final recommendation of acceptance or rejection:</span><br><span class="line">Presentation problem, and weak experimental analyses.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------------------- REVIEW 1 ---------------------</span><br><span class="line">SUBMISSION: 285</span><br><span class="line">TITLE: Short Text Classification via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line">AUTHORS: Mingchen Li and Yijia Miao</span><br><span class="line"></span><br><span class="line">----------- Paper Clarity -----------</span><br><span class="line">SCORE: 1 (Poor (Hard to follow))</span><br><span class="line">----------- Interest to Audience -----------</span><br><span class="line">SCORE: 4 (High)</span><br><span class="line">----------- Paper Significance -----------</span><br><span class="line">SCORE: 3 (Above Average)</span><br><span class="line">----------- Strengths -----------</span><br><span class="line">+ I find the idea of leveraging the knowledge graph to help classification of short text interesting.</span><br><span class="line"></span><br><span class="line">+ I also find the idea of expanding the labels to define their meanings, and then constructing an interaction matrix between words in a label and that in the text interesting.</span><br><span class="line"></span><br><span class="line">+ Performance (accuracy) is substantially better than baselines.</span><br><span class="line"></span><br><span class="line">+ The paper attempts to help the reader to understand the significance of &quot;father entity&quot; and the interaction information in Sections 5.2-5.2.</span><br><span class="line">----------- Weaknesses -----------</span><br><span class="line">- The paper is very difficult to understand with a lot of issues in language use</span><br><span class="line"></span><br><span class="line">- While the paper attempted to explain the important of father entity, and of interaction information and attention in Section 5.2-5.4, it would be much more helpful if a proper ablation study is done, where each ingredient (entities, father entities, interaction info, attention) is removed one at a time and performance measured</span><br><span class="line"></span><br><span class="line">- The datasets are quite small, and only split into training and testing sets, without validation set. Therefore it is subject to over-fitting.</span><br><span class="line">----------- Overall Evaluation -----------</span><br><span class="line">SCORE: -1 (Weak reject)</span><br><span class="line">----- TEXT:</span><br><span class="line">This paper considers the problem of classifying short texts. It suggests to augment text with entities in a knowledge graph, as well as the parent entities (called father entities in the paper). Furthermore, since labels to be classified have meanings (e.g., LOC for location, TEC for technology), the paper proposes to expand the labels into full sentences, and compute the interaction (dot products) among the embeddings of the words in the text, and those of the words that define the labels. By combining 3 sources of information (texts, entities, interactions between texts and labels), the paper show significant accuracy improvement over the baselines.</span><br><span class="line"></span><br><span class="line">I like the ideas of the paper, however, among all the papers I reviewed for WSDM, this paper is the hardest to understand, because of poor quality of the writing, as well as lack of exact definition. Some examples:</span><br><span class="line"></span><br><span class="line">- In abstract, &quot;Many existing work is difficult to address this problem&quot;-&gt;&quot;Many existing works have difficulties in addressing this problem&quot;?</span><br><span class="line">- Throughout the paper, double quotes are not typeset properly. E.g., First page right column, &#39;&#39;warrior&#39;&#39; should be typeset as &#96;&#96;warrior&#39;&#39; in latex.</span><br><span class="line">- Section 2, &quot;PROBLME FORMULATION&quot;-&gt;&quot;PROBLEM FORMULATION&quot;</span><br><span class="line">- Definition 6, &quot;by manual&quot;-&gt;&quot;manually&quot;</span><br><span class="line">- Section 3.1, &quot;limerick&quot; has only one mean-&gt;&quot;meaning&quot;</span><br><span class="line">- Equation (5), please give the dimensions of W_s and U_s and b_s.</span><br><span class="line">- Equation 6, please give the dimensions of W_l and U-t and b_t</span><br><span class="line">- Equation (9), what is x_l? I guess it is the vector of a word that define the label, but I prefer that you define for the reader.</span><br><span class="line"></span><br><span class="line">Overall, while I like the ideas of the paper and the results, I find it difficult to recommend its acceptance due to the very poor writing, and lack of careful analysis (ablation study). I do like to support its eventual publication after these issues are fixed.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------------------- REVIEW 2 ---------------------</span><br><span class="line">SUBMISSION: 285</span><br><span class="line">TITLE: Short Text Classification via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line">AUTHORS: Mingchen Li and Yijia Miao</span><br><span class="line"></span><br><span class="line">----------- Paper Clarity -----------</span><br><span class="line">SCORE: 3 (Above Average)</span><br><span class="line">----------- Interest to Audience -----------</span><br><span class="line">SCORE: 3 (Medium)</span><br><span class="line">----------- Paper Significance -----------</span><br><span class="line">SCORE: 2 (Below Average)</span><br><span class="line">----------- Strengths -----------</span><br><span class="line">The use of knowledge graph embedding is very good.</span><br><span class="line">The paper is trying to solve an important long-standing problem and the proposed model is presented well.</span><br><span class="line">The experiments are rigorous, however, more important ones are missing.</span><br><span class="line">----------- Weaknesses -----------</span><br><span class="line">The baseline comparisons should include methods that handle short text classification and short text topic modeling. There are a lot of works published in this area in the last couple of years. The authors have completely ignored that line of research.</span><br><span class="line"></span><br><span class="line">The writing of the paper needs improvement. There are several typos including some in the headings and sub-headings itself. This is not acceptable.</span><br><span class="line"></span><br><span class="line">The way in which the overall architecture is presented in Figure 2, it has a lot of similarities with Factorization machines (especially in terms of using the interaction terms). The authors will need to compare the performance of their model with more recent FM based models like deepFM and Neural FM models or indicate why their model is conceptually different.</span><br><span class="line"></span><br><span class="line">The implicit and explicit representations used in the paper are similar to the use of local and global contexts in short text models. There is also a lot of work based on matrix factorization (such as SeaNMF model). The authors should look into these methods and distinguish their work more clearly.</span><br><span class="line"></span><br><span class="line">More experiments on ablation study are needed. This will help in understanding the insights of the proposed work.</span><br><span class="line">----------- Overall Evaluation -----------</span><br><span class="line">SCORE: -1 (Weak reject)</span><br><span class="line">----- TEXT:</span><br><span class="line">The paper should be more comprehensive and include various methods developed in the literature for handling short text.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------------------- REVIEW 3 ---------------------</span><br><span class="line">SUBMISSION: 285</span><br><span class="line">TITLE: Short Text Classification via Knowledge powered Attention with Similarity Matrix based CNN</span><br><span class="line">AUTHORS: Mingchen Li and Yijia Miao</span><br><span class="line"></span><br><span class="line">----------- Paper Clarity -----------</span><br><span class="line">SCORE: 2 (Below Average)</span><br><span class="line">----------- Interest to Audience -----------</span><br><span class="line">SCORE: 2 (Fair)</span><br><span class="line">----------- Paper Significance -----------</span><br><span class="line">SCORE: 2 (Below Average)</span><br><span class="line">----------- Strengths -----------</span><br><span class="line">1. The work proposes the use of CNN combining attention with similarity matrix to present a better classification of text. The work uses Wikidata knowledge graph to compute relevance of an entity in a given sentence. This is an interesting problem with several works presented in the area. </span><br><span class="line">2. The authors present a pipeline method for text classification. Using attention mechanism to compute entity weights is a smart way to approach the problem. </span><br><span class="line">3. The experimentation datasets and baseline methods are extensive and provide a good understanding towards why this problem is an important one.</span><br><span class="line">----------- Weaknesses -----------</span><br><span class="line">1. The paper does not present a clear flow toward understanding the research question being tackled. While the introduction goes into detail how implicit and explicit representations are important to in solving text classification, I would&#39;ve liked to a more simpler approach explaining the research problem.</span><br><span class="line">2. While the datasets and baseline methods presented are extensive, I found the experimentation section to be a bit weak. I would&#39;ve liked to see a discussion on where the presented method makes a difference compared to its closest result. Also, the evaluation metric should be once again mentioned in the experiments section that this chosen label is the closest probability metric. I would advice the authors to rewrite this section with a much clear explanation. </span><br><span class="line">3. Spelling mistakes: The second line of the abstract contains WeChart instead of WeChat and the introduction section spelt acquire wrong. In addition, I found a few sentences that need clarification. A lot of sentences assume prior knowledge of existing methods. I would&#39;ve like to see how the proposed work differs from the existing related work in the relevant section over saying that the authors were inspired by these methods.</span><br><span class="line">----------- Overall Evaluation -----------</span><br><span class="line">SCORE: -1 (Weak reject)</span><br><span class="line">----- TEXT:</span><br><span class="line">The work presented gives good results compared to the baselines shown. While the question being tackled is interesting, I found that the paper lacked a flow when I was reading it and I advice to paper be rewritten for better understanding based on the mentioned weaknesses.</span><br></pre></td></tr></table></figure><p>EMNLP-IJCNLP 2019 （2019-08-13）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><span class="line">Dear Mingchen Li:</span><br><span class="line">We are sorry to inform you that the following submission was not selected by the program committee to appear at EMNLP-IJCNLP 2019: </span><br><span class="line"></span><br><span class="line">        Plentiful Knowledge Vitalized Attention And Neural Network For Short Text Classification </span><br><span class="line"></span><br><span class="line">Especially in light of the record number of submissions, the selection process was very competitive. Due to time and space limitations, we could only choose a small number of the submitted papers to appear in the program. </span><br><span class="line"></span><br><span class="line">Our selection process was not based solely on the reviewers assessment and scores, but also on discussions among reviewers, careful assessment by Area Chairs and Senior Area Chairs, and our goal to assemble a varied, interesting and high quality program. </span><br><span class="line"></span><br><span class="line">While it can be very disappointing to receive a notification of rejection of a paper on which you have worked hard, please keep in mind that a rejection does not necessarily mean that your paper is not good enough. In many cases rejected papers are not quite ready for publication in an archival venue, but the next version may be, leading to significantly more impact on the field. We hope that the reviewers&#39; comments will be useful to you in this regard. </span><br><span class="line"></span><br><span class="line">Our program committee and chairs worked hard to collect substantive feedback for each paper, and we hope that your paper finds success at another venue given the recommended changes. Reviewer comments are enclosed together with the meta-review written by the Area Chair who handled your submission. Note that it is possible for your paper to have a positive meta-review despite its being rejected. This happens precisely when the Area Chair&#39;s positive recommendation was overridden by the Senior Area Chair and&#x2F;or the Program Chairs after ranking your submission against other submissions that were reviewed in the same track as yours. </span><br><span class="line"></span><br><span class="line">Best Regards, </span><br><span class="line">Jing Jiang, Vincent Ng, and Xiaojun Wan, Program Co-Chairs, EMNLP-IJCNLP 2019</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; </span><br><span class="line">EMNLP-IJCNLP 2019 Reviews for Submission #450</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; </span><br><span class="line"></span><br><span class="line">Title: Plentiful Knowledge Vitalized Attention And Neural Network For Short Text Classification</span><br><span class="line">Authors: Mingchen Li, L.M. Gabtone, Yijia MIAO and Feng Gao</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">                            META-REVIEW</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; </span><br><span class="line"></span><br><span class="line">Comments: This paper proposes to use external knowledge from Wikidata to obtain better short document representations for text classification. Unfortunately, the paper needs to be improved a lot in terms of its writing and the presentation of the method. It also should compare against stronger baselines.</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">                            REVIEWER #1</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">Short text classification is an important task in many text mining&#x2F;NLP fields. This paper proposes to exploit the external knowledge base to enrich the semantic information underlying a given shot document for better classification. The main contribution lies at the utilization of the parent concept information extracted from Wikidata to enrich the semantic representation for a given short document. It is well validated that the parent concepts relevant to a short document carry many useful features to enhance short text classification. The experimental results over five real-world datasets demonstrate the superiority of the proposed solution against several neural techniques for short text classification. </span><br><span class="line"></span><br><span class="line">Overall, this paper is poorly prepared and written. There are many obscure and inappropriate statements, and many grammatical errors. The most vital issue is that several critical technical details are totally missing. It is hard to know how to reimplement the proposed solution for further comparison or application. Also, the baselines in comparison are somehow weak for short text classification. Several state-of-the-art techniques are totally ignored for discussion and comparison.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reasons to accept</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">The proposed solution to exploit the existing knowledge base to enrich the understanding of the short documents is somehow reasonable. The exploration of the parent concepts to enhance short text representation is interesting and novel to some extent.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reasons to reject</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">1. The paper writing is very bad. The authors should at least peform careful proofreading to eliminate the grammatical errors.</span><br><span class="line">2. Many technical details are not well presented. It is hard to know how to achieve text conceptualization? what methods are used to derive the graph embeddings and concept embeddings respectively? The formulation regarding the training phase is somehow problematic, since the meaning of vector q is not mentioned. To just name a few important concerns.</span><br><span class="line">3. The baselines used in the evaluation is somehow weak, and not in line with the proposed solution. More methods utilizing the knowledge base or the external knowledge to enhance short text classification should be compared for performance evaluation.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">Reviewer&#39;s Scores</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">                  Overall Recommendation: 1.5</span><br><span class="line"></span><br><span class="line">Questions for the Author(s)</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">1. What method&#x2F;tool is used to achieve text conceptualization?  According to &quot;I: self concept&quot;, each word in the document is mapped as a concept from Wikidata, how to achieve this procedure? How to derive the concept embeddings? Are conceptual embedded vectors Y and the sentence&#39;s conceptual graph vectors G different?</span><br><span class="line">2. For a given concept, there would be more than one parent concept in Wikidata. How did the authors handle this one-to-many relations?</span><br><span class="line">3. Does the attention mechanism utilized in both word feature representation and concept feature representation share the same parameters? According to Equation 4-5 and Equation 10-11, the parameters like A_w, A_b and A_u are reused in the two attention networks.</span><br><span class="line">4. What is the meaning of vector q in Equation 14?, is it equivalent to tr? And the probability vector e is not used at all.</span><br><span class="line">5. A live example &quot;He can&#39;t bear to be laughed at&quot; is used in Section 1 to explain the weakness of implicit representation. The authors explained that word &quot;bear&quot; can be identified as a noun. However, I hold a different opinion regarding this example. It is clear that we can derive word &quot;bear&quot; as a verb by considering the contextual information like &quot;to be laughed&quot; and &quot;He can&#39;t&quot;.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Missing References</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">The comparative baselines are quite weak. Several SOTA alternatives are listed as below:</span><br><span class="line">1. Jichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Michael R. Lyu, Irwin King:</span><br><span class="line">Topic Memory Networks for Short Text Classification. EMNLP 2018: 3120-3131</span><br><span class="line">2. Changchun Li, Jihong Ouyang, Ximing Li:</span><br><span class="line">Classifying Extremely Short Texts by Exploiting Semantic Centroids in Word Mover&#39;s Distance Space. WWW 2019: 939-949</span><br><span class="line">3. Peng Wang, Bo Xu, Jiaming Xu, Guanhua Tian, Cheng-Lin Liu, Hongwei Hao:</span><br><span class="line">Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification. Neurocomputing 174: 806-814 (2016)</span><br><span class="line">4. William Lucia, Elena Ferrari:</span><br><span class="line">EgoCentric: Ego Networks for Knowledge-based Short Text Classification. CIKM 2014: 1079-1088</span><br><span class="line">5. Guodong Long, Ling Chen, Xingquan Zhu, Chengqi Zhang:</span><br><span class="line">TCSST: transfer classification of short &amp; sparse text using external data. CIKM 2012: 764-772</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Typos, Grammar, Style, and Presentation Improvements</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">1. Page 1: The initial letter in &quot;explicit representation&quot; and &quot;implicit representation&quot; should be capitalized.</span><br><span class="line">2. Page 2: &quot;which same as pre-trained&quot; same can not be used as a verb here.</span><br><span class="line">3. Page 2: &quot;concept featuer representation&quot; representation -&gt; representations</span><br><span class="line">4. Page 2: &quot;we inspired&quot; -&gt; &quot;we are inspired&quot;</span><br><span class="line">5. Page 3: &quot;proposed attention mechanism can be used in&quot; mechanism can -&gt; mechanism that can</span><br><span class="line">6. Page 3: &quot;by retrival knowledge graph&quot; retrival -&gt; retrieving</span><br><span class="line">7: Page 4: &quot;are concatenated to h_n&quot; -&gt; to form h_n; &quot;h_n by function 4&quot; -&gt; by Equation 4; &quot;which represent&quot; -&gt; which represents; &quot;Concept feature Representation&quot; -&gt; &quot;Concept Feature Representation&quot;</span><br><span class="line">8:  Page 5: &quot;are concatenated to h_n&quot; -&gt; to form h_n; &quot;which represent&quot; -&gt; which represents</span><br><span class="line">9: Page 6: &quot;Same as SST-2&quot; -&gt; Same as SST-1; &quot;dropout rate of 0.5, learning rate of 0.001, batch size of 64&quot;, please make these sentences correct.</span><br><span class="line">10. Page 7: The fill patterns should be used to help discriminate the two settings in Figure 4.</span><br><span class="line">11. Page 8: &quot;these two type word embedding&quot; embedding -&gt; embeddings; &quot;knowledge vitalize&quot; vitalize -&gt; vitalized; &quot;for rich text&#39;s sematnic&quot; rich -&gt; enriching</span><br><span class="line">12. Page 8: &quot;the main reason is that the insufficient sample and there are some unknown words in two word embeddings&quot;. Please rephrase this sentence, it is hard to understand.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">                            REVIEWER #2</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">This paper studies short text classification, which is an important problem in NLP.</span><br><span class="line"></span><br><span class="line">Strengths:</span><br><span class="line">1. Important problem.</span><br><span class="line">2. Intuitive method.</span><br><span class="line">3. Good performance.</span><br><span class="line"></span><br><span class="line">Weaknesses:</span><br><span class="line">1. The novelty is limited.</span><br><span class="line">2. The writing can be further improved.</span><br><span class="line">3. The experiment is not convincing enough.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reasons to accept</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">The problem studied in this paper is an important in the NLP community. The proposed model is quite intuitive. The basic idea is to learn sentence encodings from both words and concepts in sentences by using Bi-GRU and attention mechanisms. Experiment on five benchmark datasets proves the effectiveness of the proposed model.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reasons to reject</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">The paper has some limitations. First, the novelty is limited. The key idea of the paper is to combine information from both words and concepts in sentences. However, there are already many existing studies exploring this direction. Given these studies, the idea is not very novel. From the model perspective, the authors combine Bi-GRU and attention mechanisms in a quite straightforward way, from which we cannot get many new insights. Second, there are many typos in the paper, and the writing can be further improved. Third, the experiment is not convincing enough. In experiments, the authors only compare with 6 baseline methods, and most of them are based on CNN. It would be more convincing if the authors could compare with more baseline methods.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">Reviewer&#39;s Scores</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">                  Overall Recommendation: 2.5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">                            REVIEWER #3</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">This paper proposes a method based on plentiful knowledge for short text classification.</span><br><span class="line"></span><br><span class="line">** Strength **</span><br><span class="line">The work facilitates classification by taking parent-child relation in KB and exploiting parent information.</span><br><span class="line">Experiments are made to verify the proposed method.</span><br><span class="line"></span><br><span class="line">** Weakness **</span><br><span class="line">The main idea of the work is incorporating KB to facilitate classification. The contribution may be not strong.</span><br><span class="line">For the second contribution, the authors declare that “this is the first work to incorporate the parent concept into text representation”. In my opinion, the main idea of embedding is to utilize contexts, including parent concept, to represent text.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Reasons to reject</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">The main contributions may be not very strong.</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">Reviewer&#39;s Scores</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">                  Overall Recommendation: 2.5</span><br><span class="line"></span><br><span class="line">Questions for the Author(s)</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">Why child concepts are not used into text representation?</span><br><span class="line">Do all baselines use the same pre-trained vector?</span><br><span class="line">Why do the authors not analyze the affection of parent concepts on the final classification performance in experiments?</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Typos, Grammar, Style, and Presentation Improvements</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">067, “And then the text feature were extracted”</span><br><span class="line">130, “Bob’s teacher want”</span><br><span class="line">586, “we use the same model configuration as anthor”</span><br><span class="line">624, “The goal of this section to describe the parent conceptual information can help…”</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- </span><br><span class="line">EMNLP-IJCNLP 2019 - https:&#x2F;&#x2F;www.softconf.com&#x2F;emnlp2019&#x2F;papers</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The rebuttals of ECAI2020 will begin</title>
      <link href="2019/12/18/The-rebuttals-of-ECAI2020-will-begin/"/>
      <url>2019/12/18/The-rebuttals-of-ECAI2020-will-begin/</url>
      
        <content type="html"><![CDATA[<p>18 December 2019: Rebuttals (for scientific papers) start (00:00 UTC-12)</p><p>20 December 2019 — Rebuttals (for scientific papers) end</p><p>15 January 2020 — Notification of acceptance/rejection</p><p>27 February 2020 — Camera Ready papers</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>analyse the ABCNN</title>
      <link href="2019/12/11/analyse-the-ABCNN/"/>
      <url>2019/12/11/analyse-the-ABCNN/</url>
      
        <content type="html"><![CDATA[<p>In this blog, I analyse the code about ABCNN, including the output for every step, some important functions. I only put the class ABCNN() in this blog, if you want to need the entire code, please contact me. ABCNN is the first model to combine CNN and attention (from two location: input, output). It’s an interesting model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ABCNN():</span><br><span class="line">    def __init__(self, s, w, l2_reg, model_type, num_features, d0&#x3D;300, di&#x3D;50, num_classes&#x3D;3, num_layers&#x3D;1):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Implmenentaion of ABCNNs</span><br><span class="line">        (https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1512.05193.pdf)</span><br><span class="line"></span><br><span class="line">        :param s: sentence length</span><br><span class="line">        :param w: filter width</span><br><span class="line">        :param l2_reg: L2 regularization coefficient</span><br><span class="line">        :param model_type: Type of the network(BCNN, ABCNN1, ABCNN2, ABCNN3).</span><br><span class="line">        :param num_features: The number of pre-set features(not coming from CNN) used in the output layer.</span><br><span class="line">        :param d0: dimensionality of word embedding(default: 300)</span><br><span class="line">        :param di: The number of convolution kernels (default: 50)</span><br><span class="line">        :param num_classes: The number of classes for answers.</span><br><span class="line">        :param num_layers: The number of convolution layers.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        self.x1 &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, d0, s], name&#x3D;&quot;x1&quot;)</span><br><span class="line">        self.x2 &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, d0, s], name&#x3D;&quot;x2&quot;)</span><br><span class="line">        self.y &#x3D; tf.placeholder(tf.int32, shape&#x3D;[None], name&#x3D;&quot;y&quot;)</span><br><span class="line">        # self.features &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, num_features], name&#x3D;&quot;features&quot;)</span><br><span class="line"></span><br><span class="line">        # zero padding to inputs for wide convolution</span><br><span class="line">        def pad_for_wide_conv(x):</span><br><span class="line">            return tf.pad(x, np.array([[0, 0], [0, 0], [w - 1, w - 1], [0, 0]]), &quot;CONSTANT&quot;, name&#x3D;&quot;pad_wide_conv&quot;)   # 4个维度填充</span><br><span class="line"></span><br><span class="line">        def cos_sim(v1, v2):</span><br><span class="line">            norm1 &#x3D; tf.sqrt(tf.reduce_sum(tf.square(v1), axis&#x3D;1))</span><br><span class="line">            norm2 &#x3D; tf.sqrt(tf.reduce_sum(tf.square(v2), axis&#x3D;1))</span><br><span class="line">            dot_products &#x3D; tf.reduce_sum(v1 * v2, axis&#x3D;1, name&#x3D;&quot;cos_sim&quot;)</span><br><span class="line"></span><br><span class="line">            return dot_products &#x2F; (norm1 * norm2)</span><br><span class="line"></span><br><span class="line">        def euclidean_score(v1, v2):</span><br><span class="line">            euclidean &#x3D; tf.sqrt(tf.reduce_sum(tf.square(v1 - v2), axis&#x3D;1))</span><br><span class="line">            return 1 &#x2F; (1 + euclidean)</span><br><span class="line"></span><br><span class="line">        def make_attention_mat(x1, x2):</span><br><span class="line">            # x1, x2 &#x3D; [batch, height, width, 1] &#x3D; [batch, d, s, 1]</span><br><span class="line">            # x2 &#x3D;&gt; [batch, height, 1, width]</span><br><span class="line">            # [batch, width, wdith] &#x3D; [batch, s, s]</span><br><span class="line">            euclidean &#x3D; tf.sqrt(tf.reduce_sum(tf.square(x1 - tf.matrix_transpose(x2)), axis&#x3D;1))</span><br><span class="line">            return 1 &#x2F; (1 + euclidean)</span><br><span class="line"></span><br><span class="line">        def convolution(name_scope, x, d, reuse):</span><br><span class="line">            with tf.name_scope(name_scope + &quot;-conv&quot;):</span><br><span class="line">                with tf.variable_scope(&quot;conv&quot;) as scope:</span><br><span class="line">                    conv &#x3D; tf.contrib.layers.conv2d( #创造卷积层</span><br><span class="line">                        inputs&#x3D;x,  #形状为[batch_size, height, width, channels]的输入。</span><br><span class="line">                        num_outputs&#x3D;di,  #代表输出几个channel。这里不需要再指定输入的channel了，因为函数会自动根据inpus的shpe去判断</span><br><span class="line">                        kernel_size&#x3D;(d, w), #卷积核大小，不需要带上batch和channel，只需要输入尺寸即可。[5,5]就代表5x5的卷积核，如果长和宽都一样，也可以只写一个数5.</span><br><span class="line">                        stride&#x3D;1, #步长，默认是长宽都相等的步长。卷积时，一般都用1，所以默认值也是1.如果长和宽都不相等，也可以用一个数组[1,2]。</span><br><span class="line">                        padding&#x3D;&quot;VALID&quot;, #填充方式，&#39;SAME&#39;或者&#39;VALID&#39;。 https:&#x2F;&#x2F;blog.csdn.net&#x2F;leviopku&#x2F;article&#x2F;details&#x2F;80327478</span><br><span class="line">                        activation_fn&#x3D;tf.nn.tanh, #激活函数。默认是ReLU。也可以设置为None</span><br><span class="line">                        weights_initializer&#x3D;tf.contrib.layers.xavier_initializer_conv2d(), #权重的初始化，默认为initializers.xavier_initializer()函数。</span><br><span class="line">                        weights_regularizer&#x3D;tf.contrib.layers.l2_regularizer(scale&#x3D;l2_reg), #权重正则化项，可以加入正则函数。biases_initializer：偏置的初始化，默认为init_ops.zeros_initializer()函数。</span><br><span class="line">                        biases_initializer&#x3D;tf.constant_initializer(1e-04),#偏置初始化</span><br><span class="line">                        reuse&#x3D;reuse,</span><br><span class="line">                        trainable&#x3D;True, #是否可训练，如作为训练节点，必须设置为True，默认即可。如果我们是微调网络，有时候需要冻结某一层的参数，则设置为False。</span><br><span class="line">                        scope&#x3D;scope</span><br><span class="line">                    )</span><br><span class="line">                    # Weight: [filter_height, filter_width, in_channels, out_channels]</span><br><span class="line">                    # output: [batch, 1, input_width+filter_Width-1, out_channels] &#x3D;&#x3D; [batch, 1, s+w-1, di]</span><br><span class="line"></span><br><span class="line">                    # [batch, di, s+w-1, 1]</span><br><span class="line">                    conv_trans &#x3D; tf.transpose(conv, [0, 3, 2, 1], name&#x3D;&quot;conv_trans&quot;)</span><br><span class="line">                    return conv_trans</span><br><span class="line"></span><br><span class="line">        def w_pool(variable_scope, x, attention):</span><br><span class="line">            # x: [batch, di, s+w-1, 1]</span><br><span class="line">            # attention: [batch, s+w-1]</span><br><span class="line">            with tf.variable_scope(variable_scope + &quot;-w_pool&quot;):</span><br><span class="line">                if model_type &#x3D;&#x3D; &quot;ABCNN2&quot; or model_type &#x3D;&#x3D; &quot;ABCNN3&quot;:</span><br><span class="line">                    pools &#x3D; []</span><br><span class="line">                    # [batch, s+w-1] &#x3D;&gt; [batch, 1, s+w-1, 1]</span><br><span class="line">                    attention &#x3D; tf.transpose(tf.expand_dims(tf.expand_dims(attention, -1), -1), [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">                    for i in range(s):</span><br><span class="line">                        # [batch, di, w, 1], [batch, 1, w, 1] &#x3D;&gt; [batch, di, 1, 1]</span><br><span class="line">                        pools.append(tf.reduce_sum(x[:, :, i:i + w, :] * attention[:, :, i:i + w, :],</span><br><span class="line">                                                   axis&#x3D;2,</span><br><span class="line">                                                   keep_dims&#x3D;True))</span><br><span class="line"></span><br><span class="line">                    # [batch, di, s, 1]</span><br><span class="line">                    w_ap &#x3D; tf.concat(pools, axis&#x3D;2, name&#x3D;&quot;w_ap&quot;)</span><br><span class="line">                #---------------over------------------</span><br><span class="line">                else:</span><br><span class="line">                    w_ap &#x3D; tf.layers.average_pooling2d(</span><br><span class="line">                        inputs&#x3D;x,</span><br><span class="line">                        # (pool_height, pool_width)</span><br><span class="line">                        pool_size&#x3D;(1, w),</span><br><span class="line">                        strides&#x3D;1,</span><br><span class="line">                        padding&#x3D;&quot;VALID&quot;,</span><br><span class="line">                        name&#x3D;&quot;w_ap&quot;</span><br><span class="line">                    )</span><br><span class="line">                    # [batch, di, s, 1]</span><br><span class="line"></span><br><span class="line">                return w_ap</span><br><span class="line"></span><br><span class="line">        def all_pool(variable_scope, x):  # variable_scope&#x3D;&quot;input-left&quot;   or &quot;input-right&quot;   x:shape&#x3D;(?, 300, 20, 1)</span><br><span class="line">            with tf.variable_scope(variable_scope + &quot;-all_pool&quot;):</span><br><span class="line">                if variable_scope.startswith(&quot;input&quot;):</span><br><span class="line">                    pool_width &#x3D; s</span><br><span class="line">                    d &#x3D; d0</span><br><span class="line">                else:</span><br><span class="line">                    pool_width &#x3D; s + w - 1</span><br><span class="line">                    d &#x3D; di</span><br><span class="line"></span><br><span class="line">                all_ap &#x3D; tf.layers.average_pooling2d(</span><br><span class="line">                    inputs&#x3D;x,  #要在池上的张量,它的秩必须为4.</span><br><span class="line">                    # (pool_height, pool_width)</span><br><span class="line">                    pool_size&#x3D;(1, pool_width),  #2个整数的整数或元组&#x2F;列表：(pool_height,pool_width),指定池窗口的大小；可以是单个整数,以指定所有空间维度的相同值.</span><br><span class="line">                    strides&#x3D;1,</span><br><span class="line">                    padding&#x3D;&quot;VALID&quot;,</span><br><span class="line">                    name&#x3D;&quot;all_ap&quot;</span><br><span class="line">                )</span><br><span class="line">                # [batch, di, 1, 1]</span><br><span class="line"></span><br><span class="line">                # [batch, di]</span><br><span class="line">                # print(&quot;all_ap&quot;,all_ap) #Tensor(&quot;input-left-all_pool&#x2F;all_ap&#x2F;AvgPool:0&quot;, shape&#x3D;(?, 300, 1, 1), dtype&#x3D;float32)</span><br><span class="line">                all_ap_reshaped &#x3D; tf.reshape(all_ap, [-1, d]) #all_ap_reshaped Tensor(&quot;input-left-all_pool&#x2F;Reshape:0&quot;, shape&#x3D;(?, 300), dtype&#x3D;float32)</span><br><span class="line"></span><br><span class="line">                #all_ap_reshaped &#x3D; tf.squeeze(all_ap, [2, 3])</span><br><span class="line"></span><br><span class="line">                return all_ap_reshaped</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        def CNN_layer(variable_scope, x1, x2, d):# LI_1, LO_1, RI_1, RO_1 &#x3D; CNN_layer(variable_scope&#x3D;&quot;CNN-1&quot;, x1&#x3D;x1_expanded, x2&#x3D;x2_expanded, d&#x3D;d0)  #x1:shape&#x3D;(?, 300, 20, 1)</span><br><span class="line">            print(&quot;----------开始cnn模型-------------&quot;)</span><br><span class="line">            # x1, x2 &#x3D; [batch, d, s, 1]</span><br><span class="line">            with tf.variable_scope(variable_scope):</span><br><span class="line">                if model_type &#x3D;&#x3D; &quot;ABCNN1&quot; or model_type &#x3D;&#x3D; &quot;ABCNN3&quot;:</span><br><span class="line">                    with tf.name_scope(&quot;att_mat&quot;):</span><br><span class="line">                        aW &#x3D; tf.get_variable(name&#x3D;&quot;aW&quot;,</span><br><span class="line">                                             shape&#x3D;(s, d),</span><br><span class="line">                                             initializer&#x3D;tf.contrib.layers.xavier_initializer(),</span><br><span class="line">                                             regularizer&#x3D;tf.contrib.layers.l2_regularizer(scale&#x3D;l2_reg))</span><br><span class="line"></span><br><span class="line">                        # [batch, s, s]</span><br><span class="line">                        att_mat &#x3D; make_attention_mat(x1, x2)</span><br><span class="line"></span><br><span class="line">                        # [batch, s, s] * [s,d] &#x3D;&gt; [batch, s, d]</span><br><span class="line">                        # matrix transpose &#x3D;&gt; [batch, d, s]</span><br><span class="line">                        # expand dims &#x3D;&gt; [batch, d, s, 1]</span><br><span class="line">                        x1_a &#x3D; tf.expand_dims(tf.matrix_transpose(tf.einsum(&quot;ijk,kl-&gt;ijl&quot;, att_mat, aW)), -1)</span><br><span class="line">                        x2_a &#x3D; tf.expand_dims(tf.matrix_transpose(</span><br><span class="line">                            tf.einsum(&quot;ijk,kl-&gt;ijl&quot;, tf.matrix_transpose(att_mat), aW)), -1)</span><br><span class="line"></span><br><span class="line">                        # [batch, d, s, 2]</span><br><span class="line">                        x1 &#x3D; tf.concat([x1, x1_a], axis&#x3D;3)</span><br><span class="line">                        x2 &#x3D; tf.concat([x2, x2_a], axis&#x3D;3)</span><br><span class="line"></span><br><span class="line">                left_conv &#x3D; convolution(name_scope&#x3D;&quot;left&quot;, x&#x3D;pad_for_wide_conv(x1), d&#x3D;d, reuse&#x3D;False)</span><br><span class="line">                right_conv &#x3D; convolution(name_scope&#x3D;&quot;right&quot;, x&#x3D;pad_for_wide_conv(x2), d&#x3D;d, reuse&#x3D;True)</span><br><span class="line"></span><br><span class="line">                left_attention, right_attention &#x3D; None, None</span><br><span class="line"></span><br><span class="line">                if model_type &#x3D;&#x3D; &quot;ABCNN2&quot; or model_type &#x3D;&#x3D; &quot;ABCNN3&quot;:</span><br><span class="line">                    # [batch, s+w-1, s+w-1]</span><br><span class="line">                    att_mat &#x3D; make_attention_mat(left_conv, right_conv)</span><br><span class="line">                    # [batch, s+w-1], [batch, s+w-1]</span><br><span class="line">                    left_attention, right_attention &#x3D; tf.reduce_sum(att_mat, axis&#x3D;2), tf.reduce_sum(att_mat, axis&#x3D;1)</span><br><span class="line"></span><br><span class="line">                left_wp &#x3D; w_pool(variable_scope&#x3D;&quot;left&quot;, x&#x3D;left_conv, attention&#x3D;left_attention)</span><br><span class="line"></span><br><span class="line">                left_ap &#x3D; all_pool(variable_scope&#x3D;&quot;left&quot;, x&#x3D;left_conv)</span><br><span class="line">                right_wp &#x3D; w_pool(variable_scope&#x3D;&quot;right&quot;, x&#x3D;right_conv, attention&#x3D;right_attention)</span><br><span class="line">                right_ap &#x3D; all_pool(variable_scope&#x3D;&quot;right&quot;, x&#x3D;right_conv)</span><br><span class="line"></span><br><span class="line">                return left_wp, left_ap, right_wp, right_ap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;类里面的主函数在此开始&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;&quot;&quot;</span><br><span class="line">        x1_expanded &#x3D; tf.expand_dims(self.x1, -1)  # x1_shape&#x3D;[None, d0, s]   d0: 300维  s:句子长度    shape(x1_expanded):Tensor(&quot;ExpandDims:0&quot;, shape&#x3D;(?, 300, 20, 1), dtype&#x3D;float32)</span><br><span class="line">        x2_expanded &#x3D; tf.expand_dims(self.x2, -1)</span><br><span class="line"></span><br><span class="line">        LO_0 &#x3D; all_pool(variable_scope&#x3D;&quot;input-left&quot;, x&#x3D;x1_expanded)</span><br><span class="line">        RO_0 &#x3D; all_pool(variable_scope&#x3D;&quot;input-right&quot;, x&#x3D;x2_expanded)</span><br><span class="line">        # print(&quot;cos&quot;,cos_sim(LO_0, RO_0))#shape&#x3D;(?,)</span><br><span class="line"></span><br><span class="line">        # CNN_layer(variable_scope&#x3D;&quot;CNN-1&quot;, x1&#x3D;x1_expanded, x2&#x3D;x2_expanded, d&#x3D;d0)</span><br><span class="line">        LI_1, LO_1, RI_1, RO_1 &#x3D; CNN_layer(variable_scope&#x3D;&quot;CNN-1&quot;, x1&#x3D;x1_expanded, x2&#x3D;x2_expanded, d&#x3D;d0)  #x1:shape&#x3D;(?, 300, 20, 1)</span><br><span class="line">        sims &#x3D; [cos_sim(LO_0, RO_0), cos_sim(LO_1, RO_1)]</span><br><span class="line">        print(&quot;cos_sim(LO_1, RO_1)&quot;,cos_sim(LO_1, RO_1))#cos_sim(LO_1, RO_1) Tensor(&quot;truediv_2:0&quot;, shape&#x3D;(?,), dtype&#x3D;float32)</span><br><span class="line">        print(&quot;cos_sim(LO_0, RO_0)&quot;,cos_sim(LO_0, RO_0))#cos_sim(LO_0, RO_0) Tensor(&quot;truediv_3:0&quot;, shape&#x3D;(?,), dtype&#x3D;float32)</span><br><span class="line"></span><br><span class="line">        # if num_layers &gt; 1:</span><br><span class="line">        #     _, LO_2, _, RO_2 &#x3D; CNN_layer(variable_scope&#x3D;&quot;CNN-2&quot;, x1&#x3D;LI_1, x2&#x3D;RI_1, d&#x3D;di)</span><br><span class="line">        #     self.test &#x3D; LO_2</span><br><span class="line">        #     self.test2 &#x3D; RO_2</span><br><span class="line">        #     sims.append(cos_sim(LO_2, RO_2))</span><br><span class="line">        # print(&quot;sims&quot;,sims)</span><br><span class="line">        with tf.variable_scope(&quot;output-layer&quot;):</span><br><span class="line">            # self.output_features &#x3D; tf.concat([self.features, tf.stack(sims, axis&#x3D;1)], axis&#x3D;1, name&#x3D;&quot;output_features&quot;)#Tensor(&quot;output-layer&#x2F;output_features:0&quot;, shape&#x3D;(?, 6), dtype&#x3D;float32)</span><br><span class="line">            self.output_features &#x3D; (tf.stack(sims, axis&#x3D;1))  # Tensor(&quot;output-layer&#x2F;output_features:0&quot;, shape&#x3D;(?, 2), dtype&#x3D;float32)</span><br><span class="line">            print(&quot;self.output_features&quot;,self.output_features)</span><br><span class="line"></span><br><span class="line">            self.estimation &#x3D; tf.contrib.layers.fully_connected(#Tensor(&quot;output-layer&#x2F;FC&#x2F;BiasAdd:0&quot;, shape&#x3D;(?, 3), dtype&#x3D;float32)</span><br><span class="line">                inputs&#x3D;self.output_features,</span><br><span class="line">                num_outputs&#x3D;num_classes,</span><br><span class="line">                activation_fn&#x3D;None,</span><br><span class="line">                weights_initializer&#x3D;tf.contrib.layers.xavier_initializer(),</span><br><span class="line">                weights_regularizer&#x3D;tf.contrib.layers.l2_regularizer(scale&#x3D;l2_reg),</span><br><span class="line">                biases_initializer&#x3D;tf.constant_initializer(1e-04),</span><br><span class="line">                scope&#x3D;&quot;FC&quot;</span><br><span class="line">            )</span><br><span class="line">            print(&quot;self.estimation&quot;,self.estimation)</span><br><span class="line">        #</span><br><span class="line">        self.prediction &#x3D; tf.contrib.layers.softmax(self.estimation)[:, 1]</span><br><span class="line">        print(&quot;self.prediction&quot;,self.prediction)#Tensor(&quot;strided_slice:0&quot;, shape&#x3D;(?,), dtype&#x3D;float32)</span><br><span class="line">        self.cost &#x3D; tf.add(</span><br><span class="line">            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits&#x3D;self.estimation, labels&#x3D;self.y)),</span><br><span class="line">            tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)),</span><br><span class="line">            name&#x3D;&quot;cost&quot;)</span><br><span class="line">        #</span><br><span class="line">        tf.summary.scalar(&quot;cost&quot;, self.cost)</span><br><span class="line">        self.merged &#x3D; tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">        print(&quot;&#x3D;&quot; * 50)</span><br><span class="line">        print(&quot;List of Variables:&quot;)</span><br><span class="line">        for v in tf.trainable_variables():</span><br><span class="line">            print(v.name)</span><br><span class="line">        print(&quot;&#x3D;&quot; * 50)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>classification for physical science</title>
      <link href="2019/12/11/classification-for-physical-science/"/>
      <url>2019/12/11/classification-for-physical-science/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">root_path&#x3D;os.getcwd()  # this dic</span><br><span class="line">print(root_path)</span><br><span class="line"># 以空格来划分每一个分词</span><br><span class="line">def preprocess(sentence):</span><br><span class="line">    text_with_space &#x3D; &quot;&quot;</span><br><span class="line"></span><br><span class="line">    textcute &#x3D; jieba.cut(sentence)</span><br><span class="line">    for word in textcute:</span><br><span class="line">        text_with_space +&#x3D; word + &quot; &quot;</span><br><span class="line">        # print(text_with_space)</span><br><span class="line">    return text_with_space</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loadtrainset(path):</span><br><span class="line"></span><br><span class="line">    processed_textset &#x3D; []</span><br><span class="line">    allclasstags &#x3D; []</span><br><span class="line">    with open(path,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line&#x3D;line.strip().split(&quot;\t&quot;)</span><br><span class="line"></span><br><span class="line">            processed_textset.append(preprocess(line[0]))</span><br><span class="line">            allclasstags.append(line[1])</span><br><span class="line">    return processed_textset,allclasstags</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    train_data, classtags_list&#x3D;loadtrainset(&quot;5_class_etc.txt&quot;)#&#123;&#39;体育&#39;, &#39;法律&#39;, &#39;经济&#39;, &#39;道德&#39;, &#39;文化&#39;, &#39;政治&#39;&#125;</span><br><span class="line">    print(classtags_list)</span><br><span class="line">    count_vector &#x3D; CountVectorizer()</span><br><span class="line">    vecot_matrix &#x3D; count_vector.fit_transform(train_data)</span><br><span class="line">    # print(train_tfidf)  # vecot_matrix输入，得到词频矩阵</span><br><span class="line">    train_tfidf &#x3D; TfidfTransformer().fit_transform(vecot_matrix)</span><br><span class="line"></span><br><span class="line">    clf &#x3D; MultinomialNB().fit(train_tfidf, classtags_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    true&#x3D;[]</span><br><span class="line">    predict&#x3D;[]</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    test_class.txt:</span><br><span class="line">    用6-70年代的眼光看中国，是你们的无知，是愚蠢，以为打压诬陷就能击垮我过世界冠军，愚蠢至极！政治</span><br><span class="line">    孙杨好样的！！！我喜欢大白杨 但我知道中国人也喜欢他。。。孙杨你不是一个人在比赛…中国人民为你骄傲 加油 外国人鄙视👎👎👎👎鄙视 同意点赞[赞]政治</span><br><span class="line">    正义可能会迟到！但绝对不会缺席！只要你是对的，无惧任何不公！你的背后有强大的祖国，有我们每一个中国人的支持！！！政治</span><br><span class="line">    </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    with open(&quot;test_class.txt&quot;,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line&#x3D;line.strip().split(&quot;\t&quot;)</span><br><span class="line">            true.append(line[1])</span><br><span class="line">            text&#x3D;line[0]</span><br><span class="line">            new_count_vector &#x3D; count_vector.transform([text])</span><br><span class="line">            new_tfidf &#x3D; TfidfTransformer().fit_transform(new_count_vector)</span><br><span class="line">            train_tfidf &#x3D; TfidfTransformer().fit_transform(vecot_matrix)</span><br><span class="line">            predict_result &#x3D; clf.predict(new_tfidf)</span><br><span class="line">            # predict_result &#x3D; clf.predict(test_tfidf)</span><br><span class="line">            # print(predict_result)</span><br><span class="line">            predict.append(predict_result[0])</span><br><span class="line">    print(&quot;true&quot;,true)</span><br><span class="line">    print(&quot;predict&quot;,predict)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The collection of my bugs</title>
      <link href="2019/12/10/The-collection-of-my-bugs/"/>
      <url>2019/12/10/The-collection-of-my-bugs/</url>
      
        <content type="html"><![CDATA[<p>In this day, I decide to collect some important bugs in my learning progress, I think bug’s collection is important. Before this, I recorded my bugs in my textbook, but I think it is not convenience. Today is 2019.12.10.</p><p>1: <font color=red size=2>InvalidArgumentError (see above for traceback): Received a label value of 2 which is outside the valid range of [0, 2).  Label values: 2[[node SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at G:\工程\问答\医疗\2020_SIGIR_experiment\1_My_five_model\HWLBSH\ABCNN.py:217) ]]</font></p><pre><code>In the fully_conncted layer, the size of output is the same as the size of num_classs, in my model, its 3. sI made the num_outputs is 3, num_classs is 2. SO, its wrong.</code></pre><p>2：<font color=red size=2>ValueError: Variable conv/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">I want to use two or many CNNs in a model, but when I defined these models, I use the same function, as below，</span><br><span class="line"></span><br><span class="line">        def convolution(name_scope, x, d, reuse,i):</span><br><span class="line">            with tf.name_scope(name_scope + &quot;-conv%s&quot; %str(i)):</span><br><span class="line">                with tf.variable_scope(&quot;conv&quot;) as scope:</span><br><span class="line">                    conv &#x3D; tf.contrib.layers.conv2d(  # 创造卷积层</span><br><span class="line">                        inputs&#x3D;x,  # 形状为[batch_size, height, width, channels]的输入。</span><br><span class="line">                        num_outputs&#x3D;di,  # 代表输出几个channel。这里不需要再指定输入的channel了，因为函数会自动根据inpus的shpe去判断</span><br><span class="line">                        kernel_size&#x3D;(d, w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">but It&#39;s wrong, we must not use the same variable for these models, the right is:</span><br><span class="line"></span><br><span class="line">        def convolution(name_scope, x, d, reuse,i):</span><br><span class="line">            with tf.name_scope(name_scope + &quot;-conv%s&quot; %str(i)):</span><br><span class="line"></span><br><span class="line">                with tf.variable_scope(&quot;conv%s&quot;%str(i)) as scope:</span><br><span class="line"></span><br><span class="line">                    conv &#x3D; tf.contrib.layers.conv2d(  # 创造卷积层</span><br><span class="line">                        inputs&#x3D;x,  # 形状为[batch_size, height, width, channels]的输入。</span><br><span class="line">                        num_outputs&#x3D;di,  # 代表输出几个channel。这里不需要再指定输入的channel了，因为函数会自动根据inpus的shpe去判断</span><br><span class="line">                        kernel_size&#x3D;(d, w),  # 卷积核大小，不需要带上batch和channel，只需要输入尺寸即可。[5,5]就代表5x5的卷积核，如果长和宽都一样，也可以只写一个数5.</span><br><span class="line">                        stride&#x3D;1,  # 步长，默认是长宽都相等的步长。卷积时，一般都用1，所以默认值也是1.如果长和宽都不相等，也可以用一个数组[1,2]。</span><br><span class="line">                        padding&#x3D;&quot;VALID&quot;,</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>3: <font color=red size=2>Tensorflow.python.framework.errors_impl.NotFoundError</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">The path is wrong.</span><br><span class="line"></span><br><span class="line">--csv_input&#x3D;data&#x2F;train_labels.csv</span><br><span class="line"></span><br><span class="line">should actually be:</span><br><span class="line">--csv_input&#x3D;&quot;C:&#x2F;admin&#x2F;directories_in_between&#x2F;data&#x2F;train_labels.csv&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Some code-models about NLP</title>
      <link href="2019/12/08/Some-code-models-about-NLP/"/>
      <url>2019/12/08/Some-code-models-about-NLP/</url>
      
        <content type="html"><![CDATA[<p>1: reload stopwords list</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def load_stopwords():</span><br><span class="line">    s&#x3D;&#123;&#125;.fromkeys([line.strip() for line in open(&#39;stopwords_and_nltk.txt&#39;, &#39;r&#39;,encoding&#x3D;&quot;utf-8&quot;)])</span><br><span class="line">    return s</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    if &quot;再说&quot; in (load_stopwords()):</span><br><span class="line">        print(&quot;gg&quot;)s</span><br></pre></td></tr></table></figure><p>2: jieba</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">def jieba_segment(text):</span><br><span class="line">    s&#x3D;jieba.cut(text)</span><br><span class="line">    return &quot; &quot;.join(s)</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    f&#x3D;&quot;中国人民站起来了&quot;</span><br><span class="line">    print(jieba_segment(f))</span><br></pre></td></tr></table></figure><p>3: to find the same word between question and answer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">word_cnt &#x3D; len([word for word in s1 if (word not in stopwords) and (word in s2)]) </span><br><span class="line"></span><br><span class="line">this function is same as below:</span><br><span class="line"> sd&#x3D;[]</span><br><span class="line"> for word in s1:</span><br><span class="line"> if (word not in stopwords) and (word in s2):</span><br><span class="line">    sd.append(word)</span><br><span class="line"> print(&quot;worddddd&quot;,len(sd))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>4: make a dic for text, tensorflow is a good tool</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.contrib import learn</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def make_dic():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    use tensorflow to make a dic</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x_all&#x3D;[&quot;我是 中国人 你 呢&quot;,&quot;中国人 我是 山东&quot;]</span><br><span class="line">    </span><br><span class="line">    print(&quot;Loading data...&quot;)</span><br><span class="line">    # Build vocabulary</span><br><span class="line">    max_document_length &#x3D; max([len(x.split(&quot; &quot;)) for x in x_all])  # must have</span><br><span class="line"></span><br><span class="line">    vocab_processor &#x3D; learn.preprocessing.VocabularyProcessor(max_document_length)</span><br><span class="line">    vocab_dict &#x3D; vocab_processor.vocabulary_._mapping</span><br><span class="line"></span><br><span class="line">    x &#x3D; np.array(list(vocab_processor.fit_transform(x_all)))  # must have</span><br><span class="line">    dic_&#x3D;dict([(x, y) for (y, x) in vocab_dict.items()])</span><br><span class="line">    print(dic_)</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    make_dic()</span><br><span class="line"></span><br><span class="line">output:-----------</span><br><span class="line">Loading data...</span><br><span class="line">&#123;0: &#39;&lt;UNK&gt;&#39;, 1: &#39;我是&#39;, 2: &#39;中国人&#39;, 3: &#39;你&#39;, 4: &#39;呢&#39;, 5: &#39;山东&#39;&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The best way:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.contrib import learn</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def word_count(file_name):</span><br><span class="line">    import collections</span><br><span class="line">    word_freq &#x3D; collections.defaultdict(int)</span><br><span class="line"></span><br><span class="line">    with open(file_name,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            line &#x3D; line.strip().split(&quot;\t&quot;)</span><br><span class="line">            l&#x3D;line[0]</span><br><span class="line">            for w in l:</span><br><span class="line">                word_freq[w] +&#x3D; 1</span><br><span class="line">    return word_freq</span><br><span class="line"></span><br><span class="line">def build_dict(file_name, min_word_freq&#x3D;10):</span><br><span class="line">    word_freq &#x3D; word_count(file_name)</span><br><span class="line">    word_freq &#x3D; filter(lambda x: x[1] &gt; min_word_freq, word_freq.items()) # filter将词频数量低于指定值的单词删除。</span><br><span class="line">    word_freq_sorted &#x3D; sorted(word_freq, key&#x3D;lambda x: (-x[1], x[0]))</span><br><span class="line">    # key用于指定排序的元素，因为sorted默认使用list中每个item的第一个元素从小到</span><br><span class="line">    #大排列，所以这里通过lambda进行前后元素调序，并对词频去相反数，从而将词频最大的排列在最前面</span><br><span class="line">    words, _ &#x3D; list(zip(*word_freq_sorted))</span><br><span class="line">    word_idx &#x3D; dict(zip(words, range(len(words))))</span><br><span class="line">    word_idx[&#39;&lt;unk&gt;&#39;] &#x3D; len(words) #unk表示unknown，未知单词</span><br><span class="line">    return word_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    # build_dict()</span><br><span class="line">    s&#x3D;build_dict(&quot;WiLi2018_all_clean.txt&quot;)</span><br><span class="line">    print(s)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Other </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def make_dic(self, inputs):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    去除低频词和停用词</span><br><span class="line">    :param inputs: 输入</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    all_words &#x3D; [word for data in inputs for word in data]</span><br><span class="line"></span><br><span class="line">    word_count &#x3D; Counter(all_words)  # 统计词频</span><br><span class="line"></span><br><span class="line">    sort_word_count &#x3D; sorted(word_count.items(), key&#x3D;lambda x: x[1], reverse&#x3D;True) #降序排列</span><br><span class="line"></span><br><span class="line">    # print(sort_word_count)</span><br><span class="line">    # # 制作train数据集的词典</span><br><span class="line">    words &#x3D; [item[0] for item in sort_word_count]</span><br><span class="line"></span><br><span class="line">    # 如果传入了停用词表，则去除停用词</span><br><span class="line">    if self._stop_word_path:</span><br><span class="line">        with open(self._stop_word_path, &quot;r&quot;, encoding&#x3D;&quot;utf8&quot;) as fr:</span><br><span class="line">            stop_words &#x3D; [line.strip() for line in fr.readlines()]</span><br><span class="line">        words &#x3D; [word for word in words if word not in stop_words]</span><br><span class="line"></span><br><span class="line">    return words</span><br></pre></td></tr></table></figure><p>5: reload word2vec by Google</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line"></span><br><span class="line">#加载Google训练的词向量</span><br><span class="line">from numba import jit</span><br><span class="line">import datetime</span><br><span class="line">import gensim</span><br><span class="line">import numpy as np</span><br><span class="line">import datetime</span><br><span class="line">begin&#x3D;datetime.datetime.now()</span><br><span class="line">def cal():</span><br><span class="line">    fw&#x3D;open(&quot;AG_word_Vector.txt&quot;,&quot;w&quot;,encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">    model&#x3D;gensim.models.KeyedVectors.load_word2vec_format(&#39;GoogleNews-vectors-negative300.bin&#39;,binary&#x3D;True)</span><br><span class="line">    vocabulary &#x3D; gensim.models.KeyedVectors.load_word2vec_format(&#39;GoogleNews-vectors-negative300.bin&#39;,binary&#x3D;True).vocab</span><br><span class="line">    # if &quot;good&quot; in model:</span><br><span class="line">    #     print(&quot;好棒&quot;)</span><br><span class="line">    i&#x3D;0</span><br><span class="line">    with open(&quot;AG_word_dic.txt&quot;,&quot;r&quot;,encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line">        for line in fr.readlines():</span><br><span class="line">            i&#x3D;i+1</span><br><span class="line">            print(i)</span><br><span class="line">            line&#x3D;line.strip().split(&quot;\t&quot;)</span><br><span class="line">            if line[0] not in vocabulary:</span><br><span class="line">                vector&#x3D;np.random.uniform(-0.25, 0.25, 300)</span><br><span class="line">                fw.write(line[0]+&quot;\t&quot;)</span><br><span class="line">                for vec in list(vector):</span><br><span class="line">                    fw.write(str(vec)+&quot; &quot;)</span><br><span class="line">                fw.write(&quot;\n&quot;)</span><br><span class="line">            else:</span><br><span class="line">                vector &#x3D; model[line[0]]</span><br><span class="line">                fw.write(line[0]+&quot;\t&quot;)</span><br><span class="line">                for vec in list(vector):</span><br><span class="line">                    fw.write(str(vec)+&quot; &quot;)</span><br><span class="line">                fw.write(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    begin&#x3D;datetime.datetime.now()</span><br><span class="line">    cal()</span><br><span class="line">    end&#x3D;datetime.datetime.now()</span><br><span class="line">    print((end-begin).seconds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>6: np.pad</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">array &#x3D; np.array([[1, 1],[2,2]])</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">((1,1),(2,2))表示在二维数组array第一维（此处便是行）前面填充1行，最后面填充1行；</span><br><span class="line">                 在二维数组array第二维（此处便是列）前面填充2列，最后面填充2列</span><br><span class="line">constant_values&#x3D;(0,3) 表示第一维填充0，第二维填充3</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">ndarray&#x3D;np.pad(array,((0,0),(0,2)),&#39;constant&#39;)</span><br><span class="line"></span><br><span class="line">print(&quot;array&quot;,array)</span><br><span class="line">print(&quot;ndarray&#x3D;&quot;,ndarray)</span><br><span class="line"></span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">array [[1 1]</span><br><span class="line">   [2 2]]</span><br><span class="line">ndarray&#x3D; [[1 1 0 0]</span><br><span class="line">         [2 2 0 0]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>7：one key to many value (2019/12/10)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># one key to many value</span><br><span class="line">d &#x3D; &#123;&#125;</span><br><span class="line">lst &#x3D; [(&quot;中国&quot;,&#39;山东&#39;),(&quot;中国&quot;,&#39;湖南&#39;),(&quot;爱尔兰&quot;,&#39;都柏林&#39;)]</span><br><span class="line">for k,v in lst:</span><br><span class="line">    if k not in d:</span><br><span class="line">        d[k]&#x3D;[]</span><br><span class="line">    d[k].append(v)</span><br><span class="line"></span><br><span class="line">print(d) # &#123;1: [&#39;apple&#39;, &#39;compute&#39;], 2: [&#39;orange&#39;]&#125;</span><br><span class="line"></span><br><span class="line"># methods defaultdict</span><br><span class="line">from collections import defaultdict</span><br><span class="line">d &#x3D; defaultdict(list)</span><br><span class="line">for k,v in lst:</span><br><span class="line">    d[k].append(v)</span><br><span class="line"></span><br><span class="line">print(d)</span><br><span class="line"></span><br><span class="line">output: &#123;&#39;中国&#39;: [&#39;山东&#39;, &#39;湖南&#39;], &#39;爱尔兰&#39;: [&#39;都柏林&#39;]&#125;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>7： Counter in many lists</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">from collections import Counter</span><br><span class="line"></span><br><span class="line">def sum_(a,b):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ca &#x3D; Counter(a)</span><br><span class="line">    cb &#x3D; Counter(b)</span><br><span class="line">    return ca+cb</span><br><span class="line"></span><br><span class="line">def sumc(*c): # when *c , it refers to a tuples.    when **c  it refers to a dic</span><br><span class="line">    if (len(c) &lt; 1):</span><br><span class="line">        return</span><br><span class="line">    mapc &#x3D; map(Counter, c)</span><br><span class="line">    s &#x3D; Counter([])</span><br><span class="line">    for ic in mapc: # ic 是一个Counter对象</span><br><span class="line">        s +&#x3D; ic</span><br><span class="line">    return s</span><br><span class="line"></span><br><span class="line">def func(**args):</span><br><span class="line">    print(args)</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    a &#x3D; [&#39;apple&#39;, &#39;orange&#39;, &#39;computer&#39;, &#39;orange&#39;]</span><br><span class="line">    b &#x3D; [&#39;computer&#39;, &#39;orange&#39;]</span><br><span class="line">    x1&#x3D;sum_(a,b)</span><br><span class="line">    x2 &#x3D; sumc(a, b, [&#39;abc&#39;], [&#39;face&#39;, &#39;computer&#39;])</span><br><span class="line">    print(x1)</span><br><span class="line">    print(x2)</span><br><span class="line">    func(a&#x3D;&#39;1&#39;, b&#x3D;&#39;2&#39;, c&#x3D;&#39;3&#39;)  # args表示&#123;‘a’:&#39;1&#39;,&#39;b&#39;:&#39;2&#39;,&#39;c&#39;:&#39;3&#39;</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;Output</span><br><span class="line">    </span><br><span class="line">            Counter(&#123;&#39;orange&#39;: 3, &#39;computer&#39;: 2, &#39;apple&#39;: 1&#125;)</span><br><span class="line">            Counter(&#123;&#39;orange&#39;: 3, &#39;computer&#39;: 3, &#39;apple&#39;: 1, &#39;abc&#39;: 1, &#39;face&#39;: 1&#125;)</span><br><span class="line">            &#123;&#39;a&#39;: &#39;1&#39;, &#39;b&#39;: &#39;2&#39;, &#39;c&#39;: &#39;3&#39;&#125;</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>7： combine two dict</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dic1 &#x3D; &#123;&#39;x&#39;: 1, &#39;y&#39;: 2 &#125;</span><br><span class="line">dic2 &#x3D; &#123;&#39;y&#39;: 3, &#39;z&#39;: 4 &#125;</span><br><span class="line">merged1 &#x3D; &#123;**dic1, **dic2&#125; # &#123;&#39;x&#39;: 1, &#39;y&#39;: 3, &#39;z&#39;: 4&#125;</span><br><span class="line">print(merged1)</span><br><span class="line"># IF we modify merged1[&#39;x&#39;]&#x3D;10, the value in dic1 is unchange.</span><br><span class="line"></span><br><span class="line">from collections import ChainMap</span><br><span class="line">merged2 &#x3D; ChainMap(dic1,dic2)</span><br><span class="line">print(merged2) # ChainMap(&#123;&#39;x&#39;: 1, &#39;y&#39;: 2&#125;, &#123;&#39;y&#39;: 3, &#39;z&#39;: 4&#125;)    # IF we modify merged1[&#39;x&#39;]&#x3D;10, the value in dic1 is change.  ChainMap save the memory</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Some resource links about NLP</title>
      <link href="2019/12/07/Some-resource-links-about-NLP/"/>
      <url>2019/12/07/Some-resource-links-about-NLP/</url>
      
        <content type="html"><![CDATA[<p>1: Chinese Word Vectors from Beijing Normal University and Renmin University of China, <a href="https://github.com/Embedding/Chinese-Word-Vectors">Chinese word vectors</a>.</p><p>2: A toolkit about sentence matching, <a href="https://github.com/NTMC-Community/MatchZoo">MatchZoo</a>.</p><p>3: Some conferences in 2020: <a href="http://ecai2020.eu/">ECAI2020</a>, <a href="http://sigir.org/sigir2020/">SIGIR2020</a>, <a href="http://www.ijcai20.org/">IJCAI2020</a>.</p><p>4: The analysis about Transformer, by Jianlin Su. <a href="https://spaces.ac.cn/archives/4765/comment-page-1">Go!!!Go!</a>.</p><p>5：Some tools in python, including wget, pendulum, imbalanced-learn, flashtext, etc. <a href="https://mp.weixin.qq.com/s?__biz=MzU2MzgyODA4OA==&mid=2247484545&idx=1&sn=2205601300644dead61ec21e440612eb&chksm=fc550096cb228980e94ee9568f90508618e2eae84244b833c40b89b0a71581639030521bcd97&mpshare=1&scene=23&srcid=&sharer_sharetime=1576214464499&sharer_shareid=3e54f7e89ddf02d1c037040e0759883b#rd">Let’s GO!</a></p><p>6: <a href="https://mp.weixin.qq.com/s?__biz=MzI4ODY2NjYzMQ==&mid=2247486446&idx=1&sn=6d4577b578a36fb7df709f290dd7f0f2&chksm=ec3bae86db4c2790e64ff30181c9d162a2f67c29e9a8272a2f70aa77e2344d12df391f851a7f&mpshare=1&scene=23&srcid=&sharer_sharetime=1576073544162&sharer_shareid=3e54f7e89ddf02d1c037040e0759883b#rd">42 python excample</a></p><p>7： Some Chinese word segmentation toolkits. <a href="http://www.52nlp.cn/%E4%BA%94%E6%AC%BE%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E7%BA%BF%E4%B8%8Apk-jieba-snownlp-pkuseg-thulac-hanlp">Let’s go!</a></p><p>8: <a href="https://github.com/ymcui/Chinese-BERT-wwm">Chinese BERT-wwm</a></p><p>9: <a href="https://blog.csdn.net/clnjust/article/details/100514231">the source about Anaconda</a></p><p>10: <a href="https://blog.csdn.net/gangeqian2/article/details/79358543">tensorflow with GPU</a></p><p>11： <a href="https://www.toutiao.com/a6799552277234319876/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1585359886&app=news_article&utm_source=mobile_qq&utm_medium=toutiao_android&req_id=2020032809444601012903610300717010&group_id=6799552277234319876">The Most Powerful NLP-Weapon Arsenal NLP</a></p><p>12: [Natural Language Processing and Text Mining with Graph-Structured Representations](Natural Language Processing and Text Mining with Graph-Structured Representations)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>NLP-base: text classification by TF-IDF and Bayes</title>
      <link href="2019/12/06/NLP-base-text-classification-by-TF-IDF-and-Bayes/"/>
      <url>2019/12/06/NLP-base-text-classification-by-TF-IDF-and-Bayes/</url>
      
        <content type="html"><![CDATA[<p>In this blog, I will use TF-IDF and Bayes to conduct text classification, I refered a demo by website. My aim is to review some knowledge about NLP.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import jieba</span><br><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">root_path&#x3D;os.getcwd()  # this dic</span><br><span class="line"></span><br><span class="line">def preprocess(path):</span><br><span class="line">    text_with_space &#x3D; &quot;&quot;</span><br><span class="line">    textfile &#x3D; open(path, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;).read()</span><br><span class="line">    textcute &#x3D; jieba.cut(textfile)</span><br><span class="line">    for word in textcute:</span><br><span class="line">        text_with_space +&#x3D; word + &quot; &quot;</span><br><span class="line">        # print(text_with_space)</span><br><span class="line">    return text_with_space</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loadtrainset(path, classtag):</span><br><span class="line">    allfiles &#x3D; os.listdir(path)  # os.path.isdir()用于判断对象是否为一个目录，并返回此目录下的所有文件名</span><br><span class="line">    print(allfiles)</span><br><span class="line"></span><br><span class="line">    processed_textset &#x3D; []</span><br><span class="line">    allclasstags &#x3D; []</span><br><span class="line">    #</span><br><span class="line">    for thisfile in allfiles:</span><br><span class="line">        # print(thisfile)</span><br><span class="line">        path_name &#x3D; path + &quot;&#x2F;&quot; + thisfile</span><br><span class="line">        processed_textset.append(preprocess(path_name))</span><br><span class="line">        allclasstags.append(classtag)</span><br><span class="line">    #</span><br><span class="line">    return processed_textset, allclasstags  # 数组形式--processed_textset 文件的具体内容， allclasstags 文件分类</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    processed_textdata1, class1&#x3D;loadtrainset(root_path+&quot;&#x2F;dataset&#x2F;train&#x2F;hotel&quot;, &quot;宾馆&quot;)</span><br><span class="line">    processed_textdata2, class2 &#x3D; loadtrainset(root_path+&quot;&#x2F;dataset&#x2F;train&#x2F;travel&quot;, &quot;旅游&quot;)</span><br><span class="line">    train_data &#x3D; processed_textdata1 + processed_textdata2</span><br><span class="line">    # # print(train_data)  # 前半部分是宾馆， 后半部分是旅游 train</span><br><span class="line">    classtags_list &#x3D; class1 + class2  # 前半部分是宾馆， 后半部分是旅游 train 集结果</span><br><span class="line">    # #</span><br><span class="line">    print(train_data)</span><br><span class="line">    # print(classtags_list)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # CountVectorizer是通过fit_transform函数将文本中的词语转换为词频矩阵</span><br><span class="line">        get_feature_names()可看到所有文本的关键字</span><br><span class="line">        vocabulary_可看到所有文本的关键字和其位置</span><br><span class="line">        toarray()可看到词频矩阵的结果</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    count_vector &#x3D; CountVectorizer()</span><br><span class="line">    vecot_matrix &#x3D; count_vector.fit_transform(train_data)</span><br><span class="line"></span><br><span class="line">    # print (count_vector.get_feature_names ())  #看到所有文本的关键字</span><br><span class="line">    # print (count_vector.vocabulary_)   #文本的关键字和其位置</span><br><span class="line">    # print (vecot_matrix.toarray ())  #词频矩阵的结果</span><br><span class="line"></span><br><span class="line">    # #TFIDF</span><br><span class="line">    &quot;&quot;&quot;TfidfTransformer是统计CountVectorizer中每个词语的tf-idf权值</span><br><span class="line">    tfidf &#x3D; transformer.fit_transform(vectorizer.fit_transform(corpus))</span><br><span class="line">    vectorizer.fit_transform(corpus)将文本corpus输入，得到词频矩阵</span><br><span class="line"></span><br><span class="line">    将这个矩阵作为输入，用transformer.fit_transform(词频矩阵)得到TF-IDF权重矩阵</span><br><span class="line">    TfidfTransformer + CountVectorizer  &#x3D;  TfidfVectorizer</span><br><span class="line"></span><br><span class="line">    这个成员的意义是词典索引，对应的是TF-IDF权重矩阵的列，只不过一个是私有成员，一个是外部输入，原则上应该保持一致。</span><br><span class="line">        use_idf：boolean， optional 启动inverse-document-frequency重新计算权重</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    train_tfidf &#x3D; TfidfTransformer().fit_transform(vecot_matrix)</span><br><span class="line">    clf &#x3D; MultinomialNB().fit(train_tfidf, classtags_list)</span><br><span class="line"></span><br><span class="line">    testset &#x3D; []</span><br><span class="line">    path &#x3D; root_path+&quot;&#x2F;dataset&#x2F;tt&quot;  # 测试此路径下的各文件属于哪个类别</span><br><span class="line">    allfiles &#x3D; os.listdir(path)</span><br><span class="line">    hotel &#x3D; 0</span><br><span class="line">    travel &#x3D; 0</span><br><span class="line">    #</span><br><span class="line"></span><br><span class="line">    for thisfile in allfiles:</span><br><span class="line">        path_name &#x3D; path + &quot;&#x2F;&quot; + thisfile  # 得到此目录下的文件绝对路径</span><br><span class="line">        new_count_vector &#x3D; count_vector.transform([preprocess(path_name)]) # 得到测试集的词频矩阵</span><br><span class="line">        # 用transformer.fit_transform(词频矩阵)得到TF-IDF权重矩阵</span><br><span class="line">        new_tfidf &#x3D; TfidfTransformer(use_idf&#x3D;False).fit_transform(new_count_vector)</span><br><span class="line">        # print(train_tfidf)  # vecot_matrix输入，得到词频矩阵</span><br><span class="line">        train_tfidf &#x3D; TfidfTransformer().fit_transform(vecot_matrix)</span><br><span class="line">        # MultinomialNB(),fit() ,多分类， Fit Naive Bayes classifier according to X,根据 X，Y，结果 类别，进行多分类</span><br><span class="line"></span><br><span class="line">        # 根据 由训练集而得到的分类模型，clf ,由 测试集的 TF-IDF权重矩阵来进行预测分类</span><br><span class="line">        predict_result &#x3D; clf.predict(new_tfidf)</span><br><span class="line">        print(predict_result)</span><br><span class="line">        print(thisfile)</span><br><span class="line"></span><br><span class="line">        if(predict_result &#x3D;&#x3D; &quot;宾馆&quot;):</span><br><span class="line">            hotel +&#x3D; 1</span><br><span class="line">        if(predict_result &#x3D;&#x3D; &quot;旅游&quot;):</span><br><span class="line">            travel +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    print(&quot;宾馆&quot; + str(hotel))</span><br><span class="line">    print(&quot;旅游&quot; + str(travel))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>CountVectorizer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">texts&#x3D;[&quot;dog cat fish&quot;,&quot;dog cat cat&quot;,&quot;fish&quot;, &#39;bird&#39;] # “dog cat fish” 为输入列表元素,即代表一个文章的字符串</span><br><span class="line">cv &#x3D; CountVectorizer()#创建词袋数据结构</span><br><span class="line">cv_fit&#x3D;cv.fit_transform(texts)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print (cv.get_feature_names ())  #看到所有文本的关键字</span><br><span class="line">print (cv.vocabulary_)   #文本的关键字和其位置</span><br><span class="line">print(cv_fit)  #第0个列表元素，**词典中索引为3的元素**， 词频</span><br><span class="line">train_tfidf &#x3D; TfidfTransformer().fit_transform(cv_fit)</span><br><span class="line">print(train_tfidf)</span><br></pre></td></tr></table></figure><p>jieba</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import  jieba</span><br><span class="line"></span><br><span class="line">s&#x3D;&quot;简介： 北京喜来登长城饭店位于中国文化和商业中心——北京的心脏，毗邻于使馆区，距农展馆和国际展览中心仅5分钟车程&quot;</span><br><span class="line">d&#x3D;(jieba.cut(s))</span><br><span class="line">text_with_space&#x3D;&quot;&quot;</span><br><span class="line">for word in d:</span><br><span class="line">    text_with_space +&#x3D; word + &quot;||&quot;</span><br><span class="line"></span><br><span class="line">print(text_with_space)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Argue MatchZoo</title>
      <link href="2019/12/04/Argue-MatchZoo/"/>
      <url>2019/12/04/Argue-MatchZoo/</url>
      
        <content type="html"><![CDATA[<p>When I use MatchZoo, I find a problem, the model ARCI can run in a normal level, but the model DRMM cannot run. We use the same data, same input. The issue is producted in its sourse code. At the same time, this tool can not give a convenient use for user. I do not like it.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Processing text_right with chain_transform of Tokenize &#x3D;&gt; Lowercase &#x3D;&gt; PuncRemoval: 100%|██████████| 25955&#x2F;25955 [00:18&lt;00:00, 1393.08it&#x2F;s]</span><br><span class="line">Processing text_right with transform: 100%|██████████| 25955&#x2F;25955 [00:00&lt;00:00, 48732.12it&#x2F;s]</span><br><span class="line">Processing text_left with transform: 100%|██████████| 2717&#x2F;2717 [00:00&lt;00:00, 136217.12it&#x2F;s]</span><br><span class="line">Processing text_right with transform: 100%|██████████| 25955&#x2F;25955 [00:00&lt;00:00, 43155.30it&#x2F;s]</span><br><span class="line">Processing length_left with len: 100%|██████████| 2717&#x2F;2717 [00:00&lt;00:00, 680597.47it&#x2F;s]</span><br><span class="line">Processing length_right with len: 100%|██████████| 25955&#x2F;25955 [00:00&lt;00:00, 963783.14it&#x2F;s]</span><br><span class="line">Processing text_left with transform: 100%|██████████| 2717&#x2F;2717 [00:00&lt;00:00, 143073.21it&#x2F;s]</span><br><span class="line">Processing text_right with transform: 100%|██████████| 25955&#x2F;25955 [00:00&lt;00:00, 52262.28it&#x2F;s]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;E:\bao\anaconda_bao\Anaconda3\lib\site-packages\keras\engine\training_utils.py&quot;, line 80, in standardize_input_data</span><br><span class="line">    for x in names</span><br><span class="line">  File &quot;E:\bao\anaconda_bao\Anaconda3\lib\site-packages\keras\engine\training_utils.py&quot;, line 80, in &lt;listcomp&gt;</span><br><span class="line">    for x in names</span><br><span class="line">KeyError: &#39;match_histogram&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Text classification in sport science -data progress</title>
      <link href="2019/12/02/Text-classification-in-sport-science-data-progress/"/>
      <url>2019/12/02/Text-classification-in-sport-science-data-progress/</url>
      
        <content type="html"><![CDATA[<p>By investigating, some researchers analyze the sportsmanship by manual, but the academy of sports science in QFNU want to use machine build a baseline about sportsmanship. I am happy to help them produce this system. Text classification is a part of this project. So, I will dicuss the data progress in this blog.</p><p>There are two types file (word and excel), my aim is to change these files into this style [“拿世界级的高薪！！！踢世界级的臭球—————–经济责任 “] which store in txt.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import xlrd</span><br><span class="line">from docx import Document</span><br><span class="line"></span><br><span class="line">file &#x3D; &#39;评论.xlsx&#39;</span><br><span class="line">def read_excel():</span><br><span class="line">    fw1&#x3D;open(&quot;ca1_data1.txt&quot;,&quot;w&quot;,encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">    fw2&#x3D; open(&quot;ca2_data1.txt&quot;, &quot;w&quot;, encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">    wb &#x3D; xlrd.open_workbook(filename&#x3D;file)#打开文件</span><br><span class="line">    sheet1 &#x3D; wb.sheet_by_index(0)#通过索引获取表格</span><br><span class="line">    text &#x3D; sheet1.col_values(1)#获取列内容</span><br><span class="line">    ca1&#x3D;sheet1.col_values(2)</span><br><span class="line">    ca2&#x3D;sheet1.col_values(3)</span><br><span class="line"></span><br><span class="line">    dic1&#x3D;dict(zip(text,ca1))</span><br><span class="line">    for key, value in dic1.items():</span><br><span class="line">        value&#x3D;value.split(&quot;、&quot;)</span><br><span class="line">        for v in value:</span><br><span class="line">            fw1.write(key+&quot;\t&quot;+v+&quot;\n&quot;)</span><br><span class="line">    dic2 &#x3D; dict(zip(text, ca2))</span><br><span class="line">    for key, value in dic2.items():</span><br><span class="line">        fw2.write(key + &quot;\t&quot; + value + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">def is_Chinese(word):</span><br><span class="line">    s&#x3D;[&quot;:&quot;,&quot;,&quot;,&quot;!&quot;,&quot;.&quot;]</span><br><span class="line">    for ch in word:</span><br><span class="line">        if &#39;\u4e00&#39; &lt;&#x3D; ch &lt;&#x3D; &#39;\u9fff&#39; or ch in s  :</span><br><span class="line">            return True</span><br><span class="line">    return False</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def read_word():</span><br><span class="line">    fw1 &#x3D; open(&quot;ca1_data2.txt&quot;, &quot;w&quot;, encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">    fw2 &#x3D; open(&quot;ca2_data2.txt&quot;, &quot;w&quot;, encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line"></span><br><span class="line">    document &#x3D; Document(&quot;300条事件评论.docx&quot;) #读入文件</span><br><span class="line">    tables &#x3D; document.tables #获取文件中的表格集</span><br><span class="line">    table &#x3D; tables[0]#获取文件中的第一个表格</span><br><span class="line">    f&#x3D;0</span><br><span class="line">    s&#x3D;list(range(0,500))</span><br><span class="line">    print(s)</span><br><span class="line">    for i in range(1,len(table.rows)):#从表格第二行开始循环读取表格数据</span><br><span class="line">        f&#x3D;f+1</span><br><span class="line">        print(f)</span><br><span class="line"></span><br><span class="line">        text &#x3D; table.cell(i,1).text.replace(&quot;\n&quot;,&quot;&quot;).split(&quot;：&quot;)[1].strip()</span><br><span class="line"></span><br><span class="line">        ca1&#x3D;table.cell(i,2).text.replace(&quot;\n&quot;,&quot;,&quot;)</span><br><span class="line"></span><br><span class="line">        ca2 &#x3D; table.cell(i, 3).text</span><br><span class="line">        if len(text)&gt;0 and len(ca1)&gt;0:</span><br><span class="line">            for e in ca1.split(&quot;,&quot;):</span><br><span class="line">                fw1.write(text + &quot;\t&quot; + e + &quot;\n&quot;)</span><br><span class="line">                fw1.flush()</span><br><span class="line">        else:</span><br><span class="line">            continue</span><br><span class="line">        if len(text) &gt; 0 and len(ca2) &gt; 0:</span><br><span class="line">            fw2.write(text + &quot;\t&quot; + ca2 + &quot;\n&quot;)</span><br><span class="line">            fw2.flush()</span><br><span class="line">        else:</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    # read_excel()</span><br><span class="line">    read_word()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>some functions in Tensorflow</title>
      <link href="2019/11/30/some-functions-in-Tensorflow/"/>
      <url>2019/11/30/some-functions-in-Tensorflow/</url>
      
        <content type="html"><![CDATA[<p>I just use this blog to record some functions which I used.</p><p><font color=red size=3>tf.range &amp; tf.expand_dims</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">sequence_length&#x3D;10</span><br><span class="line">batch_size&#x3D;32</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    s1&#x3D;tf.range(sequence_length)</span><br><span class="line">    s2&#x3D;tf.expand_dims(tf.range(sequence_length), 0)</span><br><span class="line">    print(sess.run(s1))</span><br><span class="line">    print(sess.run(s2))</span><br><span class="line"></span><br><span class="line">result:</span><br><span class="line">s1:[0 1 2 3 4 5 6 7 8 9]</span><br><span class="line">s2:[[0 1 2 3 4 5 6 7 8 9]]</span><br></pre></td></tr></table></figure><p><font color=red size=3>tf.tile</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a &#x3D; tf.tile([1,5,3],[4])</span><br><span class="line">b &#x3D; tf.tile([[1,2],</span><br><span class="line">             [5,1],</span><br><span class="line">             [7,6]],[2,3])</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(a))</span><br><span class="line">    print(sess.run(b))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result:</span><br><span class="line">a:[1 5 3 1 5 3 1 5 3 1 5 3]</span><br><span class="line">b: [[1 2 1 2 1 2]</span><br><span class="line"> [5 1 5 1 5 1]</span><br><span class="line"> [7 6 7 6 7 6]</span><br><span class="line"> [1 2 1 2 1 2]</span><br><span class="line"> [5 1 5 1 5 1]</span><br><span class="line"> [7 6 7 6 7 6]]</span><br></pre></td></tr></table></figure><p><font color=red size=3>b = a[i:j:s]</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&quot;s&quot; refers to stride, default is 1, &quot;-1&quot; refers to reverse read</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a&#x3D;[1,2,3.4,5]</span><br><span class="line">print(a[2::-1])    </span><br><span class="line">result：[ 3 2 1 ]</span><br><span class="line"></span><br><span class="line">----------------------------</span><br><span class="line"></span><br><span class="line">s&#x3D;np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])</span><br><span class="line">print(s[:, 0::2]) #from 0 to end, the stride is 2</span><br><span class="line"></span><br><span class="line">[[ 1  3]</span><br><span class="line"> [ 4  6]</span><br><span class="line"> [ 7  9]</span><br><span class="line"> [10 12]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><font color=red size=3>cast(x, dtype, name=None)</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">to change the tensor type</span><br><span class="line">int32  to float32：</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">f1 &#x3D; tf.Variable([7,8,9,10])</span><br><span class="line">f2&#x3D; tf.cast(f1, dtype&#x3D;tf.float32)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(f2))</span><br><span class="line">result:</span><br><span class="line">[ 7.  8.  9. 10.]  float</span><br></pre></td></tr></table></figure><p><font color=red size=3>as_list()(x, dtype, name=None)</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x&#x3D;tf.constant([[1,2,3],[4,5,6]])</span><br><span class="line">x.get_shape().as_list()[0]   #output: the number of rows</span><br><span class="line">x.get_shape().as_list()[1]   #output:Number of columns</span><br></pre></td></tr></table></figure><p><font color=red size=3>tf.nn.softmax_cross_entropy_with_logits &amp; tf.nn.sparse_softmax_cross_entropy_with_logits (x, dtype, name=None)</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">they are have the same result: 1.222818</span><br><span class="line"></span><br><span class="line">1:</span><br><span class="line"></span><br><span class="line">tf.nn.softmax_cross_entropy_with_logits(logits,labels) </span><br><span class="line">labels&#x3D;[[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]]</span><br><span class="line"></span><br><span class="line">#encoding&#x3D;utf-8</span><br><span class="line"></span><br><span class="line">method 1:</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">#output</span><br><span class="line">logits&#x3D;tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])</span><br><span class="line">#step1: do softmax</span><br><span class="line">y&#x3D;tf.nn.softmax(logits)</span><br><span class="line">#true label</span><br><span class="line">y_ &#x3D;tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])</span><br><span class="line">#step2: do cross_entropy</span><br><span class="line">cross_entropy&#x3D;-tf.reduce_sum(y_*tf.log(y))</span><br><span class="line"></span><br><span class="line"># do cross_entropy just one step</span><br><span class="line">cross_entropy2&#x3D;tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;logits,labels&#x3D;y_))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    softmax&#x3D;sess.run(y)</span><br><span class="line">    c_e&#x3D;sess.run(cross_entropy)</span><br><span class="line">    c_e2&#x3D;sess.run(cross_entropy2)</span><br><span class="line"></span><br><span class="line">    print(&quot;step1:&quot;)</span><br><span class="line">    print(softmax)</span><br><span class="line">    print(&quot;step2:&quot;)</span><br><span class="line">    print(c_e)</span><br><span class="line">    print(&quot;all&quot;)</span><br><span class="line">    print(c_e2)</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line">or:method 2:</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">logits &#x3D; tf.placeholder(tf.float32, shape&#x3D;[3, 3], name&#x3D;&quot;x&quot;)</span><br><span class="line">labels &#x3D; tf.placeholder(tf.float32, shape&#x3D;[3,3], name&#x3D;&quot;y&quot;)</span><br><span class="line">y&#x3D;tf.nn.softmax(logits)</span><br><span class="line"># cross_entropy &#x3D; tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;logits, labels&#x3D;labels)#此处的label经过ont-hot处理</span><br><span class="line"># cost &#x3D; tf.reduce_mean(cross_entropy)</span><br><span class="line"># y_ &#x3D;tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])</span><br><span class="line">cross_entropy1&#x3D;-tf.reduce_sum(labels*tf.log(y))</span><br><span class="line">cross_entropy2 &#x3D; tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(logits&#x3D;logits, labels&#x3D;labels))</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    x&#x3D;[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]</span><br><span class="line">    y&#x3D;[[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]]</span><br><span class="line">    logits1&#x3D;sess.run(cross_entropy1, feed_dict&#x3D;&#123;logits: x, labels:y&#125;)</span><br><span class="line">    logits2 &#x3D; sess.run(cross_entropy2, feed_dict&#x3D;&#123;logits: x, labels: y&#125;)</span><br><span class="line">    print(logits1)</span><br><span class="line">    print(logits2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">2:</span><br><span class="line"></span><br><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits((logits,labels)</span><br><span class="line">labels&#x3D;[2,2,2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">logits &#x3D; tf.placeholder(tf.float32, shape&#x3D;[3, 3], name&#x3D;&quot;x&quot;)</span><br><span class="line">labels &#x3D;tf.placeholder(tf.int32, shape&#x3D;[3], name&#x3D;&quot;y&quot;) # 3 为batch_size</span><br><span class="line"></span><br><span class="line">cross_entropy2 &#x3D; tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits&#x3D;logits, labels&#x3D;labels))</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    x&#x3D;[[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]]</span><br><span class="line">    y&#x3D;[2,2,2]</span><br><span class="line">    logits2 &#x3D; sess.run(cross_entropy2, feed_dict&#x3D;&#123;logits: x, labels: y&#125;)</span><br><span class="line">    print(logits2)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">result: 1.2228179</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Mathematical distribution by Python</title>
      <link href="2019/11/30/Mathematical-distribution-by-Python/"/>
      <url>2019/11/30/Mathematical-distribution-by-Python/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">import matplotlib</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">SAMPLE_SIZE &#x3D; 1000</span><br><span class="line">buckets &#x3D; 100</span><br><span class="line">fig &#x3D; plt.figure()</span><br><span class="line">matplotlib.rcParams.update(&#123;&quot;font.size&quot;: 7&#125;)</span><br><span class="line"># the first figure[0,1)（normal distributed random variable）。</span><br><span class="line">ax &#x3D; fig.add_subplot(5, 2, 1)</span><br><span class="line">ax.set_xlabel(&quot;random.random&quot;)</span><br><span class="line">res &#x3D; [random.random() for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line"></span><br><span class="line"># plt.xlabel(&quot;numbers&quot;)</span><br><span class="line"># plt.ylabel(&quot;count&quot;)  # xlabel  指的是res 中的数字，  buckets&#x3D;100， 相当于把轴分成了100份， 步长是1， y轴的含义是在x轴中每个小范围中的有多少个</span><br><span class="line">                     #例如： x轴第一个小范围是[0,0.01], y轴的含义就是res中的数在这个范围的数有几个</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax.hist(res, buckets)</span><br><span class="line"></span><br><span class="line"># the second figure is 均匀分布（uniformly distributed random variable）。</span><br><span class="line">ax_2 &#x3D; fig.add_subplot(5, 2, 2)</span><br><span class="line">ax_2.set_xlabel(&quot;random.uniform&quot;)</span><br><span class="line">a &#x3D; 1</span><br><span class="line">b &#x3D; SAMPLE_SIZE</span><br><span class="line">res_2 &#x3D; [random.uniform(a, b) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">ax_2.hist(res_2, buckets)</span><br><span class="line"></span><br><span class="line"># 3:（triangular distribution）。</span><br><span class="line">ax_3 &#x3D; fig.add_subplot(5, 2, 3)</span><br><span class="line">ax_3.set_xlabel(&quot;random.triangular&quot;)</span><br><span class="line">low &#x3D; 1</span><br><span class="line">high &#x3D; SAMPLE_SIZE</span><br><span class="line">res_3 &#x3D; [random.triangular(low, high) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">ax_3.hist(res_3, buckets)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 4:（beta distribution）alpha&gt;0 and beta&gt;0， 0&lt;the return value&lt;1。</span><br><span class="line">plt.subplot(5, 2, 4)</span><br><span class="line">plt.xlabel(&quot;random.betavariate&quot;)</span><br><span class="line">alpha &#x3D; 1</span><br><span class="line">beta &#x3D; 10</span><br><span class="line">res_4 &#x3D; [random.betavariate(alpha, beta) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">plt.hist(res_4, buckets)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 5（exponential distribution）。 lambd 的值是 1.0 除以期望的中值，是一个不为零的数（参数应该叫做lambda没但它是python的一个保留字）。如果lambd是整数，返回值的范围是零到正无穷大；如果lambd为负，返回值的范围是负无穷大到零。</span><br><span class="line">plt.subplot(5, 2, 5)</span><br><span class="line">plt.xlabel(&quot;random.expovariate&quot;)</span><br><span class="line">lambd &#x3D; 1.0 &#x2F; ((SAMPLE_SIZE + 1) &#x2F; 2.)</span><br><span class="line">res_5 &#x3D; [random.expovariate(lambd) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">plt.hist(res_5, buckets)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 6: gamma（gamma distribution）， alpha&gt;0 and beta &gt;0</span><br><span class="line">plt.subplot(5, 2, 6)</span><br><span class="line">plt.xlabel(&quot;random.gammavariate&quot;)</span><br><span class="line">alpha &#x3D; 1</span><br><span class="line">beta &#x3D; 10</span><br><span class="line">res_6 &#x3D; [random.gammavariate(alpha, beta) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">plt.hist(res_6, buckets)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 7:（Log normal distribution）。如果取这个分布的自然对数，会得到一个中值为mu，标准差为sigma的正态分布。mu可以取任何值，sigma必须大于零。</span><br><span class="line">plt.subplot(5, 2, 7)</span><br><span class="line">plt.xlabel(&quot;random.lognormalvariate&quot;)</span><br><span class="line">mu &#x3D; 1</span><br><span class="line">sigma &#x3D; 0.5</span><br><span class="line">res_7 &#x3D; [random.lognormvariate(mu, sigma) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">plt.hist(res_7, buckets)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 8:（normal distribution）。</span><br><span class="line">plt.subplot(5, 2, 8)</span><br><span class="line">plt.xlabel(&quot;random.normalvariate&quot;)</span><br><span class="line">mu &#x3D; 1</span><br><span class="line">sigma &#x3D; 0.5</span><br><span class="line">res_8 &#x3D; [random.normalvariate(mu, sigma) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">plt.hist(res_8, buckets)</span><br><span class="line"></span><br><span class="line"># 9:（Pareto distribution）， alpha 是形状参数</span><br><span class="line">plt.subplot(5, 2, 9)</span><br><span class="line">plt.xlabel(&quot;random.normalvariate&quot;)</span><br><span class="line">alpha &#x3D; 1</span><br><span class="line">res_9 &#x3D; [random.paretovariate(alpha) for _ in range(1, SAMPLE_SIZE)]</span><br><span class="line">plt.hist(res_9, buckets)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><div align=center><img src="figure/python_math_figure.png" width="100%" height="100%" />]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>super() in python</title>
      <link href="2019/11/30/the-super-of-class-in-Python/"/>
      <url>2019/11/30/the-super-of-class-in-Python/</url>
      
        <content type="html"><![CDATA[<p>super () is a method used to call the parent class (superclass).</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class Parent(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.parent &#x3D; &#39;I am the parent.&#39;</span><br><span class="line">        print(&#39;Parent&#39;)</span><br><span class="line"></span><br><span class="line">    def bar(self, message):</span><br><span class="line">        print(&quot;%s from Parent&quot; % message)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Child(Parent):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        #super(Child,self)   first find the father class about Child (it is Parent), and then change the object of Child  into</span><br><span class="line">        #the object of Parent</span><br><span class="line">        super(Child, self).__init__()</span><br><span class="line">        print(&#39;Child&#39;)</span><br><span class="line"></span><br><span class="line">    def bar(self, message):</span><br><span class="line">        super(Child, self).bar(message)</span><br><span class="line">        print(&#39;Child bar fuction&#39;)</span><br><span class="line">        print(self.parent)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    xChild &#x3D; Child()</span><br><span class="line">    xChild.bar(&#39;HelloWorld&#39;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>The advanced use of |for| in python </title>
      <link href="2019/11/30/The-advanced-use-of-for-in-python/"/>
      <url>2019/11/30/The-advanced-use-of-for-in-python/</url>
      
        <content type="html"><![CDATA[<p>This blog summarizes some advanced uses of “for” in python.</p><p><font color=red size=3>for  in  for  in</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def v(z):</span><br><span class="line">    for y in z:</span><br><span class="line">        for x in y:</span><br><span class="line">           yield x</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    vec1 &#x3D; [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</span><br><span class="line">    vec2&#x3D; [[[1, 2, 3], [4, 5, 6], [7, 8, 9]],[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]</span><br><span class="line">    # vec&#x3D;[1,2,3]</span><br><span class="line">    s1 &#x3D; [num for elem in vec1 for num in elem]  #elem&#x3D;[1, 2, 3],[4, 5, 6]...</span><br><span class="line">    s3&#x3D;[n for elem in vec2 for num in elem for n in num]</span><br><span class="line">    print(&quot;s1&quot;,s1)</span><br><span class="line">    s2&#x3D;[]</span><br><span class="line">    a &#x3D; v(vec1)</span><br><span class="line">    for i in a:</span><br><span class="line">        s2.append(i)</span><br><span class="line">    print(&quot;s2&quot;,s2)</span><br><span class="line">    print(&quot;s3&quot;,s3)</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    result:</span><br><span class="line">    s1 [1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">    s2 [1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">    s3 [1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9] </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>l1 and l2 regularizer in tensorflow</title>
      <link href="2019/11/28/l1-and-l2-regularizer-in-tensorflow/"/>
      <url>2019/11/28/l1-and-l2-regularizer-in-tensorflow/</url>
      
        <content type="html"><![CDATA[<p>Regularization can solve the problem of model overfitting.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.contrib as contrib</span><br><span class="line"></span><br><span class="line">weight &#x3D; tf.constant([[1.0, -2.0], [-3.0, 4.0]])</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    # the output is (|1|+|-2|+|-3|+|4|)*0.5&#x3D;5</span><br><span class="line">    print(sess.run(contrib.layers.l1_regularizer(0.5)(weight)))</span><br><span class="line">    # the output is(1²+(-2)²+(-3)²+4²)&#x2F;2*0.5&#x3D;7.5</span><br><span class="line">    # TensorFlow let the loss of l2_regularizer divide 2                </span><br><span class="line">    print(sess.run(contrib.layers.l2_regularizer(0.5)(weight)))</span><br><span class="line">    # l1_regularizer+l2_regularizer</span><br><span class="line">    print(sess.run(contrib.layers.l1_l2_regularizer(0.5, 0.5)(weight)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.contrib.layers.12_regularizer(lambda)(w) </span><br><span class="line">lambda refers to the weight of regularization, same as λ in J（θ）＋λR(w）。w is the parameter need to be calualted.</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Generate the word vector by using BERT</title>
      <link href="2019/11/28/Generate-the-word-vector-by-using-BERT/"/>
      <url>2019/11/28/Generate-the-word-vector-by-using-BERT/</url>
      
        <content type="html"><![CDATA[<p>This blog will introduce how to use BERT to generate the Chinese word vector. Otherwise, I will discuss my bugs when I was using it.</p><p><font color=red size=3>First</font>, we should do some configuration for bert-as-service, as below,</p><pre><code>pip install  bert-serving-serverpip install bert-serving-client</code></pre><p><font color=red size=3>Second</font>, BERT needs pre-trained model, we should download it by these ways:</p><pre><code>https://github.com/ymcui/Chinese-BERT-wwm, or,https://github.com/google-research/bert#pre-trained-models</code></pre><p><font color=red size=3>Third</font>, we should start bert-as-service (the path of bert-serving-start.exe. E:\bao\anaconda_bao\Anaconda3\Scripts), </p><pre><code>Entering the file which contains &quot;bert-serving-start.exe&quot; in CMD, then you can run &quot;bert-serving-start -model_dir E:\bao\BURT\HaGongDa\chinese_roberta_wwm_large_ext_L-24_H-1024_A-16\&quot; in your CMD, model_dir is your model&#39;s dic. My model is in &quot;E:\bao\BURT\HaGongDa\chinese_roberta_wwm_large_ext_L-24_H-1024_A-16&quot;</code></pre><p><font color=red size=3>At last</font>, your can write your code in Pycharm to obtain the vector,<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from bert_serving.client import BertClient</span><br><span class="line">bc &#x3D; BertClient()</span><br><span class="line">print(bc.encode([&#39;中国&#39;, &#39;美国&#39;]))</span><br></pre></td></tr></table></figure></p><p>So, we can use vectors which produced by BERT in the downstream applications, such as QA, IA, MT. You can also refer <a href="https://blog.csdn.net/qq_34832393/article/details/90414293">HERE</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Paper skill -reference</title>
      <link href="2019/11/26/Paper-skill-reference/"/>
      <url>2019/11/26/Paper-skill-reference/</url>
      
        <content type="html"><![CDATA[<p>This blog is just used to help me organize the knowledge about paper writing. My reference is “English Exposed”.</p><hr><p>Smith et al. <font color=green>states</font> that this is true for bull markets (F)<br>Smith et al. <font color=red>state</font> that this is true for bull markets (T)<br>The following are typical errors:  Smith at al.|| Smith et all. || Smith etal.</p><hr><p>A study <font color=green>of</font> Morgan(1998) tested the effectiveness of the algorithm. (F)<br>A study <font color=red>by</font> Morgan(1998) tested the effectiveness of the algorithm. (T)</p><hr><p>Kim(2010) <font color=green>says</font> that this had actually occurred the year before. (F)<br>Kim(2010) <font color=red>asserts</font> that this had actually occurred the year before. (T)</p><hr><p>When the author of the resource gives <font color=red>guidance</font> or an <font color=red>opinion</font>:<br>Kim(2010) proposes/recommends/predicts/projects/suggests…</p><hr><p>When the author <font color=red>looks closely </font>at something,<br>Kim(2010) analyses/focuses/theorizes..</p><hr><p>When the author <font color=red>uncovers </font> somethings:<br>Kim(2010) discovers/finds/learns/reveals…</p><hr><p>When the author <font color=red>does not believe in </font>somethings:<br>Kim(2010) denies/questions/refutes/rejects…</p><hr><p>Forms of the verb “to concern” should be avoided when introducing a researcher’s work or study. The verb is often used in the wrong way.</p><p>In his work, Leung(2003; 2005) <font color=green>is concerned with</font> the inspiration behind the designs of Commes des Garcons. (F)<br>In his work, Leung(2003; 2005)  <font color=red>looks at/considers</font> the inspiration behind the designs of Commes des Garcons. (T)</p><p>Suto(2010) <font color=green>concerns about</font> the artist’s technique in these three pieces and concludes that..(F)<br>Suto(2010) <font color=red>considers/assesses</font> the artist’s technique in these three pieces and concludes that..(T)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Forward Maximum Matching Method in Chinese segmentation</title>
      <link href="2019/11/20/Forward-Maximum-Matching-Method-in-Chinese-segmentation/"/>
      <url>2019/11/20/Forward-Maximum-Matching-Method-in-Chinese-segmentation/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on 11.20.2019</span><br><span class="line">@author: Chen</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">def M(sentence):</span><br><span class="line">    dic&#x3D;&#123;&quot;苹果&quot;,&quot;在哪里&quot;&#125;</span><br><span class="line"></span><br><span class="line">    max_length &#x3D; 5</span><br><span class="line">    my_list &#x3D; []</span><br><span class="line">    len_hang &#x3D; len(sentence)</span><br><span class="line">    while len_hang &gt; 0:</span><br><span class="line">        tryWord &#x3D; sentence[0:max_length]</span><br><span class="line">        while tryWord not in dic:</span><br><span class="line">            if len(tryWord) &#x3D;&#x3D; 1:</span><br><span class="line">                break</span><br><span class="line">            tryWord &#x3D; tryWord[0:len(tryWord) - 1]</span><br><span class="line">        my_list.append(tryWord)</span><br><span class="line">        sentence &#x3D; sentence[len(tryWord):]</span><br><span class="line">        if len(sentence)&#x3D;&#x3D;0:</span><br><span class="line">            break</span><br><span class="line">    print(my_list)</span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    M(&quot;我的苹果在哪里&quot;)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">refers：https:&#x2F;&#x2F;www.cnblogs.com&#x2F;Jm-15&#x2F;p&#x2F;9403352.html</span><br><span class="line">正向即从前往后取词，从7-&gt;1，每次减一个字，直到词典命中或剩下1个单字。</span><br><span class="line"></span><br><span class="line">第1次：“我们在野生动物”，扫描7字词典，无</span><br><span class="line"></span><br><span class="line">第2次：“我们在野生动”，扫描6字词典，无</span><br><span class="line"></span><br><span class="line">。。。。</span><br><span class="line"></span><br><span class="line">第6次：“我们”，扫描2字词典，有</span><br><span class="line"></span><br><span class="line">扫描中止，输出第1个词为“我们”，去除第1个词后开始第2轮扫描，即：</span><br><span class="line"></span><br><span class="line">第2轮扫描：</span><br><span class="line"></span><br><span class="line">第1次：“在野生动物园玩”，扫描7字词典，无</span><br><span class="line"></span><br><span class="line">第2次：“在野生动物园”，扫描6字词典，无</span><br><span class="line"></span><br><span class="line">。。。。</span><br><span class="line"></span><br><span class="line">第6次：“在野”，扫描2字词典，有</span><br><span class="line"></span><br><span class="line">扫描中止，输出第2个词为“在野”，去除第2个词后开始第3轮扫描，即：</span><br><span class="line"></span><br><span class="line">第3轮扫描：</span><br><span class="line"></span><br><span class="line">第1次：“生动物园玩”，扫描5字词典，无</span><br><span class="line"></span><br><span class="line">第2次：“生动物园”，扫描4字词典，无</span><br><span class="line"></span><br><span class="line">第3次：“生动物”，扫描3字词典，无</span><br><span class="line"></span><br><span class="line">第4次：“生动”，扫描2字词典，有</span><br><span class="line"></span><br><span class="line">扫描中止，输出第3个词为“生动”，第4轮扫描，即：</span><br><span class="line"></span><br><span class="line">第4轮扫描：</span><br><span class="line"></span><br><span class="line">第1次：“物园玩”，扫描3字词典，无</span><br><span class="line"></span><br><span class="line">第2次：“物园”，扫描2字词典，无</span><br><span class="line"></span><br><span class="line">第3次：“物”，扫描1字词典，无</span><br><span class="line"></span><br><span class="line">扫描中止，输出第4个词为“物”，非字典词数加1，开始第5轮扫描，即：</span><br><span class="line"></span><br><span class="line">第5轮扫描：</span><br><span class="line"></span><br><span class="line">第1次：“园玩”，扫描2字词典，无</span><br><span class="line"></span><br><span class="line">第2次：“园”，扫描1字词典，有</span><br><span class="line"></span><br><span class="line">扫描中止，输出第5个词为“园”，单字字典词数加1，开始第6轮扫描，即：</span><br><span class="line"></span><br><span class="line">第6轮扫描：</span><br><span class="line"></span><br><span class="line">第1次：“玩”，扫描1字字典词，有</span><br><span class="line"></span><br><span class="line">扫描中止，输出第6个词为“玩”，单字字典词数加1，整体扫描结束。</span><br><span class="line"></span><br><span class="line">正向最大匹配法，最终切分结果为：“我们&#x2F;在野&#x2F;生动&#x2F;物&#x2F;园&#x2F;玩”</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>PCA with python</title>
      <link href="2019/11/19/PCA-with-python/"/>
      <url>2019/11/19/PCA-with-python/</url>
      
        <content type="html"><![CDATA[<p>I will use python to finish the PCA with two version. They all have the same result.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import numpy as  np</span><br><span class="line"></span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">def PCA_version1(Index):</span><br><span class="line">    # test data</span><br><span class="line">    mat &#x3D; [[-1, -1, 0, 2, 1], [2, 0, 0, -1, -1], [2, 0, 1, 1, 0]]</span><br><span class="line"></span><br><span class="line">    # simple transform of test data</span><br><span class="line">    Mat &#x3D; np.array(mat, dtype&#x3D;&#39;float64&#39;)</span><br><span class="line">    print(&#39;Before PCA, data is:\n&#39;, Mat)</span><br><span class="line"></span><br><span class="line">    p, n &#x3D; np.shape(Mat)  # shape of Mat  3 5</span><br><span class="line">    t &#x3D; np.mean(Mat, 0)  # the mean of each column</span><br><span class="line">    # step1:  substract the mean of each column</span><br><span class="line">    for i in range(p):</span><br><span class="line">        for j in range(n):</span><br><span class="line">            Mat[i, j] &#x3D; float(Mat[i, j] - t[j])</span><br><span class="line"></span><br><span class="line">    # step2: to obtain the covariance Matrix. use below</span><br><span class="line">    cov_Mat &#x3D; np.dot(Mat.T, Mat) &#x2F; (p - 1)</span><br><span class="line"></span><br><span class="line">    # step3: to obtain the eigvalues and eigenvectors of covariance Matrix with eigvalues descending</span><br><span class="line">    U, V &#x3D; np.linalg.eigh(cov_Mat)</span><br><span class="line">    # step4: Rearrange the eigenvectors and eigenvalues</span><br><span class="line">    U &#x3D; U[::-1]</span><br><span class="line">    for i in range(n):</span><br><span class="line">        V[i, :] &#x3D; V[i, :][::-1]</span><br><span class="line"></span><br><span class="line">    #step5: to obtain the subset of [eigenvectors of covariance Matrix]</span><br><span class="line">    v &#x3D; V[:, :Index]</span><br><span class="line"></span><br><span class="line">    # step6: data transformation, to get the last matrix</span><br><span class="line">    T1 &#x3D; np.dot(Mat, v)</span><br><span class="line"></span><br><span class="line">    print(&#39;We choose %d main factors.&#39; % Index)</span><br><span class="line">    print(&#39;After PCA:\n&#39;, T1)</span><br><span class="line">def PCA_version2():</span><br><span class="line">    pca &#x3D; PCA(n_components&#x3D;2)  # n_components can be integer or float in (0,1)</span><br><span class="line">    mat &#x3D; [[-1, -1, 0, 2, 1], [2, 0, 0, -1, -1], [2, 0, 1, 1, 0]]</span><br><span class="line">    pca.fit(mat)  # fit the model</span><br><span class="line">    print(pca.fit_transform(mat))</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    PCA_version1(Index&#x3D;2)</span><br><span class="line">    PCA_version2()</span><br></pre></td></tr></table></figure><p>ECAI2019 will stop delivery in 19 November 2019—23:59 UTC-12. Now the time is [19:23:04 UTC-12, Monday, 18 November 2019]  [Beijing: 15:24:02, Tue, 19 November 2019]</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Chen! Please divide a file into some childfiles!</title>
      <link href="2019/11/15/Chen-Please-divide-a-file-into-some-childfiles/"/>
      <url>2019/11/15/Chen-Please-divide-a-file-into-some-childfiles/</url>
      
        <content type="html"><![CDATA[<p>Come with me!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from shutil import copy2</span><br><span class="line">dir_path &#x3D; os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">trainfiles &#x3D; os.listdir(dir_path+&#39;\cccc&#39;)#list all file</span><br><span class="line"></span><br><span class="line">def split_list_average_n(origin_list, n):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    divided in the same scale</span><br><span class="line">    :param origin_list:</span><br><span class="line">    :param n:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for i in range(0, len(origin_list), n):</span><br><span class="line">        yield origin_list[i:i + n]</span><br><span class="line"></span><br><span class="line">def make_dir():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    to make dir</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    for i in range(23):</span><br><span class="line">        i&#x3D;i+1</span><br><span class="line">        os.makedirs(dir_path + &quot;\\dsd&quot;+str(i))</span><br><span class="line"></span><br><span class="line">def divid_wo():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    divide file</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    num &#x3D; 0</span><br><span class="line">    divid &#x3D; split_list_average_n(trainfiles, 100)  # each file has 100 doc</span><br><span class="line">    for titile in divid:</span><br><span class="line">        num&#x3D;num+1</span><br><span class="line">        Dir&#x3D;dir_path+&quot;\\分&quot;+str(num)</span><br><span class="line">        print(num)</span><br><span class="line">        for suffix in titile:</span><br><span class="line">            fileName &#x3D; os.path.join(dir_path+&#39;\cccc&#39;, suffix)</span><br><span class="line">            copy2(fileName, Dir)</span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">#     make_dir()</span><br><span class="line">    divid_wo()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Cosine similarity</title>
      <link href="2019/11/15/Cosine-similarity/"/>
      <url>2019/11/15/Cosine-similarity/</url>
      
        <content type="html"><![CDATA[<p>This section, I will use tensorflow to realise cosine similarity, It an important function in NLP.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def cos_sim(v1, v2):</span><br><span class="line">    norm1 &#x3D; tf.sqrt(tf.reduce_sum(tf.square(v1), axis&#x3D;1))</span><br><span class="line">    norm2 &#x3D; tf.sqrt(tf.reduce_sum(tf.square(v2), axis&#x3D;1))</span><br><span class="line">    dot_products &#x3D; tf.reduce_sum(v1 * v2, axis&#x3D;1, name&#x3D;&quot;cos_sim&quot;)</span><br><span class="line"></span><br><span class="line">    return dot_products &#x2F; (norm1 * norm2)</span><br><span class="line"></span><br><span class="line">v1 &#x3D; tf.constant([[1, 2], [3, 3]], dtype&#x3D;tf.float32)</span><br><span class="line">v2 &#x3D; tf.constant([[25, 9], [4, 49]], dtype&#x3D;tf.float32)</span><br><span class="line">v3&#x3D;tf.square(v1)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    s&#x3D;sess.run(cos_sim(v1,v2))</span><br><span class="line">    print(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">*************************************************</span><br><span class="line"></span><br><span class="line">def cos(vector1,vector2):</span><br><span class="line">    dot_product&#x3D;0</span><br><span class="line">normA&#x3D;0.0</span><br><span class="line">normB&#x3D;0.0</span><br><span class="line">for a,b in zip(vector1,vector2):</span><br><span class="line">dot_product+&#x3D;a*b</span><br><span class="line"></span><br><span class="line"># print(&quot;dot&quot;,dot_product)</span><br><span class="line">normA+&#x3D;a**2</span><br><span class="line">normB+&#x3D;b**2</span><br><span class="line">if normA&#x3D;&#x3D;0.0 or normB&#x3D;&#x3D;0.0:</span><br><span class="line">return None</span><br><span class="line">else:</span><br><span class="line">return dot_product&#x2F;(math.sqrt(normA)*math.sqrt(normB))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TF-pad</title>
      <link href="2019/11/15/TF-pad/"/>
      <url>2019/11/15/TF-pad/</url>
      
        <content type="html"><![CDATA[<pre><code>I first learnd tf-pad is in ABCNN. tf.pad(tensor, paddings, mode, name), mode have three styles, it includes: &quot;CONSTANT&quot;, &quot;REFLECT&quot; and &quot;SYMMERTIC&quot;. In this section, I only use &quot;CONSTANT&quot;==padding 0. The rank of tensor and padding must be same. So, what does it mean? For excample, t=[[[2,3,4],[5,6,7]],[[2,3,4],[5,6,7]]], shape (t) is 3, so the length of array in padding  must be 3. a=(tf.pad(t,[[0,0],[0,0],[4,4]],&quot;CONSTANT&quot;))</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">------------dimension 1:--------------</span><br><span class="line">with tensorflow as tf</span><br><span class="line">t&#x3D;[2,3,4,8]</span><br><span class="line"></span><br><span class="line">a&#x3D;(tf.pad(t,[[2,2]],&quot;CONSTANT&quot;))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(a))</span><br><span class="line"></span><br><span class="line">result: [0 0 2 3 4 8 0 0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">------------dimension 2:--------------</span><br><span class="line">t&#x3D;[[2,3,4],[5,6,7]]</span><br><span class="line"></span><br><span class="line">a&#x3D;(tf.pad(t,[[0,0],[4,4]],&quot;CONSTANT&quot;))</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(a))</span><br><span class="line"></span><br><span class="line">result: [[0 0 0 0 2 3 4 0 0 0 0]</span><br><span class="line"> [0 0 0 0 5 6 7 0 0 0 0]]</span><br><span class="line"></span><br><span class="line">------------dimension 3:--------------</span><br><span class="line"></span><br><span class="line">t&#x3D;[[[2,3,4],[5,6,7]],[[2,3,4],[5,6,7]]]</span><br><span class="line"></span><br><span class="line">a&#x3D;(tf.pad(t,[[0,0],[0,0],[4,4]],&quot;CONSTANT&quot;))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(a))</span><br><span class="line"></span><br><span class="line">result: [[[0 0 0 0 2 3 4 0 0 0 0]</span><br><span class="line">       [0 0 0 0 5 6 7 0 0 0 0]]</span><br><span class="line"></span><br><span class="line">       [[0 0 0 0 2 3 4 0 0 0 0]</span><br><span class="line">       [0 0 0 0 5 6 7 0 0 0 0]]]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Doc to docx</title>
      <link href="2019/11/12/Docx-to-doc/"/>
      <url>2019/11/12/Docx-to-doc/</url>
      
        <content type="html"><![CDATA[<p>I want to extract the table from Word, but its suffix is “doc”, python can only progress “docx” by using <font color=red>“from docx import Document”</font>. We cannot modify file directly, there are too many implicit errors in it. “Save as” is a good idea, so, what should we do when we have a lot of “doc” files? </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from win32com import client as wc</span><br><span class="line"></span><br><span class="line">dir_path &#x3D; os.path.dirname(os.path.abspath(__file__))</span><br><span class="line"></span><br><span class="line">def make_dir():</span><br><span class="line">    paths&#x3D;[]</span><br><span class="line">    rootdir &#x3D; dir_path+&#39;\**&#39;</span><br><span class="line">    list &#x3D; os.listdir(rootdir)  </span><br><span class="line">    for i in range(0, len(list)):</span><br><span class="line">        path &#x3D; os.path.join(rootdir, list[i])</span><br><span class="line">        paths.append(path)</span><br><span class="line">    return paths</span><br><span class="line"></span><br><span class="line">def doSaveAas():</span><br><span class="line">    paths&#x3D;make_dir()</span><br><span class="line">    newpath&#x3D;dir_path+&#39;\\NEW&#39;</span><br><span class="line">    i&#x3D;0</span><br><span class="line">    for path in paths:</span><br><span class="line">        i&#x3D;i+1</span><br><span class="line">        print(i)</span><br><span class="line">        word &#x3D; wc.Dispatch(&#39;Word.Application&#39;)</span><br><span class="line">        name&#x3D;path.split(&quot;.&quot;)[0].replace(&#39;**&#39;,&#39;NEW&#39;)</span><br><span class="line"></span><br><span class="line">        doc &#x3D; word.Documents.Open(path)  </span><br><span class="line">        doc.SaveAs(name+&#39;.docx&#39;, 12)  # wdFormatXMLDocument &#x3D; 12</span><br><span class="line">        doc.Close()</span><br><span class="line">        word.Quit()</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    doSaveAas()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Remove duplicate words in txt (more than 1G)</title>
      <link href="2019/11/11/Remove-duplicate-words-in-txt/"/>
      <url>2019/11/11/Remove-duplicate-words-in-txt/</url>
      
        <content type="html"><![CDATA[<pre><code>Today I will share a method about how to remove duplicate words in txt (more than 1G). In the first, I want to read all words into a list, and then use &quot;set()&quot; or &quot;if word not in list&quot; to solve it, it&#39;s a bad idea! Because, python need to travel each word in the list, it is time consuming (very long time). &quot;dic&quot; is good thing in python, there is no same key in it, so I use fromkeys to settle this issues, as shown in below:</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def  DUP():</span><br><span class="line">     fw &#x3D; open(&quot;outputfile.txt&quot;, &quot;w&quot;, encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line">     s&#x3D;[]</span><br><span class="line">     with open(&quot;inputfile.txt&quot;, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as fr:</span><br><span class="line"></span><br><span class="line">         list2&#x3D;&#123;&#125;.fromkeys((fr.readlines())).keys()</span><br><span class="line">         i&#x3D;0</span><br><span class="line">         for word in list2:</span><br><span class="line">             i&#x3D;i+1</span><br><span class="line">             fw.write(word)</span><br><span class="line"> fw.flush()</span><br><span class="line"></span><br><span class="line">fw.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>It’s all!! In today, I have received my new phone–realme Q (8+128). The battery of my old phone is broken, I have to say goodbye with it. I am amazed by the developement of internet, mobliephone’s RAM is more than 8G.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Two metrics (MRR and NDCG) for QA or IR</title>
      <link href="2019/11/09/Metrics-of-question-answering-or-information-retrieval/"/>
      <url>2019/11/09/Metrics-of-question-answering-or-information-retrieval/</url>
      
        <content type="html"><![CDATA[<p>Duo to project need, I used python to realize MRR and NDCG, it has two premises: 1&gt; the greater the number, the greater the correlation. 2&gt; it have 3 grades (3,2,1)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line"></span><br><span class="line">def ndcg(rankedlist, test_matrix,k):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    :param rankedlist:    right list</span><br><span class="line">    :param test_matrix:   predict list</span><br><span class="line">    :param k:  top k</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    dcg &#x3D; 0</span><br><span class="line">    idcg &#x3D; 0</span><br><span class="line">    rankedlist&#x3D;rankedlist[0:k]</span><br><span class="line">    test_matrix&#x3D;test_matrix[0:k]</span><br><span class="line">    for i,reli in enumerate(rankedlist):</span><br><span class="line">        idcg+&#x3D;reli&#x2F; math.log(i+2,2)</span><br><span class="line"></span><br><span class="line">    for i, reli in enumerate(test_matrix):</span><br><span class="line">        dcg +&#x3D; reli &#x2F; math.log(i + 2, 2)</span><br><span class="line">    return dcg, idcg, dcg&#x2F;idcg</span><br><span class="line"></span><br><span class="line">def MRR(rankedlist, test_matrix):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    just for: The greater the number, the greater the correlation</span><br><span class="line">    :param rankedlist:</span><br><span class="line">    :param test_matrix:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    a&#x3D;list(set(rankedlist))</span><br><span class="line">    a.reverse()</span><br><span class="line">    m&#x3D;0</span><br><span class="line">    for i in test_matrix:</span><br><span class="line">        if i&#x3D;&#x3D;a[0]:</span><br><span class="line">            m&#x3D;m+1</span><br><span class="line">        elif i&#x3D;&#x3D;a[1]:</span><br><span class="line">            m&#x3D;m+1&#x2F;2</span><br><span class="line">        elif i&#x3D;&#x3D;a[2]:</span><br><span class="line">            m &#x3D; m+1 &#x2F; 3</span><br><span class="line"></span><br><span class="line">    return m&#x2F;len(test_matrix)</span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    rankedlist&#x3D;[3,3,3,2,2,1]</span><br><span class="line">    test_matrix&#x3D;[3,2,3,0,1,2]</span><br><span class="line">    dcg,idcg,ndcg&#x3D;ndcg(rankedlist,test_matrix,6)</span><br><span class="line">    mrr&#x3D;MRR(rankedlist,test_matrix)</span><br><span class="line">    print(mrr)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ECAI2020</title>
      <link href="2019/11/08/note-ECAI/"/>
      <url>2019/11/08/note-ECAI/</url>
      
        <content type="html"><![CDATA[<p>Seven days to go before the abstract submission deadline of ECAI2020. The biennial <a href="http://ecai2020.eu/">ECAI</a> is Europe’s premier venue for presenting scientific results in AI. It has NLP topic. CCF grade is b.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Realize a semantic matching model (Chinese) with MatchZoo.</title>
      <link href="2019/11/06/CMQA-1/"/>
      <url>2019/11/06/CMQA-1/</url>
      
        <content type="html"><![CDATA[<p><font color=red>MatchZoo</font> is a text matching Toolkit, it includes many typical models, such as DRMM, ARC-ii, KNRM, etc. In many work, authors choose MatchZoo to implement their baseline models. This blog will introduce how to use MatchZoo to complete a semantic matching model for Chinese question answer. In the next, I will give my code, this code references: <a href="https://github.com/NTMC-Community/MatchZoo">MatchZoo’s github</a> and  <a href="https://blog.csdn.net/wkh7717/article/details/89886713">notebook about MatchZoo</a>.  If there is any question, please contact me!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">#encoding&#x3D;utf-8</span><br><span class="line">import matchzoo as mz</span><br><span class="line">import  pandas as pd</span><br><span class="line">import csv</span><br><span class="line">task&#x3D;mz.tasks.Ranking()</span><br><span class="line"></span><br><span class="line">#OK! let&#39;s begin! In the first you should download this toolkit in https:&#x2F;&#x2F;github.com&#x2F;NTMC-Community&#x2F;MatchZoo</span><br><span class="line"></span><br><span class="line">def _read_data(path):</span><br><span class="line">    print(path)</span><br><span class="line">    table &#x3D; pd.read_csv(path, sep&#x3D;&#39;\t&#39;, header&#x3D;0, quoting&#x3D;csv.QUOTE_NONE)</span><br><span class="line">    df &#x3D; pd.DataFrame(&#123;</span><br><span class="line">        &#39;text_left&#39;: table[&#39;Question&#39;],</span><br><span class="line">        &#39;text_right&#39;: table[&#39;Sentence&#39;],</span><br><span class="line">        # &#39;id_left&#39;: table[&#39;QuestionID&#39;],</span><br><span class="line">        # &#39;id_right&#39;: table[&#39;SentenceID&#39;],</span><br><span class="line">        &#39;label&#39;: table[&#39;Label&#39;]</span><br><span class="line">    &#125;)</span><br><span class="line">    return mz.pack(df)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(train_raw.left)</span><br><span class="line">print(train_raw.right)</span><br><span class="line">print(train_raw.relation.head())</span><br><span class="line">print(train_raw.frame().head())</span><br><span class="line"></span><br><span class="line">def configure_model():</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    emb &#x3D; mz.embedding.load_from_file(mz.datasets.embeddings.EMBED_CPWS)</span><br><span class="line">    this function is to reload  the Chinese vector matrix,</span><br><span class="line">    for emb: step:</span><br><span class="line">    1: download the Chinese word embedding, put it in this dic:</span><br><span class="line">    this mine:E: \Anaconda3\Lib\site-packages\MatchZoo-2.2.0-py3.6.egg\matchzoo\datasets\embeddings</span><br><span class="line">    2: add one code in \Anaconda3\Lib\site-packages\MatchZoo-2.2.0-py3.6.egg\matchzoo\datasets\embeddings\__init__.py:</span><br><span class="line">                    EMBED_CPWS &#x3D; DATA_ROOT.joinpath(&#39;sgns.baidubaike.txt&#39;)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    emb &#x3D; mz.embedding.load_from_file(mz.datasets.embeddings.EMBED_CPWS)</span><br><span class="line">    #model</span><br><span class="line">    model_class &#x3D; mz.models.ArcI</span><br><span class="line">    #reload the embedding</span><br><span class="line">    model, preprocessor, data_generator_builder, embedding_matrix &#x3D; mz.auto.prepare(</span><br><span class="line">        task&#x3D;task,</span><br><span class="line">        model_class&#x3D;model_class,</span><br><span class="line">        data_pack&#x3D;train_raw,</span><br><span class="line">        embedding&#x3D;emb</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    return model, preprocessor, data_generator_builder, embedding_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__&#x3D;&#x3D;&quot;__main__&quot;:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # the style of  &quot;train_wikistype.tsv&quot; and &quot;train_wikistype.tsv&quot; a shown in the next</span><br><span class="line">    QuestionSentenceLabel</span><br><span class="line">    how are glacier caves formed?A partly submerged glacier cave on Perito Moreno Glacier .0</span><br><span class="line">    how are glacier caves formed?The ice facade is approximately 60 m high0</span><br><span class="line">    how are glacier caves formed?Glacier caves are often called ice caves , but this term is proper0</span><br><span class="line">    How are the directions of..In physics , circular motion is a movement of an object along the0</span><br><span class="line">    How are the directions of.It can be uniform, with constant angular rate of rotation (and consta0</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    train_path &#x3D; &quot;train_wikistype.tsv&quot;</span><br><span class="line">    test_path &#x3D; &quot;test_wikistype.tsv&quot;</span><br><span class="line"></span><br><span class="line">    train_raw &#x3D; _read_data(train_path)</span><br><span class="line">    test_raw &#x3D; _read_data(test_path)</span><br><span class="line">    model, preprocessor, data_generator_builder, embedding_matrix&#x3D;configure_model()</span><br><span class="line">#adjust parameter</span><br><span class="line">    model.params[&#39;mlp_num_units&#39;] &#x3D; 3</span><br><span class="line">    model.params[&#39;with_embedding&#39;] &#x3D; True</span><br><span class="line">    model.params[&#39;dropout_rate&#39;] &#x3D; 0.5</span><br><span class="line">    #there are six  #evaluation metrics,</span><br><span class="line">    model.params[&#39;task&#39;].metrics &#x3D; [mz.metrics.AveragePrecision(threshold&#x3D;1), #threshold  refers to &quot;The label threshold of relevance degree&quot;  default is 0.  [0,1,2], in its source code, it refers to the minimum value.</span><br><span class="line">                                    mz.metrics.Precision(k&#x3D;2, threshold&#x3D;2),</span><br><span class="line">                                    mz.metrics.DiscountedCumulativeGain(k&#x3D;2),</span><br><span class="line">                                    mz.metrics.NormalizedDiscountedCumulativeGain(k&#x3D;3, threshold&#x3D;-1),</span><br><span class="line">                                    mz.metrics.MeanReciprocalRank(threshold&#x3D;2),</span><br><span class="line">                                    mz.metrics.MeanAveragePrecision(threshold&#x3D;3)]</span><br><span class="line"></span><br><span class="line">    print(model.params)  # to show the parameters which can be adjusted</span><br><span class="line">    s&#x3D;preprocessor.context  # check the size of dic, such as vocab_size&#39;: 13, &#39;embedding_input_dim&#39;: 13, &#39;input_shapes&#39;: [(30,), (30,)</span><br><span class="line">    train_processed &#x3D; preprocessor.transform(train_raw)</span><br><span class="line">    test_processed &#x3D; preprocessor.transform(test_raw)</span><br><span class="line"></span><br><span class="line">    #generate trainset</span><br><span class="line">    x, y &#x3D; train_processed.unpack()</span><br><span class="line">    #generate testset</span><br><span class="line">    test_x, test_y &#x3D; test_processed.unpack()</span><br><span class="line">    model.build()</span><br><span class="line">    model.compile()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    model.fit(x, y, batch_size&#x3D;8, epochs&#x3D;2)</span><br><span class="line">    #evaluate model</span><br><span class="line">    b&#x3D;model.evaluate(test_x, test_y)</span><br><span class="line">    print(b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Some NLP conferences in 2020, SIGIR 2020 in China !!</title>
      <link href="2019/11/03/2019-11-3/"/>
      <url>2019/11/03/2019-11-3/</url>
      
        <content type="html"><![CDATA[<p>ACL 2020</p><pre><code>Full_name: Annual Meeting of the Association for Computational LinguisticsWebsite: https://acl2020.orgSubmission deadline: 2019/12/9 Notification of acceptance (long &amp; short papers): 2020/4/3Conference place: Seattle, WashingtonConfernce time: July 5-10 2020</code></pre><p>IJCAI 2020</p><pre><code>Full_name: International Joint Conference on Artificial IntelligenceWebsite: http://www.ijcai20.org/Abject submission deadline: 2020/1/15Paper submission deadline: 2020/1/21Notification of acceptance (long &amp; short papers): 2020/4/19Conference place: Yokohama, JapanConfernce time: July 11-17 2020</code></pre><p>SIGIR 2020</p><pre><code>Full_name: International Conference on Research and Development in Information RetrievalWebsite: http://sigir.org/sigir2020/Abject submission deadline: 2020/1/15Paper submission deadline: 2020/1/22Notification of acceptance (long &amp; short papers): 2020/4/22Conference place: Xi an, ChinaConfernce time: July 25-30 2020</code></pre>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
